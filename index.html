<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>

<meta charset="utf-8" />
<meta name="generator" content="quarto-1.3.450" />

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />


<title>Comparing Decision Tree and Random Forest Classifier</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<!-- htmldependencies:E3FAD763 -->

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src  = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>


</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <div id="quarto-toc-target"></div>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Comparing Decision Tree and Random Forest Classifier</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>
<nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sample-data-used-in-classification" id="toc-sample-data-used-in-classification">Sample Data Used in Classification</a></li>
  <li><a href="#building-the-decision-tree-model" id="toc-building-the-decision-tree-model">Building the Decision Tree Model</a></li>
  <li><a href="#randomforestclassifer-ensemble-approach" id="toc-randomforestclassifer-ensemble-approach">RandomForestClassifer (Ensemble approach)</a></li>
  <li><a href="#results-and-conclusions" id="toc-results-and-conclusions">Results and Conclusions</a></li>
  </ul>
</nav>
<!-- title: "Comparing Decision Tree and Random Forest Classifier Performance"
format:
  html:
    code-fold: true
jupyter: python3 -->
<p><strong>Author: Daniel Hassler</strong></p>
<section id="sample-data-used-in-classification" class="level2">
<h2>Sample Data Used in Classification</h2>
<p>To compare a DecisionTree and a RandomForestClassifier, the first step I took was to gather some data and run some visualizations and analysis. Through Kaggle, I was able to obtain a small dataset on person features and their BMI (Body Mass Index) data. The data consists of just around 400 samples with features: gender, height, and weight, and the goal is to predict BMI.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode" id="cb1"><pre class="sourceCode markdown cell-code"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split, cross_val_score, StratifiedKFold, GridSearchCV</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier, plot_tree</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> tree</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode" id="cb2"><pre class="sourceCode markdown cell-code"><code class="sourceCode markdown"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(<span class="st">&quot;./datasets/bmi_train.csv&quot;</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>category_mapping <span class="op">=</span> {<span class="st">&#39;Male&#39;</span>: <span class="dv">0</span>, <span class="st">&#39;Female&#39;</span>: <span class="dv">1</span>}</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>data[<span class="st">&#39;Gender_Encoded&#39;</span>] <span class="op">=</span> data[<span class="st">&#39;Gender&#39;</span>].<span class="bu">map</span>(category_mapping) <span class="co"># converts categorical data to numeric data.</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data.drop([<span class="st">&#39;Gender&#39;</span>,<span class="st">&#39;Index&#39;</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data.drop([<span class="st">&#39;Gender&#39;</span>, <span class="st">&#39;Gender_Encoded&#39;</span>, <span class="st">&#39;Height&#39;</span>, <span class="st">&#39;Weight&#39;</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;All X shape: &quot;</span>, X.shape)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;All y shape: &quot;</span>, y.shape)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>validation_data <span class="op">=</span> pd.read_csv(<span class="st">&quot;./datasets/bmi_validation.csv&quot;</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Validation data shape: &quot;</span>, validation_data.shape)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;X_train shape: &quot;</span>, X_train.shape)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;y_train shape: &quot;</span>, y_train.shape)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;X_test shape: &quot;</span>, X_test.shape)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;y_test shape: &quot;</span>, y_test.shape)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>All X shape:  (400, 3)
All y shape:  (400, 1)
Validation data shape:  (100, 3)
X_train shape:  (320, 3)
y_train shape:  (320, 1)
X_test shape:  (80, 3)
y_test shape:  (80, 1)</code></pre>
</div>
</div>
<p>In the above code snippet, I first populated my data into a Pandas dataframe and then split up the data into a “training” and “testing” datasets. I decided to go with an 80/20% split between train and test (with its corresponding labels), as that seems to be the most standard approach in the industry. The significant benefit here is that I possess labeled data on both sets, a challenge in practice. This enables me to make comparisons between predictions and outcomes on my data, eliminating the need to procure any additional “test” data.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode" id="cb4"><pre class="sourceCode markdown cell-code"><code class="sourceCode markdown"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Class imbalance, more obesity.</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>unique_values, counts <span class="op">=</span> np.unique(y, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>plt.bar(unique_values, counts)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;BMI Classes in the Entire Dataset&quot;</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;BMI Class&quot;</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;Occurences in Entire Dataset&quot;</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-4-output-1.png" width="593" height="449" /></p>
</div>
</div>
<p>The labels are all discrete and sequential, consisting of whole numbers between 0 and 5, further enforcing my intuition for using a classifier approach. A “0” in my case represents someone with an <strong>exeptionally low</strong> BMI, whereas a “5” depicts an <strong>exceptionally high</strong> BMI. Based on the distribution of the data, there appears to be a huge class imbalance, heavily favoring the amount of <strong>exceptionally high</strong> instances in the dataset; this was something I needed to keep in mind when building the classifiers for this dataset.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode" id="cb5"><pre class="sourceCode markdown cell-code"><code class="sourceCode markdown"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>correlation_matrix <span class="op">=</span> data.corr()</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Display a heatmap of the correlation matrix</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>sns.heatmap(correlation_matrix, annot<span class="op">=</span><span class="va">True</span>, cmap<span class="op">=</span><span class="st">&#39;coolwarm&#39;</span>, center<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Correlation Heatmap&#39;</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\dwh71\AppData\Local\Temp\ipykernel_17876\1253239144.py:1: FutureWarning:

The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.
</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-5-output-2.png" width="738" height="505" /></p>
</div>
</div>
<p>The correlation matrix depicts the correlation between features (height, weight, gender, BMI) in the dataset. It uses the pearson’s correlation coefficient to compute this: <span class="math display">\[
r =
  \frac{ \sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y}) }{
        \sqrt{\sum_{i=1}^{n}(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^{n}(y_i-\bar{y})^2}}
\]</span></p>
<p>Based on the features presented, most are not correlated strongly, but there is a glaring strong correlation between weight and BMI.</p>
</section>
<section id="building-the-decision-tree-model" class="level2">
<h2>Building the Decision Tree Model</h2>
<p>In order to start the model building process, I decided to tune the hyperparamters first by running a <code>GridSearch</code></p>
<div class="cell" data-execution_count="5">
<div class="sourceCode" id="cb7"><pre class="sourceCode markdown cell-code"><code class="sourceCode markdown"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;max_depth&#39;</span>: [i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>, <span class="dv">10</span>)],</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;min_samples_leaf&#39;</span>: [<span class="dv">2</span> <span class="op">**</span> i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">4</span>)],</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;criterion&#39;</span>: [<span class="st">&quot;entropy&quot;</span>, <span class="st">&quot;gini&quot;</span>]</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>dt <span class="op">=</span> DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>grid_search_dt <span class="op">=</span> GridSearchCV(dt, param_grid, cv<span class="op">=</span>StratifiedKFold(n_splits<span class="op">=</span><span class="dv">5</span>), scoring<span class="op">=</span><span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>grid_search_dt.fit(X_train, y_train)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>best_params_dt <span class="op">=</span> grid_search_dt.best_params_</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Best Hyperparameters:&quot;</span>, best_params_dt)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Best Score:&quot;</span>, grid_search_dt.best_score_)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best Hyperparameters: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 9, &#39;min_samples_leaf&#39;: 1}
Best Score: 0.834375</code></pre>
</div>
</div>
<p>I recognized that <code>max_depth</code> was an important hyperparameter for the DecisionTree (DT), as the depth of the tree heavily influences overfitting, but other hyperparameters are important as well, such as:</p>
<ul>
<li><p><code>min_samples_leaf</code>: the minimum amount of samples needed in a leaf node of the DT. For example, when min_samples_leaf is set to 10, that means a node won’t split if it has fewer than 10 samples. When this number is higher, the model can create a more generalized tree, although, when the number is smaller, it’ll create more specific splits, resulting in a more complex tree (more potential for overfitting).</p></li>
<li><p><code>criterion</code>: this hyperparameter chooses whether to use entropy or Gini index as a way to calculate dissimilarity in a node. I found that in most cases, entropy outpreformed the Gini index. <span class="math display">\[
Entropy(C) = -\sum_{c=1}^Cp(c)\log(p(c))
\]</span></p></li>
</ul>
<p><span class="math display">\[
Gini(C) = 1 - \sum_{c=1}^Cp(c)^2
\]</span></p>
<p>Now that I’ve determined the necessary hyperparameters for this classifier, I initialize the <code>GridSearchCV</code> object to analyze every combination of the above hyperparameters. Within its search, it goes through an important cross-validation step (cv) that splits the training data into multiple folds and iterates through each fold for each hyperparameter combination.</p>
<p>There were a few options I could’ve chose from for the cv parameter in <code>GridSearchCV</code>, but in order to account for class imbalance like I stated earlier, I decided to go with a <code>StratifiedKFold</code> cross-validator. StratifiedKFold accounts for class label imbalance by keeping an equal precentage of classes for training and testing represented in the dataset. Below is a picture representing this:</p>
<p><img style="display: block;
    margin-left: auto;
    margin-right: auto;"
    height="300" width="300" src="https://amueller.github.io/aml/_images/stratified_cv.png"></img></p>
<div class="cell" data-execution_count="6">
<div class="sourceCode" id="cb9"><pre class="sourceCode markdown cell-code"><code class="sourceCode markdown"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>dt <span class="op">=</span> DecisionTreeClassifier(max_depth<span class="op">=</span>best_params_dt[<span class="st">&quot;max_depth&quot;</span>], min_samples_leaf<span class="op">=</span>best_params_dt[<span class="st">&quot;min_samples_leaf&quot;</span>], criterion<span class="op">=</span>best_params_dt[<span class="st">&quot;criterion&quot;</span>])</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>dt.fit(X_train, y_train)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> dt.predict(X_test)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code></pre></div>
</div>
<p>I then created a <code>DecisionTreeClassifier</code> with the ‘best’ tuned hyperparameters from the above grid search and populated the <code>y_pred</code> array with the predictions from the test dataset. After that, I plotted the tree out using Sklearn’s plot_tree method.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode" id="cb10"><pre class="sourceCode markdown cell-code"><code class="sourceCode markdown"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">40</span>, <span class="dv">40</span>))</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>plot_tree(dt, feature_names<span class="op">=</span>X_train.columns.tolist(), class_names<span class="op">=</span>[<span class="st">&#39;0&#39;</span>, <span class="st">&#39;1&#39;</span>, <span class="st">&#39;2&#39;</span>,<span class="st">&#39;3&#39;</span>,<span class="st">&#39;4&#39;</span>,<span class="st">&#39;5&#39;</span>], filled<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-8-output-1.png" width="2995" height="2976" /></p>
</div>
</div>
<p>After plotting the tree, I created a confusion matrix, showing where my predictions fell. Currently, the model sits around 75-86% accurate due to the above hyperparameter values and the randomly generated tree with those hyperparameter values. Not bad for a small dataset with class imbalance.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode" id="cb11"><pre class="sourceCode markdown cell-code"><code class="sourceCode markdown"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>confusion_matrix <span class="op">=</span> sklearn.metrics.confusion_matrix(y_test, y_pred)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>conf_df <span class="op">=</span> pd.DataFrame(confusion_matrix, index<span class="op">=</span>[<span class="ss">f&quot;</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">&quot;</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">6</span>)], columns<span class="op">=</span>[<span class="ss">f&quot;</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">&quot;</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">6</span>)])</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>heatmap <span class="op">=</span> sns.heatmap(conf_df, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">&quot;d&quot;</span>, linewidths<span class="op">=</span><span class="fl">0.35</span>, cmap<span class="op">=</span><span class="st">&quot;YlGnBu&quot;</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f&quot;Model Predictions With </span><span class="sc">{</span>(np.<span class="bu">sum</span>(confusion_matrix.diagonal()) <span class="op">/</span> y_test.shape[<span class="dv">0</span>]) <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">% Accuracy&quot;</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>Text(0.5, 1.0, &#39;Model Predictions With 73.75% Accuracy&#39;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-9-output-2.png" width="529" height="431" /></p>
</div>
</div>
</section>
<section id="randomforestclassifer-ensemble-approach" class="level2">
<h2>RandomForestClassifer (Ensemble approach)</h2>
<p>As above with the <code>DecisionTreeClassifer</code>, I first started to implement the <code>RandomForestClassifier</code> by tuning the hyperparameter values. Since a RandomForest is just a collection of DecisionTrees, RandomForestClassifiers, like a <code>DecisionTreeClassifier</code>, have mostly the same hyperparameters, but the <code>RandomForestClassifier</code> has an extra one for the amount of DecisionTrees that should be included in the forest (<code>n_estimators</code>).</p>
<p>Though this step wasn’t as necessary, since I already did the hyperparameter tuning part for the DecisionTree, but I decided to include it again for the RandomForest with the number of estimators.</p>
<p>It is important to note that the <code>n_estimators</code> hyperparameter won’t cause the model to overfit. In fact, it actually does better at generalization when increasing the number of estimators due to the diversity of opinions the model presents for each unique DecisionTree. The only way overfitting can happen in a RandomForest depends on how the underlying DecisionTrees are set up, not the quantity of them.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode" id="cb13"><pre class="sourceCode markdown cell-code"><code class="sourceCode markdown"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;max_depth&#39;</span>: [i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>, <span class="dv">10</span>)],</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;min_samples_leaf&#39;</span>: [<span class="dv">2</span> <span class="op">**</span> i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">4</span>)],</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;criterion&#39;</span>: [<span class="st">&quot;entropy&quot;</span>, <span class="st">&quot;gini&quot;</span>]</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>rf <span class="op">=</span> RandomForestClassifier(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y_train.shape)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>grid_search_rf <span class="op">=</span> GridSearchCV(rf, param_grid, cv<span class="op">=</span>StratifiedKFold(n_splits<span class="op">=</span><span class="dv">5</span>), scoring<span class="op">=</span><span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>grid_search_rf.fit(X_train, y_train.values.ravel())</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>best_params_rf <span class="op">=</span> grid_search_rf.best_params_</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Best Hyperparameters:&quot;</span>, best_params_rf)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Best Score:&quot;</span>, grid_search_rf.best_score_)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(320, 1)
Best Hyperparameters: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 8, &#39;min_samples_leaf&#39;: 1}
Best Score: 0.86875</code></pre>
</div>
</div>
<div class="cell" data-execution_count="10">
<div class="sourceCode" id="cb15"><pre class="sourceCode markdown cell-code"><code class="sourceCode markdown"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>rf <span class="op">=</span> RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">300</span>, </span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>                            max_depth<span class="op">=</span>best_params_rf[<span class="st">&quot;max_depth&quot;</span>], </span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>                            min_samples_leaf<span class="op">=</span>best_params_rf[<span class="st">&quot;min_samples_leaf&quot;</span>],</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>                            criterion<span class="op">=</span>best_params_rf[<span class="st">&quot;criterion&quot;</span>])</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>rf.fit(X_train, y_train.values.ravel())</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> rf.predict(X_test)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code></pre></div>
</div>
<p>The above code snippet creates the <code>RandomForestClassifier</code> with the same hyperparameters as the DecisionTree, in addition to the number of estimators (number of decision trees in the forest), trains the classifier, then stores a prediction array.</p>
<p>Here is a visualizaiton of a subset of DecisionTrees in this RandomForest:</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode" id="cb16"><pre class="sourceCode markdown cell-code"><code class="sourceCode markdown"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(nrows <span class="op">=</span> <span class="dv">1</span>,ncols <span class="op">=</span> <span class="dv">5</span>,figsize <span class="op">=</span> (<span class="dv">10</span>,<span class="dv">3</span>), dpi<span class="op">=</span><span class="dv">250</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> index <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>        tree.plot_tree(rf.estimators_[index],</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>                    feature_names <span class="op">=</span> X_train.columns.tolist(), </span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>                    class_names<span class="op">=</span> [<span class="ss">f&quot;</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">&quot;</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">6</span>)],</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>                    filled <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>                    ax <span class="op">=</span> axes[index])</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>        axes[index].set_title(<span class="st">&#39;Estimator: &#39;</span> <span class="op">+</span> <span class="bu">str</span>(index <span class="op">+</span> <span class="dv">1</span>), fontsize <span class="op">=</span> <span class="dv">10</span>)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-12-output-1.png" class="img-fluid" /></p>
</div>
</div>
<p>After running the model, I checked the accuracy output of the prediction array and found that the RandomForestClassifier was able to increase the accuracy of the predictions by a considerable amount on average.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode" id="cb17"><pre class="sourceCode markdown cell-code"><code class="sourceCode markdown"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>confusion_matrix <span class="op">=</span> sklearn.metrics.confusion_matrix(y_test, y_pred)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>conf_df <span class="op">=</span> pd.DataFrame(confusion_matrix, index<span class="op">=</span>[<span class="ss">f&quot;</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">&quot;</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">6</span>)], columns<span class="op">=</span>[<span class="ss">f&quot;</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">&quot;</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">6</span>)])</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>heatmap <span class="op">=</span> sns.heatmap(conf_df, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">&quot;d&quot;</span>, linewidths<span class="op">=</span><span class="fl">0.35</span>, cmap<span class="op">=</span><span class="st">&quot;YlGnBu&quot;</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f&quot;Model Predictions With </span><span class="sc">{</span>(np.<span class="bu">sum</span>(confusion_matrix.diagonal()) <span class="op">/</span> y_test.shape[<span class="dv">0</span>]) <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">% Accuracy&quot;</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>Text(0.5, 1.0, &#39;Model Predictions With 82.50% Accuracy&#39;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-13-output-2.png" width="529" height="431" /></p>
</div>
</div>
<p>Finally, I decided to calculate the accuracy preformance on multiple samples of RandomForestClassifiers and DecisionTrees at the same time and plot them out in a line chart.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode" id="cb19"><pre class="sourceCode markdown cell-code"><code class="sourceCode markdown"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;&#39;&#39;</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co">Plot a graph that compares the two models, randomly generated with tuned hyperparameter models</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;&#39;&#39;</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>dt_results <span class="op">=</span> []</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>rf_results <span class="op">=</span> []</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>indexes <span class="op">=</span> [i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_samples)]</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> indexes:</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    dt <span class="op">=</span> DecisionTreeClassifier(max_depth<span class="op">=</span>best_params_dt[<span class="st">&quot;max_depth&quot;</span>], </span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>                   min_samples_leaf<span class="op">=</span>best_params_dt[<span class="st">&quot;min_samples_leaf&quot;</span>], criterion<span class="op">=</span>best_params_dt[<span class="st">&quot;criterion&quot;</span>])</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    dt.fit(X_train, y_train)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    y_pred_dt <span class="op">=</span> dt.predict(X_test)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>    rf <span class="op">=</span> RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">300</span>, </span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>                            max_depth<span class="op">=</span>best_params_rf[<span class="st">&quot;max_depth&quot;</span>], </span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>                            min_samples_leaf<span class="op">=</span>best_params_rf[<span class="st">&quot;min_samples_leaf&quot;</span>],</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>                            criterion<span class="op">=</span>best_params_rf[<span class="st">&quot;criterion&quot;</span>])</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>    rf.fit(X_train, y_train.values.ravel())</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>    y_pred_rf <span class="op">=</span> rf.predict(X_test)</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>    confusion_matrix_dt <span class="op">=</span> sklearn.metrics.confusion_matrix(y_test, y_pred_dt)</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>    confusion_matrix_rf <span class="op">=</span> sklearn.metrics.confusion_matrix(y_test, y_pred_rf)</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>    dt_results.append((np.<span class="bu">sum</span>(confusion_matrix_dt.diagonal()) <span class="op">/</span> y_test.shape[<span class="dv">0</span>]) <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>    rf_results.append((np.<span class="bu">sum</span>(confusion_matrix_rf.diagonal()) <span class="op">/</span> y_test.shape[<span class="dv">0</span>]) <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>plt.plot(indexes, dt_results, label<span class="op">=</span><span class="st">&quot;DT results&quot;</span>)</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>plt.plot(indexes, rf_results, label<span class="op">=</span><span class="st">&quot;RF results&quot;</span>)</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;Sample&quot;</span>)</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;Accuracy on Test Data in %&quot;</span>)</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Accuracy Comparison Between DT and RF on Randomly Generated Models&quot;</span>)</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-14-output-1.png" width="623" height="449" /></p>
</div>
</div>
</section>
<section id="results-and-conclusions" class="level2">
<h2>Results and Conclusions</h2>
<p>After doing simple experimentation with these models, I have found that, on average, the RandomForestClassifier outpreforms just a singular DecisionTreeClassifier. There are several advantages to having a forest of DecisionTrees rather than a singular tree:</p>
<ul>
<li><p>More generalizability due to the ensemble approach to this problem</p></li>
<li><p>Limits overfitting compared to a DT</p></li>
<li><p>DT has high variance and instability, so having a forest of them results in a more collective approach.</p></li>
</ul>
<p>Though there is more resource complexity with a forest, the benefits of using that over a DT is worth the tradeoff.</p>
<!-- -->
<div class="quarto-embedded-source-code">
<div class="sourceCode" id="cb20" data-shortcodes="false"><pre class="sourceCode markdown"><code class="sourceCode markdown"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co">  echo: fenced</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> &quot;Comparing Decision Tree and Random Forest Classifier&quot;</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="an">toc-title:</span><span class="co"> &quot;Table of contents&quot;</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="co">    embed-resources: true</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="co">    code-copy: true</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="co">    code-link: true</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a><span class="co">    theme:</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a><span class="co">      dark: darkly</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a><span class="co">      light: flatly</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a><span class="co">  docx: default</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a><span class="co">  ipynb: default</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a><span class="co">  gfm: default</span></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- title: &quot;Comparing Decision Tree and Random Forest Classifier Performance&quot;</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a><span class="co">format:</span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a><span class="co">jupyter: python3 --&gt;</span></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>**Author: Daniel Hassler**</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a><span class="fu">## Sample Data Used in Classification</span></span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>To compare a DecisionTree and a RandomForestClassifier, the first step I took was to gather some data and run some visualizations and analysis. Through Kaggle, I was able to obtain a small dataset on person features and their BMI (Body Mass Index) data. The data consists of just around 400 samples with features: gender, height, and weight, and the goal is to predict BMI. </span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split, cross_val_score, StratifiedKFold, GridSearchCV</span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier, plot_tree</span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> tree</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(<span class="st">&quot;./datasets/bmi_train.csv&quot;</span>)</span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a>category_mapping <span class="op">=</span> {<span class="st">&#39;Male&#39;</span>: <span class="dv">0</span>, <span class="st">&#39;Female&#39;</span>: <span class="dv">1</span>}</span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a>data[<span class="st">&#39;Gender_Encoded&#39;</span>] <span class="op">=</span> data[<span class="st">&#39;Gender&#39;</span>].<span class="bu">map</span>(category_mapping) <span class="co"># converts categorical data to numeric data.</span></span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data.drop([<span class="st">&#39;Gender&#39;</span>,<span class="st">&#39;Index&#39;</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data.drop([<span class="st">&#39;Gender&#39;</span>, <span class="st">&#39;Gender_Encoded&#39;</span>, <span class="st">&#39;Height&#39;</span>, <span class="st">&#39;Weight&#39;</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;All X shape: &quot;</span>, X.shape)</span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;All y shape: &quot;</span>, y.shape)</span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a>validation_data <span class="op">=</span> pd.read_csv(<span class="st">&quot;./datasets/bmi_validation.csv&quot;</span>)</span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Validation data shape: &quot;</span>, validation_data.shape)</span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;X_train shape: &quot;</span>, X_train.shape)</span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;y_train shape: &quot;</span>, y_train.shape)</span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;X_test shape: &quot;</span>, X_test.shape)</span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;y_test shape: &quot;</span>, y_test.shape)</span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a>In the above code snippet, I first populated my data into a Pandas dataframe and then split up the data into a &quot;training&quot; and &quot;testing&quot; datasets. I decided to go with an 80/20% split between train and test (with its corresponding labels), as that seems to be the most standard approach in the industry. The significant benefit here is that I possess labeled data on both sets, a challenge in practice. This enables me to make comparisons between predictions and outcomes on my data, eliminating the need to procure any additional &quot;test&quot; data.</span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-67"><a href="#cb20-67" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb20-68"><a href="#cb20-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-69"><a href="#cb20-69" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb20-70"><a href="#cb20-70" aria-hidden="true" tabindex="-1"></a><span class="co"># Class imbalance, more obesity.</span></span>
<span id="cb20-71"><a href="#cb20-71" aria-hidden="true" tabindex="-1"></a>unique_values, counts <span class="op">=</span> np.unique(y, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-72"><a href="#cb20-72" aria-hidden="true" tabindex="-1"></a>plt.bar(unique_values, counts)</span>
<span id="cb20-73"><a href="#cb20-73" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;BMI Classes in the Entire Dataset&quot;</span>)</span>
<span id="cb20-74"><a href="#cb20-74" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;BMI Class&quot;</span>)</span>
<span id="cb20-75"><a href="#cb20-75" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;Occurences in Entire Dataset&quot;</span>)</span>
<span id="cb20-76"><a href="#cb20-76" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-77"><a href="#cb20-77" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-78"><a href="#cb20-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-79"><a href="#cb20-79" aria-hidden="true" tabindex="-1"></a>The labels are all discrete and sequential, consisting of whole numbers between 0 and 5, further enforcing my intuition for using a classifier approach. A &quot;0&quot; in my case represents someone with an **exeptionally low** BMI, whereas a &quot;5&quot; depicts an **exceptionally high** BMI. Based on the distribution of the data, there appears to be a huge class imbalance, heavily favoring the amount of **exceptionally high** instances in the dataset; this was something I needed to keep in mind when building the classifiers for this dataset.</span>
<span id="cb20-80"><a href="#cb20-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-81"><a href="#cb20-81" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb20-82"><a href="#cb20-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-83"><a href="#cb20-83" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb20-84"><a href="#cb20-84" aria-hidden="true" tabindex="-1"></a>correlation_matrix <span class="op">=</span> data.corr()</span>
<span id="cb20-85"><a href="#cb20-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-86"><a href="#cb20-86" aria-hidden="true" tabindex="-1"></a><span class="co"># Display a heatmap of the correlation matrix</span></span>
<span id="cb20-87"><a href="#cb20-87" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb20-88"><a href="#cb20-88" aria-hidden="true" tabindex="-1"></a>sns.heatmap(correlation_matrix, annot<span class="op">=</span><span class="va">True</span>, cmap<span class="op">=</span><span class="st">&#39;coolwarm&#39;</span>, center<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb20-89"><a href="#cb20-89" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Correlation Heatmap&#39;</span>)</span>
<span id="cb20-90"><a href="#cb20-90" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-91"><a href="#cb20-91" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-92"><a href="#cb20-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-93"><a href="#cb20-93" aria-hidden="true" tabindex="-1"></a>The correlation matrix depicts the correlation between features (height, weight, gender, BMI) in the dataset. It uses the pearson&#39;s correlation coefficient to compute this:</span>
<span id="cb20-94"><a href="#cb20-94" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-95"><a href="#cb20-95" aria-hidden="true" tabindex="-1"></a> r =</span>
<span id="cb20-96"><a href="#cb20-96" aria-hidden="true" tabindex="-1"></a>  \frac{ \sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y}) }{</span>
<span id="cb20-97"><a href="#cb20-97" aria-hidden="true" tabindex="-1"></a>        \sqrt{\sum_{i=1}^{n}(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^{n}(y_i-\bar{y})^2}}</span>
<span id="cb20-98"><a href="#cb20-98" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-99"><a href="#cb20-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-100"><a href="#cb20-100" aria-hidden="true" tabindex="-1"></a>Based on the features presented, most are not correlated strongly, but there is a glaring strong correlation between weight and BMI. </span>
<span id="cb20-101"><a href="#cb20-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-102"><a href="#cb20-102" aria-hidden="true" tabindex="-1"></a><span class="fu">## Building the Decision Tree Model</span></span>
<span id="cb20-103"><a href="#cb20-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-104"><a href="#cb20-104" aria-hidden="true" tabindex="-1"></a>In order to start the model building process, I decided to tune the hyperparamters first by running a <span class="in">`GridSearch`</span></span>
<span id="cb20-105"><a href="#cb20-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-106"><a href="#cb20-106" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb20-107"><a href="#cb20-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-108"><a href="#cb20-108" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb20-109"><a href="#cb20-109" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb20-110"><a href="#cb20-110" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;max_depth&#39;</span>: [i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>, <span class="dv">10</span>)],</span>
<span id="cb20-111"><a href="#cb20-111" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;min_samples_leaf&#39;</span>: [<span class="dv">2</span> <span class="op">**</span> i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">4</span>)],</span>
<span id="cb20-112"><a href="#cb20-112" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;criterion&#39;</span>: [<span class="st">&quot;entropy&quot;</span>, <span class="st">&quot;gini&quot;</span>]</span>
<span id="cb20-113"><a href="#cb20-113" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-114"><a href="#cb20-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-115"><a href="#cb20-115" aria-hidden="true" tabindex="-1"></a>dt <span class="op">=</span> DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb20-116"><a href="#cb20-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-117"><a href="#cb20-117" aria-hidden="true" tabindex="-1"></a>grid_search_dt <span class="op">=</span> GridSearchCV(dt, param_grid, cv<span class="op">=</span>StratifiedKFold(n_splits<span class="op">=</span><span class="dv">5</span>), scoring<span class="op">=</span><span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb20-118"><a href="#cb20-118" aria-hidden="true" tabindex="-1"></a>grid_search_dt.fit(X_train, y_train)</span>
<span id="cb20-119"><a href="#cb20-119" aria-hidden="true" tabindex="-1"></a>best_params_dt <span class="op">=</span> grid_search_dt.best_params_</span>
<span id="cb20-120"><a href="#cb20-120" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Best Hyperparameters:&quot;</span>, best_params_dt)</span>
<span id="cb20-121"><a href="#cb20-121" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Best Score:&quot;</span>, grid_search_dt.best_score_)</span>
<span id="cb20-122"><a href="#cb20-122" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-123"><a href="#cb20-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-124"><a href="#cb20-124" aria-hidden="true" tabindex="-1"></a>I recognized that <span class="in">`max_depth`</span> was an important hyperparameter for the DecisionTree (DT), as the depth of the tree heavily influences overfitting, but other hyperparameters are important as well, such as:</span>
<span id="cb20-125"><a href="#cb20-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-126"><a href="#cb20-126" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`min_samples_leaf`</span>: the minimum amount of samples needed in a leaf node of the DT. For example, when min_samples_leaf is set to 10, that means a node won&#39;t split if it has fewer than 10 samples. When this number is higher, the model can create a more generalized tree, although, when the number is smaller, it&#39;ll create more specific splits, resulting in a more complex tree (more potential for overfitting).</span>
<span id="cb20-127"><a href="#cb20-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-128"><a href="#cb20-128" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`criterion`</span>: this hyperparameter chooses whether to use entropy or Gini index as a way to calculate dissimilarity in a node. I found that in most cases, entropy outpreformed the Gini index.</span>
<span id="cb20-129"><a href="#cb20-129" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-130"><a href="#cb20-130" aria-hidden="true" tabindex="-1"></a>Entropy(C) = -\sum_{c=1}^Cp(c)\log(p(c))</span>
<span id="cb20-131"><a href="#cb20-131" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-132"><a href="#cb20-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-133"><a href="#cb20-133" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-134"><a href="#cb20-134" aria-hidden="true" tabindex="-1"></a>Gini(C) = 1 - \sum_{c=1}^Cp(c)^2</span>
<span id="cb20-135"><a href="#cb20-135" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-136"><a href="#cb20-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-137"><a href="#cb20-137" aria-hidden="true" tabindex="-1"></a>Now that I&#39;ve determined the necessary hyperparameters for this classifier, I initialize the <span class="in">`GridSearchCV`</span> object to analyze every combination of the above hyperparameters. Within its search, it goes through an important cross-validation step (cv) that splits the training data into multiple folds and iterates through each fold for each hyperparameter combination.</span>
<span id="cb20-138"><a href="#cb20-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-139"><a href="#cb20-139" aria-hidden="true" tabindex="-1"></a>There were a few options I could&#39;ve chose from for the cv parameter in <span class="in">`GridSearchCV`</span>, but in order to account for class imbalance like I stated earlier, I decided to go with a <span class="in">`StratifiedKFold`</span> cross-validator. StratifiedKFold accounts for class label imbalance by keeping an equal precentage of classes for training and testing represented in the dataset. Below is a picture representing this:</span>
<span id="cb20-140"><a href="#cb20-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-141"><a href="#cb20-141" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;img</span> <span class="er">style</span><span class="ot">=</span><span class="st">&quot;display: block;</span></span>
<span id="cb20-142"><a href="#cb20-142" aria-hidden="true" tabindex="-1"></a><span class="st">    margin-left: auto;</span></span>
<span id="cb20-143"><a href="#cb20-143" aria-hidden="true" tabindex="-1"></a><span class="st">    margin-right: auto;&quot;</span></span>
<span id="cb20-144"><a href="#cb20-144" aria-hidden="true" tabindex="-1"></a><span class="ot">    height=</span><span class="st">&quot;300&quot;</span> <span class="er">width</span><span class="ot">=</span><span class="st">&quot;300&quot;</span> <span class="er">src</span><span class="ot">=</span><span class="st">&quot;https://amueller.github.io/aml/_images/stratified_cv.png&quot;</span><span class="kw">&gt;&lt;/img&gt;</span></span>
<span id="cb20-145"><a href="#cb20-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-146"><a href="#cb20-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-147"><a href="#cb20-147" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb20-148"><a href="#cb20-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-149"><a href="#cb20-149" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb20-150"><a href="#cb20-150" aria-hidden="true" tabindex="-1"></a>dt <span class="op">=</span> DecisionTreeClassifier(max_depth<span class="op">=</span>best_params_dt[<span class="st">&quot;max_depth&quot;</span>], min_samples_leaf<span class="op">=</span>best_params_dt[<span class="st">&quot;min_samples_leaf&quot;</span>], criterion<span class="op">=</span>best_params_dt[<span class="st">&quot;criterion&quot;</span>])</span>
<span id="cb20-151"><a href="#cb20-151" aria-hidden="true" tabindex="-1"></a>dt.fit(X_train, y_train)</span>
<span id="cb20-152"><a href="#cb20-152" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> dt.predict(X_test)</span>
<span id="cb20-153"><a href="#cb20-153" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-154"><a href="#cb20-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-155"><a href="#cb20-155" aria-hidden="true" tabindex="-1"></a>I then created a <span class="in">`DecisionTreeClassifier`</span> with the &#39;best&#39; tuned hyperparameters from the above grid search and populated the <span class="in">`y_pred`</span> array with the predictions from the test dataset. After that, I plotted the tree out using Sklearn&#39;s plot_tree method.</span>
<span id="cb20-156"><a href="#cb20-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-157"><a href="#cb20-157" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb20-158"><a href="#cb20-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-159"><a href="#cb20-159" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb20-160"><a href="#cb20-160" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">40</span>, <span class="dv">40</span>))</span>
<span id="cb20-161"><a href="#cb20-161" aria-hidden="true" tabindex="-1"></a>plot_tree(dt, feature_names<span class="op">=</span>X_train.columns.tolist(), class_names<span class="op">=</span>[<span class="st">&#39;0&#39;</span>, <span class="st">&#39;1&#39;</span>, <span class="st">&#39;2&#39;</span>,<span class="st">&#39;3&#39;</span>,<span class="st">&#39;4&#39;</span>,<span class="st">&#39;5&#39;</span>], filled<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-162"><a href="#cb20-162" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-163"><a href="#cb20-163" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-164"><a href="#cb20-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-165"><a href="#cb20-165" aria-hidden="true" tabindex="-1"></a>After plotting the tree, I created a confusion matrix, showing where my predictions fell. Currently, the model sits around 75-86% accurate due to the above hyperparameter values and the randomly generated tree with those hyperparameter values. Not bad for a small dataset with class imbalance. </span>
<span id="cb20-166"><a href="#cb20-166" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb20-167"><a href="#cb20-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-168"><a href="#cb20-168" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb20-169"><a href="#cb20-169" aria-hidden="true" tabindex="-1"></a>confusion_matrix <span class="op">=</span> sklearn.metrics.confusion_matrix(y_test, y_pred)</span>
<span id="cb20-170"><a href="#cb20-170" aria-hidden="true" tabindex="-1"></a>conf_df <span class="op">=</span> pd.DataFrame(confusion_matrix, index<span class="op">=</span>[<span class="ss">f&quot;</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">&quot;</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">6</span>)], columns<span class="op">=</span>[<span class="ss">f&quot;</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">&quot;</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">6</span>)])</span>
<span id="cb20-171"><a href="#cb20-171" aria-hidden="true" tabindex="-1"></a>heatmap <span class="op">=</span> sns.heatmap(conf_df, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">&quot;d&quot;</span>, linewidths<span class="op">=</span><span class="fl">0.35</span>, cmap<span class="op">=</span><span class="st">&quot;YlGnBu&quot;</span>)</span>
<span id="cb20-172"><a href="#cb20-172" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f&quot;Model Predictions With </span><span class="sc">{</span>(np.<span class="bu">sum</span>(confusion_matrix.diagonal()) <span class="op">/</span> y_test.shape[<span class="dv">0</span>]) <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">% Accuracy&quot;</span>)</span>
<span id="cb20-173"><a href="#cb20-173" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-174"><a href="#cb20-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-175"><a href="#cb20-175" aria-hidden="true" tabindex="-1"></a><span class="fu">## RandomForestClassifer (Ensemble approach)</span></span>
<span id="cb20-176"><a href="#cb20-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-177"><a href="#cb20-177" aria-hidden="true" tabindex="-1"></a>As above with the <span class="in">`DecisionTreeClassifer`</span>, I first started to implement the <span class="in">`RandomForestClassifier`</span> by tuning the hyperparameter values. Since a RandomForest is just a collection of DecisionTrees, RandomForestClassifiers, like a <span class="in">`DecisionTreeClassifier`</span>, have mostly the same hyperparameters, but the <span class="in">`RandomForestClassifier`</span> has an extra one for the amount of DecisionTrees that should be included in the forest (<span class="in">`n_estimators`</span>).</span>
<span id="cb20-178"><a href="#cb20-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-179"><a href="#cb20-179" aria-hidden="true" tabindex="-1"></a>Though this step wasn&#39;t as necessary, since I already did the hyperparameter tuning part for the DecisionTree, but I decided to include it again for the RandomForest with the number of estimators.</span>
<span id="cb20-180"><a href="#cb20-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-181"><a href="#cb20-181" aria-hidden="true" tabindex="-1"></a>It is important to note that the <span class="in">`n_estimators`</span> hyperparameter won&#39;t cause the model to overfit. In fact, it actually does better at generalization when increasing the number of estimators due to the diversity of opinions the model presents for each unique DecisionTree. The only way overfitting can happen in a RandomForest depends on how the underlying DecisionTrees are set up, not the quantity of them.</span>
<span id="cb20-182"><a href="#cb20-182" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb20-183"><a href="#cb20-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-184"><a href="#cb20-184" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb20-185"><a href="#cb20-185" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb20-186"><a href="#cb20-186" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;max_depth&#39;</span>: [i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>, <span class="dv">10</span>)],</span>
<span id="cb20-187"><a href="#cb20-187" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;min_samples_leaf&#39;</span>: [<span class="dv">2</span> <span class="op">**</span> i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">4</span>)],</span>
<span id="cb20-188"><a href="#cb20-188" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;criterion&#39;</span>: [<span class="st">&quot;entropy&quot;</span>, <span class="st">&quot;gini&quot;</span>]</span>
<span id="cb20-189"><a href="#cb20-189" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-190"><a href="#cb20-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-191"><a href="#cb20-191" aria-hidden="true" tabindex="-1"></a>rf <span class="op">=</span> RandomForestClassifier(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb20-192"><a href="#cb20-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-193"><a href="#cb20-193" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y_train.shape)</span>
<span id="cb20-194"><a href="#cb20-194" aria-hidden="true" tabindex="-1"></a>grid_search_rf <span class="op">=</span> GridSearchCV(rf, param_grid, cv<span class="op">=</span>StratifiedKFold(n_splits<span class="op">=</span><span class="dv">5</span>), scoring<span class="op">=</span><span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb20-195"><a href="#cb20-195" aria-hidden="true" tabindex="-1"></a>grid_search_rf.fit(X_train, y_train.values.ravel())</span>
<span id="cb20-196"><a href="#cb20-196" aria-hidden="true" tabindex="-1"></a>best_params_rf <span class="op">=</span> grid_search_rf.best_params_</span>
<span id="cb20-197"><a href="#cb20-197" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Best Hyperparameters:&quot;</span>, best_params_rf)</span>
<span id="cb20-198"><a href="#cb20-198" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Best Score:&quot;</span>, grid_search_rf.best_score_)</span>
<span id="cb20-199"><a href="#cb20-199" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-200"><a href="#cb20-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-201"><a href="#cb20-201" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb20-202"><a href="#cb20-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-203"><a href="#cb20-203" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb20-204"><a href="#cb20-204" aria-hidden="true" tabindex="-1"></a>rf <span class="op">=</span> RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">300</span>, </span>
<span id="cb20-205"><a href="#cb20-205" aria-hidden="true" tabindex="-1"></a>                            max_depth<span class="op">=</span>best_params_rf[<span class="st">&quot;max_depth&quot;</span>], </span>
<span id="cb20-206"><a href="#cb20-206" aria-hidden="true" tabindex="-1"></a>                            min_samples_leaf<span class="op">=</span>best_params_rf[<span class="st">&quot;min_samples_leaf&quot;</span>],</span>
<span id="cb20-207"><a href="#cb20-207" aria-hidden="true" tabindex="-1"></a>                            criterion<span class="op">=</span>best_params_rf[<span class="st">&quot;criterion&quot;</span>])</span>
<span id="cb20-208"><a href="#cb20-208" aria-hidden="true" tabindex="-1"></a>rf.fit(X_train, y_train.values.ravel())</span>
<span id="cb20-209"><a href="#cb20-209" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> rf.predict(X_test)</span>
<span id="cb20-210"><a href="#cb20-210" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-211"><a href="#cb20-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-212"><a href="#cb20-212" aria-hidden="true" tabindex="-1"></a>The above code snippet creates the <span class="in">`RandomForestClassifier`</span> with the same hyperparameters as the DecisionTree, in addition to the number of estimators (number of decision trees in the forest), trains the classifier, then stores a prediction array.</span>
<span id="cb20-213"><a href="#cb20-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-214"><a href="#cb20-214" aria-hidden="true" tabindex="-1"></a>Here is a visualizaiton of a subset of DecisionTrees in this RandomForest:</span>
<span id="cb20-215"><a href="#cb20-215" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb20-216"><a href="#cb20-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-217"><a href="#cb20-217" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb20-218"><a href="#cb20-218" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(nrows <span class="op">=</span> <span class="dv">1</span>,ncols <span class="op">=</span> <span class="dv">5</span>,figsize <span class="op">=</span> (<span class="dv">10</span>,<span class="dv">3</span>), dpi<span class="op">=</span><span class="dv">250</span>)</span>
<span id="cb20-219"><a href="#cb20-219" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> index <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb20-220"><a href="#cb20-220" aria-hidden="true" tabindex="-1"></a>        tree.plot_tree(rf.estimators_[index],</span>
<span id="cb20-221"><a href="#cb20-221" aria-hidden="true" tabindex="-1"></a>                    feature_names <span class="op">=</span> X_train.columns.tolist(), </span>
<span id="cb20-222"><a href="#cb20-222" aria-hidden="true" tabindex="-1"></a>                    class_names<span class="op">=</span> [<span class="ss">f&quot;</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">&quot;</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">6</span>)],</span>
<span id="cb20-223"><a href="#cb20-223" aria-hidden="true" tabindex="-1"></a>                    filled <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb20-224"><a href="#cb20-224" aria-hidden="true" tabindex="-1"></a>                    ax <span class="op">=</span> axes[index])</span>
<span id="cb20-225"><a href="#cb20-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-226"><a href="#cb20-226" aria-hidden="true" tabindex="-1"></a>        axes[index].set_title(<span class="st">&#39;Estimator: &#39;</span> <span class="op">+</span> <span class="bu">str</span>(index <span class="op">+</span> <span class="dv">1</span>), fontsize <span class="op">=</span> <span class="dv">10</span>)</span>
<span id="cb20-227"><a href="#cb20-227" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb20-228"><a href="#cb20-228" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-229"><a href="#cb20-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-230"><a href="#cb20-230" aria-hidden="true" tabindex="-1"></a>After running the model, I checked the accuracy output of the prediction array and found that the RandomForestClassifier was able to increase the accuracy of the predictions by a considerable amount on average.</span>
<span id="cb20-231"><a href="#cb20-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-232"><a href="#cb20-232" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb20-233"><a href="#cb20-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-234"><a href="#cb20-234" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb20-235"><a href="#cb20-235" aria-hidden="true" tabindex="-1"></a>confusion_matrix <span class="op">=</span> sklearn.metrics.confusion_matrix(y_test, y_pred)</span>
<span id="cb20-236"><a href="#cb20-236" aria-hidden="true" tabindex="-1"></a>conf_df <span class="op">=</span> pd.DataFrame(confusion_matrix, index<span class="op">=</span>[<span class="ss">f&quot;</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">&quot;</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">6</span>)], columns<span class="op">=</span>[<span class="ss">f&quot;</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">&quot;</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">6</span>)])</span>
<span id="cb20-237"><a href="#cb20-237" aria-hidden="true" tabindex="-1"></a>heatmap <span class="op">=</span> sns.heatmap(conf_df, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">&quot;d&quot;</span>, linewidths<span class="op">=</span><span class="fl">0.35</span>, cmap<span class="op">=</span><span class="st">&quot;YlGnBu&quot;</span>)</span>
<span id="cb20-238"><a href="#cb20-238" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f&quot;Model Predictions With </span><span class="sc">{</span>(np.<span class="bu">sum</span>(confusion_matrix.diagonal()) <span class="op">/</span> y_test.shape[<span class="dv">0</span>]) <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">% Accuracy&quot;</span>)</span>
<span id="cb20-239"><a href="#cb20-239" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-240"><a href="#cb20-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-241"><a href="#cb20-241" aria-hidden="true" tabindex="-1"></a>Finally, I decided to calculate the accuracy preformance on multiple samples of RandomForestClassifiers and DecisionTrees at the same time and plot them out in a line chart.</span>
<span id="cb20-242"><a href="#cb20-242" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb20-243"><a href="#cb20-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-244"><a href="#cb20-244" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb20-245"><a href="#cb20-245" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;&#39;&#39;</span></span>
<span id="cb20-246"><a href="#cb20-246" aria-hidden="true" tabindex="-1"></a><span class="co">Plot a graph that compares the two models, randomly generated with tuned hyperparameter models</span></span>
<span id="cb20-247"><a href="#cb20-247" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;&#39;&#39;</span></span>
<span id="cb20-248"><a href="#cb20-248" aria-hidden="true" tabindex="-1"></a>dt_results <span class="op">=</span> []</span>
<span id="cb20-249"><a href="#cb20-249" aria-hidden="true" tabindex="-1"></a>rf_results <span class="op">=</span> []</span>
<span id="cb20-250"><a href="#cb20-250" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb20-251"><a href="#cb20-251" aria-hidden="true" tabindex="-1"></a>indexes <span class="op">=</span> [i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_samples)]</span>
<span id="cb20-252"><a href="#cb20-252" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> indexes:</span>
<span id="cb20-253"><a href="#cb20-253" aria-hidden="true" tabindex="-1"></a>    dt <span class="op">=</span> DecisionTreeClassifier(max_depth<span class="op">=</span>best_params_dt[<span class="st">&quot;max_depth&quot;</span>], </span>
<span id="cb20-254"><a href="#cb20-254" aria-hidden="true" tabindex="-1"></a>                   min_samples_leaf<span class="op">=</span>best_params_dt[<span class="st">&quot;min_samples_leaf&quot;</span>], criterion<span class="op">=</span>best_params_dt[<span class="st">&quot;criterion&quot;</span>])</span>
<span id="cb20-255"><a href="#cb20-255" aria-hidden="true" tabindex="-1"></a>    dt.fit(X_train, y_train)</span>
<span id="cb20-256"><a href="#cb20-256" aria-hidden="true" tabindex="-1"></a>    y_pred_dt <span class="op">=</span> dt.predict(X_test)</span>
<span id="cb20-257"><a href="#cb20-257" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-258"><a href="#cb20-258" aria-hidden="true" tabindex="-1"></a>    rf <span class="op">=</span> RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">300</span>, </span>
<span id="cb20-259"><a href="#cb20-259" aria-hidden="true" tabindex="-1"></a>                            max_depth<span class="op">=</span>best_params_rf[<span class="st">&quot;max_depth&quot;</span>], </span>
<span id="cb20-260"><a href="#cb20-260" aria-hidden="true" tabindex="-1"></a>                            min_samples_leaf<span class="op">=</span>best_params_rf[<span class="st">&quot;min_samples_leaf&quot;</span>],</span>
<span id="cb20-261"><a href="#cb20-261" aria-hidden="true" tabindex="-1"></a>                            criterion<span class="op">=</span>best_params_rf[<span class="st">&quot;criterion&quot;</span>])</span>
<span id="cb20-262"><a href="#cb20-262" aria-hidden="true" tabindex="-1"></a>    rf.fit(X_train, y_train.values.ravel())</span>
<span id="cb20-263"><a href="#cb20-263" aria-hidden="true" tabindex="-1"></a>    y_pred_rf <span class="op">=</span> rf.predict(X_test)</span>
<span id="cb20-264"><a href="#cb20-264" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-265"><a href="#cb20-265" aria-hidden="true" tabindex="-1"></a>    confusion_matrix_dt <span class="op">=</span> sklearn.metrics.confusion_matrix(y_test, y_pred_dt)</span>
<span id="cb20-266"><a href="#cb20-266" aria-hidden="true" tabindex="-1"></a>    confusion_matrix_rf <span class="op">=</span> sklearn.metrics.confusion_matrix(y_test, y_pred_rf)</span>
<span id="cb20-267"><a href="#cb20-267" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-268"><a href="#cb20-268" aria-hidden="true" tabindex="-1"></a>    dt_results.append((np.<span class="bu">sum</span>(confusion_matrix_dt.diagonal()) <span class="op">/</span> y_test.shape[<span class="dv">0</span>]) <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb20-269"><a href="#cb20-269" aria-hidden="true" tabindex="-1"></a>    rf_results.append((np.<span class="bu">sum</span>(confusion_matrix_rf.diagonal()) <span class="op">/</span> y_test.shape[<span class="dv">0</span>]) <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb20-270"><a href="#cb20-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-271"><a href="#cb20-271" aria-hidden="true" tabindex="-1"></a>plt.plot(indexes, dt_results, label<span class="op">=</span><span class="st">&quot;DT results&quot;</span>)</span>
<span id="cb20-272"><a href="#cb20-272" aria-hidden="true" tabindex="-1"></a>plt.plot(indexes, rf_results, label<span class="op">=</span><span class="st">&quot;RF results&quot;</span>)</span>
<span id="cb20-273"><a href="#cb20-273" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;Sample&quot;</span>)</span>
<span id="cb20-274"><a href="#cb20-274" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;Accuracy on Test Data in %&quot;</span>)</span>
<span id="cb20-275"><a href="#cb20-275" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Accuracy Comparison Between DT and RF on Randomly Generated Models&quot;</span>)</span>
<span id="cb20-276"><a href="#cb20-276" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb20-277"><a href="#cb20-277" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-278"><a href="#cb20-278" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-279"><a href="#cb20-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-280"><a href="#cb20-280" aria-hidden="true" tabindex="-1"></a><span class="fu">## Results and Conclusions</span></span>
<span id="cb20-281"><a href="#cb20-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-282"><a href="#cb20-282" aria-hidden="true" tabindex="-1"></a>After doing simple experimentation with these models, I have found that, on average, the RandomForestClassifier outpreforms just a singular DecisionTreeClassifier. There are several advantages to having a forest of DecisionTrees rather than a singular tree:</span>
<span id="cb20-283"><a href="#cb20-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-284"><a href="#cb20-284" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>More generalizability due to the ensemble approach to this problem</span>
<span id="cb20-285"><a href="#cb20-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-286"><a href="#cb20-286" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Limits overfitting compared to a DT</span>
<span id="cb20-287"><a href="#cb20-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-288"><a href="#cb20-288" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>DT has high variance and instability, so having a forest of them results in a more collective approach.</span>
<span id="cb20-289"><a href="#cb20-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-290"><a href="#cb20-290" aria-hidden="true" tabindex="-1"></a>Though there is more resource complexity with a forest, the benefits of using that over a DT is worth the tradeoff. </span></code></pre></div>
</div>
</section>

</main>
<!-- /main column -->
<script id = "quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'alternate';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->

</body>

</html>