[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Daniel Hassler Machine Learning Blogs",
    "section": "",
    "text": "Comparing Multiple Linear and Non-Linear Regression on MLB Data\n\n\n\n\n\n\n\ncode\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2023\n\n\nDaniel Hassler\n\n\n\n\n\n\n  \n\n\n\n\nSpotify Recommendation System With Clustering\n\n\n\n\n\n\n\ncode\n\n\nclustering\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2023\n\n\nDaniel Hassler\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory with Naive Bayes Application\n\n\n\n\n\n\n\ncode\n\n\nprobability\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2023\n\n\nDaniel Hassler\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly Detection\n\n\n\n\n\n\n\ncode\n\n\nanomaly\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nDaniel Hassler\n\n\n\n\n\n\n  \n\n\n\n\nComparing Decision Tree and Random Forest Classifier\n\n\n\n\n\n\n\ncode\n\n\nclassification\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2023\n\n\nDaniel Hassler\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "RegressionBlog/index.html",
    "href": "RegressionBlog/index.html",
    "title": "Comparing Multiple Linear and Non-Linear Regression on MLB Data",
    "section": "",
    "text": "Author: Daniel Hassler\n```{python}\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.svm import SVR\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n```"
  },
  {
    "objectID": "RegressionBlog/index.html#data",
    "href": "RegressionBlog/index.html#data",
    "title": "Comparing Multiple Linear and Non-Linear Regression on MLB Data",
    "section": "Data",
    "text": "Data\nWith Kaggle, I was able to find an MLB (Major League Baseball) dataset consisting of player data from the 1986-1987 seasons (https://www.kaggle.com/datasets/mathchi/hitters-baseball-data). The data itself has many features consisting of individual stats for the season, cumulative career stats, fielding stats, and salary. In total, there are 20 features with 322 entries before preprocessing.\nThe goal of this notebook is to showcase linear and non-linear regression as a way of predicting salary (in thousands), a continuous variable, for my dataset. But before I run through the regression process, I have to clean the data first and figure out correlations.\nOne negative influence on regression models is collinearity in the feature space. Collinearity is problematic for several reasons, including overfitting, interpretability, and inefficiency. When there are many features that are highly correlated, this will create a strong negative impact on the performance. So before plugging into the model, I analyzed the correlation matrix consisting of correlations between all the features.\n\n```{python}\ndf = pd.read_csv(\"./Hitters.csv\")\nnum_cols = [col for col in df.columns if df[col].dtypes != \"O\"]\ncorr = df[num_cols].corr()\nsns.set(rc={'figure.figsize': (15, 15)})\nsns.heatmap(corr, cmap=\"RdBu\", annot=True)\nplt.show()\n```\n\n\n\n\nBased on the above correlation matrix, I can see there’s a high correlation between all the individual season player stats (AtBat, Hits, HmRun, Runs, RBI, and Walks), cumulative player stats (CAtBat, CHits, CHmRun, CRuns, CRBI, Years, and CWalks), and finally some fielding stats (Assists, Errors).\nOne way to remove these strong correlations is to run a dimensionality reduction technique. In this case, I will be using PCA (principal component analysis) seperately on those three highly correlated areas: individual stats, cumulative stats, and fielding.\n\n```{python}\npca_custat = PCA(n_components=1)\npca_indstat = PCA(n_components=1)\npca_fieldstat = PCA(n_components=1)\n\ncustat_df = df[[\"CAtBat\", \"CHits\", \"CHmRun\", \"CRuns\", \"CRBI\", \"CWalks\"]]\nindstat_df = df[[\"AtBat\", \"Hits\", \"HmRun\", \"Runs\", \"RBI\", \"Walks\"]]\nfieldstat_df = df[[\"Assists\", \"Errors\"]]\n\ncustat_df_pca = pca_custat.fit_transform(custat_df)\nindstat_df_pca = pca_indstat.fit_transform(indstat_df)\nfieldstat_df_pca = pca_fieldstat.fit_transform(fieldstat_df)\n\ndf_reduced = df.drop(columns=[\"AtBat\", \"Hits\", \"HmRun\", \"Runs\", \"RBI\", \"Walks\", \"CAtBat\", \"CHits\", \"CHmRun\", \"CRuns\", \"CRBI\", \"CWalks\", \"Assists\", \"Errors\"])\ndf_reduced = df_reduced.drop(columns=[\"League\", \"NewLeague\", \"Division\", \"Years\"])\ndf_reduced[\"I_PC1\"] = indstat_df_pca\ndf_reduced[\"C_PC2\"] = custat_df_pca\ndf_reduced[\"F_PC3\"] = fieldstat_df_pca\n\ndf_reduced.dropna(axis=0, inplace=True)\n```\n\n\n```{python}\nnum_cols = [col for col in df_reduced.columns if df_reduced[col].dtypes != \"O\"]\ncorr = df_reduced[num_cols].corr()\nsns.set(rc={'figure.figsize': (15, 15)})\nsns.heatmap(corr, cmap=\"RdBu\", annot=True)\nplt.show()\n```\n\n\n\n\nAfter applying PCA and removing other features, I’ve reduced the data from 20 columns to just 5 columns, which is important, as collinearity can negatively effect the performance of regression models. Apart from the collinearity effect, I decided to get rid of discretely labeled binary relationships (labels 0 or 1), as this makes the linear regression model more complex and can only negatively impact the performance. I also discarded years as a feature because it was highly correlated with the PCA of the cumulative stats (r=0.91).\n\nCreating the Training and Test Sets\nSince we’re trying to predict salary, I extract salary column from the df_reduced, storing it into labels array y, and dropping that column from the feature data. The X data feature space has dropped from 5 columns to 4 columns, so there are 4 total features. For splitting the data into test and train, I am using sklearn’s train_test_split().\n\n```{python}\ndf = df_reduced\ny = df[\"Salary\"]\nX = df.drop(columns=\"Salary\")\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n\nprint(f\"Salary STD: ${np.std(y) * 1000:,.2f}\")\nprint(f\"Salary Mean: ${np.mean(y) * 1000:,.2f}\")\nprint(f\"Salary Low: ${np.min(y) * 1000:,.2f}\")\nprint(f\"Salary High: ${np.max(y) * 1000:,.2f}\")\n```\n\nSalary STD: $450,260.22\nSalary Mean: $535,925.88\nSalary Low: $67,500.00\nSalary High: $2,460,000.00"
  },
  {
    "objectID": "RegressionBlog/index.html#multiple-linear-support-vector-regression-kernellinear",
    "href": "RegressionBlog/index.html#multiple-linear-support-vector-regression-kernellinear",
    "title": "Comparing Multiple Linear and Non-Linear Regression on MLB Data",
    "section": "Multiple Linear Support Vector Regression (kernel=“linear”)",
    "text": "Multiple Linear Support Vector Regression (kernel=“linear”)\nNow, I will test out multiple linear regression using Sklearn’s SVR (support vector regression) class from the SVM library. Before passing the data into the regression model, I scaled the data using StandardScaler() as this is important for faster computation with regression. For the hyperparameters, I toyed with different C values, which influences the degree of regularization applied to the SVR model. A smaller C value leads to a simpler model, but a larger C value would fit to the training data more closely, which can potentially overfit if the number is too high. For the C value on the linear kernel, my model showed good preformance at C=1.\n\n```{python}\nsc = StandardScaler()\n\nX_train_scaled = sc.fit_transform(X_train, y_train)\nX_test_scaled = sc.fit_transform(X_test, y_test)\n# pca_all = PCA(n_components=1)\n# X_train_scaled_pca = pca_all.fit_transform(X_train_scaled)\n# X_test_scaled_pca = pca_all.fit_transform(X_test_scaled)\n\nsvr_lin = SVR(kernel=\"linear\", C=1, gamma=\"auto\")\nsvr_lin.fit(X_train_scaled, y_train)\ny_pred = svr_lin.predict(X_test_scaled)\n```\n\n\nMultiple Linear Regression Visualization\n\n```{python}\nplt.figure(figsize=(16, 12))\nindependent_variables = X_train.columns\ndependent_variable = \"Salary\"\nX_test_numpy = X_test.to_numpy()\nfor i, col in enumerate(independent_variables, 1):\n    plt.subplot(3, 3, i)\n    sns.regplot(x=X_train[col],y=y_train,ci=None,color ='red')\n    sns.scatterplot(data=X_train, x=col, y=y_train, color='blue', label='Training Points')\n    sns.scatterplot(data=X_test, x=col, y=y_test, color='green', label='Testing Points')\n    sns.scatterplot(data=X_test, x=col, y=y_pred, color='red', label='Predicted Points')\n    plt.title(f'{dependent_variable} vs. {col}')\n    plt.xlabel(col)\n    plt.ylabel(dependent_variable)\n\nplt.tight_layout()\nplt.show()\n```\n\n\n\n\nHere is a visualization showing linear regression applied on all of the features in the feature space, which make up the overall prediction. For each feature, the red line represents the function applied, which in this case is linear because I’m using a linear kernel, and the points represent all the different datapoints in the dataset. X_train points are in blue, X_test points are in green with their actual label y_test, and predicted points are in red (the X_test dataset on the y_pred).\nBased on the above visualization, it appears that linear regression works very well for all the features, although in any case, outliers are a problem. It is also worth noting that changing the C value does change the results, so modifying that may improve preformance with more fine-tuning. Below, I have outputed some metrics on the model:\n\n```{python}\nresult_df = pd.DataFrame(columns=[\"id\", \"actual\", \"predicted\"])\n\nfor i, actual, predicted in zip(y_test.index, y_test, y_pred):\n    entry = [i, actual, predicted]\n    df_entry = pd.DataFrame(entry, index=[\"id\", \"actual\", \"predicted\"]).T\n    result_df = pd.concat((result_df, df_entry))\n#print(result_df)\ndifference = abs(result_df[\"actual\"] - result_df[\"predicted\"])\nprint(f\"Cumulative Difference: ${np.sum(difference) * 1000:,.2f}\")\nprint(f\"Min Difference: ${np.min(difference) * 1000:,.2f}\")\nprint(f\"Max Difference: ${np.max(difference) * 1000:,.2f}\")\nprint(f\"Average Difference: ${np.mean(difference) * 1000:,.2f}\")\nprint(f\"Std Difference: ${np.std(difference) * 1000:,.2f}\")\nprint(f\"Mean Squared Error: ${mean_squared_error(y_test, y_pred):,.2f}\")\n```\n\nCumulative Difference: $10,792,084.13\nMin Difference: $19,061.95\nMax Difference: $1,968,301.30\nAverage Difference: $269,802.10\nStd Difference: $361,894.28\nMean Squared Error: $203,760.65\n\n\nBased on this data, the total difference (sum of all the differences) between the actual and predicted outputs could be better, as the average difference between the labels is around 270 = $270,000. The MSE is a common metric used in these types of problems, and my MSE(mean squared error) score is around 205 ($205,000), which is a respectable MSE value for this dataset due to its high standard deviation at ($450,260.22).\n\n```{python}\nplt.figure(figsize=(8,8))\nresiduals = result_df['actual'] - result_df['predicted']\n\nplt.scatter(result_df['id'], residuals)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('ID')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.show()\n```\n\n\n\n\nAbove is a visualization showing how far the differences are for each value in the test set (the residuals). 0 means no difference between the actual and the predicted, any number below 0 means the predicted value was higher than the actual value, and any number above 0 means the predicted value was lower than the actual value."
  },
  {
    "objectID": "RegressionBlog/index.html#multiple-non-linear-support-vector-regression-kernelpoly",
    "href": "RegressionBlog/index.html#multiple-non-linear-support-vector-regression-kernelpoly",
    "title": "Comparing Multiple Linear and Non-Linear Regression on MLB Data",
    "section": "Multiple Non-Linear Support Vector Regression (kernel=“poly”)",
    "text": "Multiple Non-Linear Support Vector Regression (kernel=“poly”)\nNow I run SVR on a non-linear kernel and assess its comparison to a linear kernel. Since real world data has a lot of non-linearity, this comparison is worth attempting.\n\n```{python}\nsvr_poly = SVR(kernel=\"poly\", degree=2, C=75, gamma=\"scale\")\nsvr_poly.fit(X_train_scaled, y_train)\ny_pred = svr_poly.predict(X_test_scaled)\n\nresult_df = pd.DataFrame(columns=[\"id\", \"actual\", \"predicted\"])\n\nfor i, actual, predicted in zip(y_test.index, y_test, y_pred):\n    entry = [i, actual, predicted]\n    df_entry = pd.DataFrame(entry, index=[\"id\", \"actual\", \"predicted\"]).T\n    result_df = pd.concat((result_df, df_entry))\n    \n#print(result_df)\ndifference = abs(result_df[\"actual\"] - result_df[\"predicted\"])\nprint(f\"Cumulative Difference: ${np.sum(difference) * 1000:,.2f}\")\nprint(f\"Min Difference: ${np.min(difference) * 1000:,.2f}\")\nprint(f\"Max Difference: ${np.max(difference) * 1000:,.2f}\")\nprint(f\"Average Difference: ${np.mean(difference) * 1000:,.2f}\")\nprint(f\"Std Difference: ${np.std(difference) * 1000:,.2f}\")\nprint(f\"Mean Squared Error: ${mean_squared_error(y_test, y_pred):,.2f}\")\n```\n\nCumulative Difference: $13,385,640.30\nMin Difference: $798.03\nMax Difference: $1,581,505.51\nAverage Difference: $334,641.01\nStd Difference: $330,986.90\nMean Squared Error: $221,536.94\n\n\nBased on the model run through with a polynomial kernel, the results are overall noticeably worse than the linear kernel, but not by a whole lot. Although, it is worth noting that the C value is crucial in this result. I was trying to balance the trade-off between conforming to the function and simplicity. For the polynomial degree, I decide to go with 2, as 1 is linear and 3 didn’t preform as anticipated, as the function plotted didn’t represent some of the features as well as 2.\n\nMultiple Non-Linear Regression Visualization\n\n```{python}\nplt.figure(figsize=(16, 12))\nindependent_variables = X_train.columns\ndependent_variable = \"Salary\"\nX_test_numpy = X_test.to_numpy()\nfor i, col in enumerate(independent_variables, 1):\n    plt.subplot(3, 3, i)\n    sns.regplot(x=X_train[col],y=y_train,ci=None,color ='red', order=svr_poly.degree)\n    sns.scatterplot(data=X_train, x=col, y=y_train, color='blue', label='Training Points')\n    sns.scatterplot(data=X_test, x=col, y=y_test, color='green', label='Testing Points')\n    sns.scatterplot(data=X_test, x=col, y=y_pred, color='red', label='Predicted Points')\n    plt.title(f'{dependent_variable} vs. {col}')\n    plt.xlabel(col)\n    plt.ylabel(dependent_variable)\n\nplt.tight_layout()\nplt.show()\n```\n\n\n\n\nBased on the above visualzation, using a polynomial function with degree 2, shows interesting results. For the Salary vs PutOuts plot, the data plotted resembles a linear kernel, but in actuality it’s a very zoomed in polynomial kernel. The Salary vs I_PC1 showed a curve which I expected. It starts off at a peak and then lowers like a parabolic function (degree 2). The Salary vs C_PC2 plot is interesting in the sense that it’s a negative parabola; I would say this is not a fully representative curve as it seems to be fitting to the outlier at the end of the plot. Finally, the Salary vs F_PC3 plot seems to be similar to the first plot as it resembles more of a linear kernel.\n\n```{python}\nplt.figure(figsize=(8,8))\nresiduals = result_df['actual'] - result_df['predicted']\n\nplt.scatter(result_df['id'], residuals)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('ID')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.show()\n```\n\n\n\n\nAbove is the residual plot for this model with the same setup as the linear kernel one."
  },
  {
    "objectID": "RegressionBlog/index.html#discussion-and-improvements",
    "href": "RegressionBlog/index.html#discussion-and-improvements",
    "title": "Comparing Multiple Linear and Non-Linear Regression on MLB Data",
    "section": "Discussion and Improvements",
    "text": "Discussion and Improvements\nIn actuality, it appears to me that the poly non-linear kernel represents the data better for certain features even though it doesn’t perform as well against the linear kernel. I believe the tradeoff to this approach highly depends on the C value for both approaches and requires more hyperparameter awareness and optimization.\nIn terms of features, I believe all of these features have strong presuasion in determing an MLB player’s salary, but there is one flaw. Some players, regardless of preformance, are more “famous” than other players. It is likely that more famous players have higher salaries simply because they generate more revenue for the teams they play for, but that doesn’t necessarily mean the player’s popularity is correlated with skill. This is the reason why I believe there are outliers in this dataset. If there were a way to accurately determine popularity, that would be a key feature in predicting salary as well for this particular domain. Speaking of outliers, I would also like to point out that outliers can have a significant impact on these regression models. Depending on the case, deleting outliers may be a valid option, but that should come with caution as deleting data can result in loss of valuable information."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "NaiveBayesBlog/index.html",
    "href": "NaiveBayesBlog/index.html",
    "title": "Probability Theory with Naive Bayes Application",
    "section": "",
    "text": "Author: Daniel Hassler\n```{python}\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport mnist\n\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn.metrics import confusion_matrix\n```"
  },
  {
    "objectID": "NaiveBayesBlog/index.html#data",
    "href": "NaiveBayesBlog/index.html#data",
    "title": "Probability Theory with Naive Bayes Application",
    "section": "Data",
    "text": "Data\nBefore diving into the probability theory and using Naive Bayes, I will first introduce the dataset I am using for this application.\nThe dataset I am using to explain probability theory with Naive Bayes is the MNIST dataset, a large dataset containing pictures/drawings of digits 0-9. There are 60,000 training images and 10,000 testing images in my particular dataset.\nBelow, is a visualization of one entry in the training set and one entry in the test set to show what these digits look like.\n\n```{python}\n# mnist.init()\nX_train, y_train, X_test, y_test = mnist.load()\nprint(\"X_train len: \", len(X_train))\nprint(\"X_test len: \", len(X_test))\nprint(\"X_train shape: \", X_train.shape)\nprint(\"X_test shape: \", X_test.shape)\n\nplt.subplot(1, 2, 1)\nplt.title(\"Entry in Train Set\")\nimg = X_train[0,:].reshape(28,28) # First image in the training set.\nplt.imshow(img,cmap='gray')\n\nplt.subplot(1, 2, 2)\nplt.title(\"Entry in Test Set\")\nimg = X_test[0,:].reshape(28,28) # First image in the test set.\nplt.imshow(img,cmap='gray')\nplt.show() # Show the image\n```\n\nX_train len:  60000\nX_test len:  10000\nX_train shape:  (60000, 784)\nX_test shape:  (10000, 784)\n\n\n\n\n\nHere is a visualization showing a unique entry in the X_train data for each digit.\n\n```{python}\nunique_values, indices = np.unique(y_train, return_index=True)\n\nfor i, label_index in enumerate(indices):\n    plt.subplot(1, len(indices), i + 1)\n    img = X_train[label_index,:].reshape(28,28)\n    plt.imshow(img,cmap='gray')\nplt.show()\n```"
  },
  {
    "objectID": "NaiveBayesBlog/index.html#naive-bayes-background",
    "href": "NaiveBayesBlog/index.html#naive-bayes-background",
    "title": "Probability Theory with Naive Bayes Application",
    "section": "Naive Bayes Background",
    "text": "Naive Bayes Background\nA fundamental concept in probability theory is Bayes’ Theorem. The theorem is used to update the belief of an event occuring given new evidence: \\[\nP(A|B) =\n  \\frac{ P(B|A)P(A)}{\n       P(B)}\n\\]\nIn the theorem above, P(A|B) is represented as the probability of event A occurring given event B occurred. P(B|A) is the probability of B occurring given that event A occured. P(A) and P(B) are independent events.\nNaive Bayes is a machine learning algorithm that relies on the concept of Bayes Theorem and idea of conditional independence. In an application using Naive Bayes, we are assuming that the features are independent of each other (naive assumption), which is rarely ever true in real world scenarios, but it is a valid benchmark and has some benefits. This is the overall idea of the application of Bayes’ Theorem with Naive Bayes: \\[\nP(C|X) =\n  \\frac{ P(X|C)P(C)}{\n       P(X)}\n\\]\nThe goal is to find the probability of class C given observation X. In our case with the MNIST dataset, X is the feature set that represents every pixel in the 28x28 images (784 total features) and C is a representation of all the classes, digits 0-9. The naive assumption with the MNIST dataset is treating the pixels as independent observations.\nFirst, we can get the prior probabilities, represented as P(C) from the training set itself. This is the probability occurrence of each class in the dataset. I calculated this by this equation, where N_c is the number of occurrences of class c and N is the sum of all classes occurrences. \\[\nP(C=c) = \\frac{N_c}{N}\n\\]\nWe can then get the likelihood probability P(X|C), the probability of the feature (pixel) given class C. We can get this directly from the training data itself. We can get this by observing the data or by calculating the probability density function if we’re assuming the data flows like a Gaussian distribution.\nP(C|X), the posterior probability, represents the probability of class C given feature X. Based on this, in our prediction stage with Naive Bayes, we are taking the class with the max probability to get the classification."
  },
  {
    "objectID": "NaiveBayesBlog/index.html#naive-bayes-classification",
    "href": "NaiveBayesBlog/index.html#naive-bayes-classification",
    "title": "Probability Theory with Naive Bayes Application",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\nFor the purposes of explaining probability theory with NB, visualizing data, speed consideration, and understanding of naive bayes on the MNIST dataset, I decided to go with sklearn’s GaussianNB model, which is a commonly used baseline model for a lot of distributions that follow a Gaussian distribution. By chosing this model, I am assuming that my MNIST data follows a Gaussian distribution, but the data itself doesn’t directly follow this assumption. So, compared to other methods like CNNs (see improvements), we will see a performance degredation with this model, as MNIST pixels are not normally distributed. Although, the performance of this model we will see is “reasonable” and better than expected, especially since we can still assume conditional independence between the features, which is a large assumption for Naive Bayes.\nFor the hyperparameter values, I gathered the dataset’s prior distribution by simply taking the frequency of the dataset and dividing it by the sum of all the frequencies, and then passed it into the GaussianNB model. Another parameter I had to tune is the var_smoothing value. This value is used to prevent any division by zero during probability estimation. By default, sklearn sets this value to 1e-09, but the effectiveness of this value depends on the dataset, so in the end, I found 0.1 to have the best accuracy performance.\n\n```{python}\nunique, counts = np.unique(y_train, return_counts=True)\nsum_counts = np.sum(counts)\npriors = np.divide(counts, sum_counts)\n\nnb = GaussianNB(priors=priors, var_smoothing=0.1)\nnb.fit(X_train, y_train)\ny_pred = nb.predict(X_test)\nprint(\"Priors: \", nb.priors)\n```\n\nPriors:  [0.09871667 0.11236667 0.0993     0.10218333 0.09736667 0.09035\n 0.09863333 0.10441667 0.09751667 0.09915   ]"
  },
  {
    "objectID": "NaiveBayesBlog/index.html#results",
    "href": "NaiveBayesBlog/index.html#results",
    "title": "Probability Theory with Naive Bayes Application",
    "section": "Results",
    "text": "Results\nIn order to evaluate the preformance of the GaussianNB classifier, I ran a confusion matrix to visualize where the predictions fall. As you can see, we have around 81% accuracy on our given classifier, which is about what we expected for this dataset.\n\n```{python}\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nconf_df = pd.DataFrame(confusion_matrix, index=[f\"{i}\" for i in range(10)], columns=[f\"{i}\" for i in range(10)])\nheatmap = sns.heatmap(conf_df, annot=True, fmt=\"d\", linewidths=0.35, cmap=\"YlGnBu\")\nplt.title(f\"Model Predictions With {(np.sum(confusion_matrix.diagonal()) / y_test.shape[0]) * 100:.2f}% Accuracy\")\n```\n\nText(0.5, 1.0, 'Model Predictions With 81.40% Accuracy')\n\n\n\n\n\nNext, I used the GaussianNB theta value to extract the mean_pixel_values, which stores the estimated mean of each pixel for every class. This visualization plots a heatmap of each pixel, where the pixels that are highlighted in yellow have the highest mean for that digit and the dark blue being the lowest.\n\n```{python}\nmean_pixel_values = nb.theta_\nplt.figure(figsize=(5,10))\nfor i, digit in enumerate(range(10)):\n    plt.subplot(len(indices) // 2, 2, i + 1)\n    plt.title(f\"Digit {digit}\")\n    plt.axis('off')\n    img = mean_pixel_values[digit].reshape(28,28)\n    plt.imshow(img)\nplt.plot()\n```\n\n[]"
  },
  {
    "objectID": "NaiveBayesBlog/index.html#improvements",
    "href": "NaiveBayesBlog/index.html#improvements",
    "title": "Probability Theory with Naive Bayes Application",
    "section": "Improvements",
    "text": "Improvements\nThough we achieve decent accuracy with the MNIST dataset using GaussianNB classifier, this model has one big flaw for image classification; it only looks at the discretized values for specific points in the image. What would happen if I shifted the “0” or “9” to a different section of the image (not centered)? it would not be able to classify this case effectively.\nA way to fix the above limitation is using convolutional neural networks (CNNs), which is a deep learning classifier used in a lot of computer vision and even NLP related applications. Its main feature is using the idea of a “sliding window” to find more meaningful representations, which means the location of the object we’re classifying is less important."
  },
  {
    "objectID": "DecisionTreeRFBlog/DecisionTreeNotebook.html",
    "href": "DecisionTreeRFBlog/DecisionTreeNotebook.html",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "",
    "text": "import numpy as np\nimport sklearn\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, LearningCurveDisplay\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn import tree\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "DecisionTreeRFBlog/DecisionTreeNotebook.html#imports",
    "href": "DecisionTreeRFBlog/DecisionTreeNotebook.html#imports",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "",
    "text": "import numpy as np\nimport sklearn\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, LearningCurveDisplay\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn import tree\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "DecisionTreeRFBlog/DecisionTreeNotebook.html#class-distribution",
    "href": "DecisionTreeRFBlog/DecisionTreeNotebook.html#class-distribution",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Class Distribution",
    "text": "Class Distribution\n\n# Class imbalance, more obesity.\nunique_values, counts = np.unique(y, return_counts=True)\nplt.bar(unique_values, counts)\nplt.title(\"BMI Classes in the Entire Dataset\")\nplt.xlabel(\"BMI Class\")\nplt.ylabel(\"Occurences in Entire Dataset\")\nplt.show()"
  },
  {
    "objectID": "DecisionTreeRFBlog/DecisionTreeNotebook.html#feature-correlation",
    "href": "DecisionTreeRFBlog/DecisionTreeNotebook.html#feature-correlation",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Feature Correlation",
    "text": "Feature Correlation\n\n\ncorrelation_matrix = data.corr()\n\n# Display a heatmap of the correlation matrix\nplt.figure(figsize=(10, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('Correlation Heatmap')\nplt.show()\n\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_33884\\1179218035.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  correlation_matrix = data.corr()"
  },
  {
    "objectID": "DecisionTreeRFBlog/DecisionTreeNotebook.html#scatter-plot-of-data",
    "href": "DecisionTreeRFBlog/DecisionTreeNotebook.html#scatter-plot-of-data",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Scatter Plot of Data",
    "text": "Scatter Plot of Data\n\nplt.figure(figsize=(6, 6))\nsns.scatterplot(data=data, x='Height', y='Weight', hue='Index', palette='deep')\nplt.title('Scatter Plot of Height vs Weight')\nplt.show()"
  },
  {
    "objectID": "DecisionTreeRFBlog/DecisionTreeNotebook.html#distribution-of-features",
    "href": "DecisionTreeRFBlog/DecisionTreeNotebook.html#distribution-of-features",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Distribution of Features",
    "text": "Distribution of Features\n\n# Compare height and weight between male and female genders using box plots\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=data, x='Gender', y='Height')\nplt.title('Comparison of Height between Genders')\nplt.xlabel('Gender')\nplt.ylabel('Height')\nplt.show()\n\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=data, x='Gender', y='Weight')\nplt.title('Comparison of Weight between Genders')\nplt.xlabel('Gender')\nplt.ylabel('Weight')\nplt.show()"
  },
  {
    "objectID": "DecisionTreeRFBlog/DecisionTreeNotebook.html#hyperparameter-tuning-dt",
    "href": "DecisionTreeRFBlog/DecisionTreeNotebook.html#hyperparameter-tuning-dt",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Hyperparameter Tuning DT",
    "text": "Hyperparameter Tuning DT\n\nparam_grid = {\n    'max_depth': [i for i in range(2, 9)],\n    'min_samples_leaf': [2 ** i for i in range(0, 4)],\n    'criterion': [\"entropy\", \"gini\"]\n}\n\ndt = DecisionTreeClassifier(random_state=42)\n\ngrid_search_dt = GridSearchCV(dt, param_grid, cv=StratifiedKFold(n_splits=3), scoring='accuracy')\ngrid_search_dt.fit(X_train, y_train)\nbest_params_dt = grid_search_dt.best_params_\nprint(\"Best Hyperparameters:\", best_params_dt)\nprint(\"Best Score:\", grid_search_dt.best_score_)\n\nBest Hyperparameters: {'criterion': 'entropy', 'max_depth': 8, 'min_samples_leaf': 1}\nBest Score: 0.8281314289073061"
  },
  {
    "objectID": "DecisionTreeRFBlog/DecisionTreeNotebook.html#decision-tree-classifier",
    "href": "DecisionTreeRFBlog/DecisionTreeNotebook.html#decision-tree-classifier",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Decision Tree Classifier",
    "text": "Decision Tree Classifier\n\ndt = DecisionTreeClassifier(max_depth=best_params_dt[\"max_depth\"], min_samples_leaf=best_params_dt[\"min_samples_leaf\"], criterion=best_params_dt[\"criterion\"])\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)"
  },
  {
    "objectID": "DecisionTreeRFBlog/DecisionTreeNotebook.html#decision-tree-visualization",
    "href": "DecisionTreeRFBlog/DecisionTreeNotebook.html#decision-tree-visualization",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Decision Tree Visualization",
    "text": "Decision Tree Visualization\n\nplt.figure(figsize=(30, 20))\nplot_tree(dt, feature_names=X_train.columns.tolist(), class_names=['0', '1', '2','3','4','5'], filled=True)\n\n[Text(0.5918141592920354, 0.9444444444444444, 'Weight &lt;= 94.5\\nentropy = 2.189\\nsamples = 320\\nvalue = [11, 17, 39, 47, 76, 130]\\nclass = 5'),\n Text(0.3915929203539823, 0.8333333333333334, 'Weight &lt;= 69.5\\nentropy = 2.381\\nsamples = 126\\nvalue = [11, 17, 39, 31, 21, 7]\\nclass = 2'),\n Text(0.23893805309734514, 0.7222222222222222, 'Height &lt;= 167.5\\nentropy = 2.037\\nsamples = 56\\nvalue = [11, 17, 18, 9, 1, 0]\\nclass = 2'),\n Text(0.1415929203539823, 0.6111111111111112, 'Height &lt;= 152.5\\nentropy = 1.371\\nsamples = 25\\nvalue = [1, 0, 14, 9, 1, 0]\\nclass = 2'),\n Text(0.07079646017699115, 0.5, 'Weight &lt;= 57.5\\nentropy = 1.189\\nsamples = 12\\nvalue = [0, 0, 3, 8, 1, 0]\\nclass = 3'),\n Text(0.035398230088495575, 0.3888888888888889, 'Height &lt;= 144.0\\nentropy = 0.811\\nsamples = 4\\nvalue = [0, 0, 3, 1, 0, 0]\\nclass = 2'),\n Text(0.017699115044247787, 0.2777777777777778, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 1, 0, 0]\\nclass = 3'),\n Text(0.05309734513274336, 0.2777777777777778, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 0, 3, 0, 0, 0]\\nclass = 2'),\n Text(0.10619469026548672, 0.3888888888888889, 'Weight &lt;= 68.0\\nentropy = 0.544\\nsamples = 8\\nvalue = [0, 0, 0, 7, 1, 0]\\nclass = 3'),\n Text(0.08849557522123894, 0.2777777777777778, 'entropy = 0.0\\nsamples = 7\\nvalue = [0, 0, 0, 7, 0, 0]\\nclass = 3'),\n Text(0.12389380530973451, 0.2777777777777778, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0]\\nclass = 4'),\n Text(0.21238938053097345, 0.5, 'Weight &lt;= 66.0\\nentropy = 0.773\\nsamples = 13\\nvalue = [1, 0, 11, 1, 0, 0]\\nclass = 2'),\n Text(0.17699115044247787, 0.3888888888888889, 'Height &lt;= 162.5\\nentropy = 0.439\\nsamples = 11\\nvalue = [1, 0, 10, 0, 0, 0]\\nclass = 2'),\n Text(0.1592920353982301, 0.2777777777777778, 'entropy = 0.0\\nsamples = 7\\nvalue = [0, 0, 7, 0, 0, 0]\\nclass = 2'),\n Text(0.19469026548672566, 0.2777777777777778, 'Weight &lt;= 57.5\\nentropy = 0.811\\nsamples = 4\\nvalue = [1, 0, 3, 0, 0, 0]\\nclass = 2'),\n Text(0.17699115044247787, 0.16666666666666666, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0, 0, 0, 0, 0]\\nclass = 0'),\n Text(0.21238938053097345, 0.16666666666666666, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 0, 3, 0, 0, 0]\\nclass = 2'),\n Text(0.24778761061946902, 0.3888888888888889, 'Height &lt;= 163.5\\nentropy = 1.0\\nsamples = 2\\nvalue = [0, 0, 1, 1, 0, 0]\\nclass = 2'),\n Text(0.23008849557522124, 0.2777777777777778, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 1, 0, 0]\\nclass = 3'),\n Text(0.26548672566371684, 0.2777777777777778, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 1, 0, 0, 0]\\nclass = 2'),\n Text(0.336283185840708, 0.6111111111111112, 'Weight &lt;= 55.0\\nentropy = 1.383\\nsamples = 31\\nvalue = [10, 17, 4, 0, 0, 0]\\nclass = 1'),\n Text(0.3008849557522124, 0.5, 'Height &lt;= 180.0\\nentropy = 0.918\\nsamples = 15\\nvalue = [10, 5, 0, 0, 0, 0]\\nclass = 0'),\n Text(0.2831858407079646, 0.3888888888888889, 'entropy = 0.0\\nsamples = 5\\nvalue = [0, 5, 0, 0, 0, 0]\\nclass = 1'),\n Text(0.3185840707964602, 0.3888888888888889, 'entropy = 0.0\\nsamples = 10\\nvalue = [10, 0, 0, 0, 0, 0]\\nclass = 0'),\n Text(0.37168141592920356, 0.5, 'Height &lt;= 182.0\\nentropy = 0.811\\nsamples = 16\\nvalue = [0, 12, 4, 0, 0, 0]\\nclass = 1'),\n Text(0.35398230088495575, 0.3888888888888889, 'Weight &lt;= 59.5\\nentropy = 0.985\\nsamples = 7\\nvalue = [0, 3, 4, 0, 0, 0]\\nclass = 2'),\n Text(0.336283185840708, 0.2777777777777778, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 3, 0, 0, 0, 0]\\nclass = 1'),\n Text(0.37168141592920356, 0.2777777777777778, 'entropy = 0.0\\nsamples = 4\\nvalue = [0, 0, 4, 0, 0, 0]\\nclass = 2'),\n Text(0.3893805309734513, 0.3888888888888889, 'entropy = 0.0\\nsamples = 9\\nvalue = [0, 9, 0, 0, 0, 0]\\nclass = 1'),\n Text(0.5442477876106194, 0.7222222222222222, 'Height &lt;= 168.5\\nentropy = 1.894\\nsamples = 70\\nvalue = [0, 0, 21, 22, 20, 7]\\nclass = 3'),\n Text(0.4778761061946903, 0.6111111111111112, 'Height &lt;= 148.5\\nentropy = 1.408\\nsamples = 33\\nvalue = [0, 0, 0, 7, 19, 7]\\nclass = 4'),\n Text(0.4424778761061947, 0.5, 'Weight &lt;= 84.5\\nentropy = 0.996\\nsamples = 13\\nvalue = [0, 0, 0, 0, 6, 7]\\nclass = 5'),\n Text(0.4247787610619469, 0.3888888888888889, 'Height &lt;= 141.5\\nentropy = 0.592\\nsamples = 7\\nvalue = [0, 0, 0, 0, 6, 1]\\nclass = 4'),\n Text(0.40707964601769914, 0.2777777777777778, 'Height &lt;= 140.5\\nentropy = 1.0\\nsamples = 2\\nvalue = [0, 0, 0, 0, 1, 1]\\nclass = 4'),\n Text(0.3893805309734513, 0.16666666666666666, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0]\\nclass = 4'),\n Text(0.4247787610619469, 0.16666666666666666, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 1]\\nclass = 5'),\n Text(0.4424778761061947, 0.2777777777777778, 'entropy = 0.0\\nsamples = 5\\nvalue = [0, 0, 0, 0, 5, 0]\\nclass = 4'),\n Text(0.46017699115044247, 0.3888888888888889, 'entropy = 0.0\\nsamples = 6\\nvalue = [0, 0, 0, 0, 0, 6]\\nclass = 5'),\n Text(0.5132743362831859, 0.5, 'Weight &lt;= 80.5\\nentropy = 0.934\\nsamples = 20\\nvalue = [0, 0, 0, 7, 13, 0]\\nclass = 4'),\n Text(0.49557522123893805, 0.3888888888888889, 'Height &lt;= 159.0\\nentropy = 0.946\\nsamples = 11\\nvalue = [0, 0, 0, 7, 4, 0]\\nclass = 3'),\n Text(0.4778761061946903, 0.2777777777777778, 'Weight &lt;= 72.0\\nentropy = 0.722\\nsamples = 5\\nvalue = [0, 0, 0, 1, 4, 0]\\nclass = 4'),\n Text(0.46017699115044247, 0.16666666666666666, 'Gender_Encoded &lt;= 0.5\\nentropy = 1.0\\nsamples = 2\\nvalue = [0, 0, 0, 1, 1, 0]\\nclass = 3'),\n Text(0.4424778761061947, 0.05555555555555555, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 1, 0, 0]\\nclass = 3'),\n Text(0.4778761061946903, 0.05555555555555555, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0]\\nclass = 4'),\n Text(0.49557522123893805, 0.16666666666666666, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 0, 0, 0, 3, 0]\\nclass = 4'),\n Text(0.5132743362831859, 0.2777777777777778, 'entropy = 0.0\\nsamples = 6\\nvalue = [0, 0, 0, 6, 0, 0]\\nclass = 3'),\n Text(0.5309734513274337, 0.3888888888888889, 'entropy = 0.0\\nsamples = 9\\nvalue = [0, 0, 0, 0, 9, 0]\\nclass = 4'),\n Text(0.6106194690265486, 0.6111111111111112, 'Weight &lt;= 80.5\\nentropy = 1.133\\nsamples = 37\\nvalue = [0, 0, 21, 15, 1, 0]\\nclass = 2'),\n Text(0.5929203539823009, 0.5, 'entropy = 0.0\\nsamples = 11\\nvalue = [0, 0, 11, 0, 0, 0]\\nclass = 2'),\n Text(0.6283185840707964, 0.5, 'Height &lt;= 188.5\\nentropy = 1.169\\nsamples = 26\\nvalue = [0, 0, 10, 15, 1, 0]\\nclass = 3'),\n Text(0.6106194690265486, 0.3888888888888889, 'Weight &lt;= 89.0\\nentropy = 1.049\\nsamples = 21\\nvalue = [0, 0, 5, 15, 1, 0]\\nclass = 3'),\n Text(0.5663716814159292, 0.2777777777777778, 'Height &lt;= 176.0\\nentropy = 0.98\\nsamples = 12\\nvalue = [0, 0, 5, 7, 0, 0]\\nclass = 3'),\n Text(0.5309734513274337, 0.16666666666666666, 'Gender_Encoded &lt;= 0.5\\nentropy = 0.811\\nsamples = 4\\nvalue = [0, 0, 3, 1, 0, 0]\\nclass = 2'),\n Text(0.5132743362831859, 0.05555555555555555, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 1, 0, 0]\\nclass = 3'),\n Text(0.5486725663716814, 0.05555555555555555, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 0, 3, 0, 0, 0]\\nclass = 2'),\n Text(0.6017699115044248, 0.16666666666666666, 'Height &lt;= 183.0\\nentropy = 0.811\\nsamples = 8\\nvalue = [0, 0, 2, 6, 0, 0]\\nclass = 3'),\n Text(0.584070796460177, 0.05555555555555555, 'entropy = 0.0\\nsamples = 5\\nvalue = [0, 0, 0, 5, 0, 0]\\nclass = 3'),\n Text(0.6194690265486725, 0.05555555555555555, 'entropy = 0.918\\nsamples = 3\\nvalue = [0, 0, 2, 1, 0, 0]\\nclass = 2'),\n Text(0.6548672566371682, 0.2777777777777778, 'Height &lt;= 173.0\\nentropy = 0.503\\nsamples = 9\\nvalue = [0, 0, 0, 8, 1, 0]\\nclass = 3'),\n Text(0.6371681415929203, 0.16666666666666666, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0]\\nclass = 4'),\n Text(0.672566371681416, 0.16666666666666666, 'entropy = 0.0\\nsamples = 8\\nvalue = [0, 0, 0, 8, 0, 0]\\nclass = 3'),\n Text(0.6460176991150443, 0.3888888888888889, 'entropy = 0.0\\nsamples = 5\\nvalue = [0, 0, 5, 0, 0, 0]\\nclass = 2'),\n Text(0.7920353982300885, 0.8333333333333334, 'Height &lt;= 171.5\\nentropy = 1.229\\nsamples = 194\\nvalue = [0, 0, 0, 16, 55, 123]\\nclass = 5'),\n Text(0.7345132743362832, 0.7222222222222222, 'Weight &lt;= 116.0\\nentropy = 0.363\\nsamples = 101\\nvalue = [0, 0, 0, 0, 7, 94]\\nclass = 5'),\n Text(0.7168141592920354, 0.6111111111111112, 'Height &lt;= 164.0\\nentropy = 0.797\\nsamples = 29\\nvalue = [0, 0, 0, 0, 7, 22]\\nclass = 5'),\n Text(0.6991150442477876, 0.5, 'Weight &lt;= 95.5\\nentropy = 0.258\\nsamples = 23\\nvalue = [0, 0, 0, 0, 1, 22]\\nclass = 5'),\n Text(0.6814159292035398, 0.3888888888888889, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0]\\nclass = 4'),\n Text(0.7168141592920354, 0.3888888888888889, 'entropy = 0.0\\nsamples = 22\\nvalue = [0, 0, 0, 0, 0, 22]\\nclass = 5'),\n Text(0.7345132743362832, 0.5, 'entropy = 0.0\\nsamples = 6\\nvalue = [0, 0, 0, 0, 6, 0]\\nclass = 4'),\n Text(0.7522123893805309, 0.6111111111111112, 'entropy = 0.0\\nsamples = 72\\nvalue = [0, 0, 0, 0, 0, 72]\\nclass = 5'),\n Text(0.8495575221238938, 0.7222222222222222, 'Weight &lt;= 126.5\\nentropy = 1.454\\nsamples = 93\\nvalue = [0, 0, 0, 16, 48, 29]\\nclass = 4'),\n Text(0.7876106194690266, 0.6111111111111112, 'Weight &lt;= 114.5\\nentropy = 0.918\\nsamples = 48\\nvalue = [0, 0, 0, 16, 32, 0]\\nclass = 4'),\n Text(0.7699115044247787, 0.5, 'Height &lt;= 181.5\\nentropy = 1.0\\nsamples = 32\\nvalue = [0, 0, 0, 16, 16, 0]\\nclass = 3'),\n Text(0.7522123893805309, 0.3888888888888889, 'entropy = 0.0\\nsamples = 13\\nvalue = [0, 0, 0, 0, 13, 0]\\nclass = 4'),\n Text(0.7876106194690266, 0.3888888888888889, 'Weight &lt;= 105.5\\nentropy = 0.629\\nsamples = 19\\nvalue = [0, 0, 0, 16, 3, 0]\\nclass = 3'),\n Text(0.7699115044247787, 0.2777777777777778, 'entropy = 0.0\\nsamples = 10\\nvalue = [0, 0, 0, 10, 0, 0]\\nclass = 3'),\n Text(0.8053097345132744, 0.2777777777777778, 'Height &lt;= 190.0\\nentropy = 0.918\\nsamples = 9\\nvalue = [0, 0, 0, 6, 3, 0]\\nclass = 3'),\n Text(0.7876106194690266, 0.16666666666666666, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 0, 0, 0, 3, 0]\\nclass = 4'),\n Text(0.8230088495575221, 0.16666666666666666, 'entropy = 0.0\\nsamples = 6\\nvalue = [0, 0, 0, 6, 0, 0]\\nclass = 3'),\n Text(0.8053097345132744, 0.5, 'entropy = 0.0\\nsamples = 16\\nvalue = [0, 0, 0, 0, 16, 0]\\nclass = 4'),\n Text(0.911504424778761, 0.6111111111111112, 'Height &lt;= 185.5\\nentropy = 0.939\\nsamples = 45\\nvalue = [0, 0, 0, 0, 16, 29]\\nclass = 5'),\n Text(0.8761061946902655, 0.5, 'Weight &lt;= 139.5\\nentropy = 0.529\\nsamples = 25\\nvalue = [0, 0, 0, 0, 3, 22]\\nclass = 5'),\n Text(0.8584070796460177, 0.3888888888888889, 'Height &lt;= 178.5\\nentropy = 0.881\\nsamples = 10\\nvalue = [0, 0, 0, 0, 3, 7]\\nclass = 5'),\n Text(0.8407079646017699, 0.2777777777777778, 'entropy = 0.0\\nsamples = 5\\nvalue = [0, 0, 0, 0, 0, 5]\\nclass = 5'),\n Text(0.8761061946902655, 0.2777777777777778, 'Height &lt;= 184.5\\nentropy = 0.971\\nsamples = 5\\nvalue = [0, 0, 0, 0, 3, 2]\\nclass = 4'),\n Text(0.8584070796460177, 0.16666666666666666, 'Height &lt;= 180.0\\nentropy = 0.811\\nsamples = 4\\nvalue = [0, 0, 0, 0, 3, 1]\\nclass = 4'),\n Text(0.8407079646017699, 0.05555555555555555, 'entropy = 1.0\\nsamples = 2\\nvalue = [0, 0, 0, 0, 1, 1]\\nclass = 4'),\n Text(0.8761061946902655, 0.05555555555555555, 'entropy = 0.0\\nsamples = 2\\nvalue = [0, 0, 0, 0, 2, 0]\\nclass = 4'),\n Text(0.8938053097345132, 0.16666666666666666, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 1]\\nclass = 5'),\n Text(0.8938053097345132, 0.3888888888888889, 'entropy = 0.0\\nsamples = 15\\nvalue = [0, 0, 0, 0, 0, 15]\\nclass = 5'),\n Text(0.9469026548672567, 0.5, 'Weight &lt;= 139.0\\nentropy = 0.934\\nsamples = 20\\nvalue = [0, 0, 0, 0, 13, 7]\\nclass = 4'),\n Text(0.9292035398230089, 0.3888888888888889, 'entropy = 0.0\\nsamples = 9\\nvalue = [0, 0, 0, 0, 9, 0]\\nclass = 4'),\n Text(0.9646017699115044, 0.3888888888888889, 'Height &lt;= 196.5\\nentropy = 0.946\\nsamples = 11\\nvalue = [0, 0, 0, 0, 4, 7]\\nclass = 5'),\n Text(0.9469026548672567, 0.2777777777777778, 'Height &lt;= 194.0\\nentropy = 0.544\\nsamples = 8\\nvalue = [0, 0, 0, 0, 1, 7]\\nclass = 5'),\n Text(0.9292035398230089, 0.16666666666666666, 'entropy = 0.0\\nsamples = 5\\nvalue = [0, 0, 0, 0, 0, 5]\\nclass = 5'),\n Text(0.9646017699115044, 0.16666666666666666, 'Weight &lt;= 150.0\\nentropy = 0.918\\nsamples = 3\\nvalue = [0, 0, 0, 0, 1, 2]\\nclass = 5'),\n Text(0.9469026548672567, 0.05555555555555555, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0]\\nclass = 4'),\n Text(0.9823008849557522, 0.05555555555555555, 'entropy = 0.0\\nsamples = 2\\nvalue = [0, 0, 0, 0, 0, 2]\\nclass = 5'),\n Text(0.9823008849557522, 0.2777777777777778, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 0, 0, 0, 3, 0]\\nclass = 4')]"
  },
  {
    "objectID": "DecisionTreeRFBlog/DecisionTreeNotebook.html#accuracy-results-on-test-data",
    "href": "DecisionTreeRFBlog/DecisionTreeNotebook.html#accuracy-results-on-test-data",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Accuracy Results on Test Data",
    "text": "Accuracy Results on Test Data\n\nprint(\"Micro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='micro'))\nprint(\"Macro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='macro'))\nprint(\"Precision: \", sklearn.metrics.precision_score(y_test, y_pred, average=\"weighted\"))\nprint(\"Recall: \", sklearn.metrics.recall_score(y_test, y_pred, average='weighted'))\n\n# Calculate AUC-PRC for each class\nauc_prc_scores = []\nfor class_index in range(y_pred.shape[0]):\n    precision, recall, _ = sklearn.metrics.precision_recall_curve(y_test == class_index, y_pred == class_index)\n    auc_prc_scores.append(sklearn.metrics.auc(recall, precision))\n\n# Compute the summary metric (e.g., micro-average)\nmicro_avg_auc_prc = sklearn.metrics.auc(recall, precision)\n\nprint(\"AUC-PRC for Each Class:\", auc_prc_scores)\nprint(\"Micro-Average AUC-PRC:\", micro_avg_auc_prc)\n\nMicro F1:  0.8000000000000002\nMacro F1:  0.7033683460612853\nPrecision:  0.8400535714285715\nRecall:  0.8\nAUC-PRC for Each Class: [0.75, 0.625, 0.7875, 0.8284340659340659, 0.8070833333333334, 0.915301724137931, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\nMicro-Average AUC-PRC: 0.5\n\n\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n\n\n\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nconf_df = pd.DataFrame(confusion_matrix, index=[f\"{i}\" for i in range(6)], columns=[f\"{i}\" for i in range(6)])\nheatmap = sns.heatmap(conf_df, annot=True, fmt=\"d\", linewidths=0.35, cmap=\"YlGnBu\")\nplt.title(f\"Model Predictions With {(np.sum(confusion_matrix.diagonal()) / y_test.shape[0]) * 100:.2f}% Accuracy\")\n\nText(0.5, 1.0, 'Model Predictions With 80.00% Accuracy')"
  },
  {
    "objectID": "DecisionTreeRFBlog/DecisionTreeNotebook.html#hyperparamter-tuning",
    "href": "DecisionTreeRFBlog/DecisionTreeNotebook.html#hyperparamter-tuning",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Hyperparamter Tuning",
    "text": "Hyperparamter Tuning\n\nparam_grid = {\n    'max_depth': [i for i in range(2, 9)],\n    'min_samples_leaf': [2 ** i for i in range(0, 4)],\n    'criterion': [\"entropy\", \"gini\"]\n}\n\nrf = RandomForestClassifier(random_state=42)\n\nprint(y_train.shape)\ngrid_search_rf = GridSearchCV(rf, param_grid, cv=StratifiedKFold(n_splits=3), scoring='accuracy')\ngrid_search_rf.fit(X_train, y_train.values.ravel())\nbest_params_rf = grid_search_rf.best_params_\nprint(\"Best Hyperparameters:\", best_params_rf)\nprint(\"Best Score:\", grid_search_rf.best_score_)\n\n(320, 1)\nBest Hyperparameters: {'criterion': 'gini', 'max_depth': 8, 'min_samples_leaf': 1}\nBest Score: 0.8125844942103098"
  },
  {
    "objectID": "DecisionTreeRFBlog/DecisionTreeNotebook.html#randomforest-classifier",
    "href": "DecisionTreeRFBlog/DecisionTreeNotebook.html#randomforest-classifier",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "RandomForest Classifier",
    "text": "RandomForest Classifier\n\nrf = RandomForestClassifier(n_estimators=300, \n                            max_depth=best_params_rf[\"max_depth\"], \n                            min_samples_leaf=best_params_rf[\"min_samples_leaf\"],\n                            criterion=best_params_rf[\"criterion\"])\nrf.fit(X_train, y_train.values.ravel())\ny_pred = rf.predict(X_test)"
  },
  {
    "objectID": "DecisionTreeRFBlog/DecisionTreeNotebook.html#plotting-the-randomforest-trees",
    "href": "DecisionTreeRFBlog/DecisionTreeNotebook.html#plotting-the-randomforest-trees",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Plotting the RandomForest Trees",
    "text": "Plotting the RandomForest Trees\n\n    fig, axes = plt.subplots(nrows = 1,ncols = 5,figsize = (10,3), dpi=250)\n    for index in range(5):\n        tree.plot_tree(rf.estimators_[index],\n                    feature_names = X_train.columns.tolist(), \n                    class_names= [f\"{i}\" for i in range(6)],\n                    filled = True,\n                    ax = axes[index])\n\n        axes[index].set_title('Estimator: ' + str(index + 1), fontsize = 10)\n#     fig.savefig(f'rf_{dt.n_estimators}trees.png')\n    plt.show()\n\n\n\n\n\nprint(\"Micro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='micro'))\nprint(\"Macro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='macro'))\n\nMicro F1:  0.8875\nMacro F1:  0.8523938236704195\n\n\n\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nconf_df = pd.DataFrame(confusion_matrix, index=[f\"{i}\" for i in range(6)], columns=[f\"{i}\" for i in range(6)])\nheatmap = sns.heatmap(conf_df, annot=True, fmt=\"d\", linewidths=0.35, cmap=\"YlGnBu\")\nplt.title(f\"Model Predictions With {(np.sum(confusion_matrix.diagonal()) / y_test.shape[0]) * 100:.2f}% Accuracy\")\n\nText(0.5, 1.0, 'Model Predictions With 88.75% Accuracy')\n\n\n\n\n\n\n'''\nPlot a graph that compares the two models, randomly generated with tuned hyperparameter models\n'''\ndt_results = []\nrf_results = []\nn_samples = 40\nindexes = [i for i in range(n_samples)]\nfor i in indexes:\n    dt = DecisionTreeClassifier(max_depth=8, min_samples_leaf=2, min_samples_split=4)\n    dt.fit(X_train, y_train)\n    y_pred_dt = dt.predict(X_test)\n    \n    rf = RandomForestClassifier(n_estimators=325, max_depth=8)\n    rf.fit(X_train, y_train.values.ravel())\n    y_pred_rf = rf.predict(X_test)\n    \n    confusion_matrix_dt = sklearn.metrics.confusion_matrix(y_test, y_pred_dt)\n    confusion_matrix_rf = sklearn.metrics.confusion_matrix(y_test, y_pred_rf)\n    \n    dt_results.append((np.sum(confusion_matrix_dt.diagonal()) / y_test.shape[0]) * 100)\n    rf_results.append((np.sum(confusion_matrix_rf.diagonal()) / y_test.shape[0]) * 100)\n\nplt.plot(indexes, dt_results, label=\"DT results\")\nplt.plot(indexes, rf_results, label=\"RF results\")\nplt.xlabel(\"Sample\")\nplt.ylabel(\"Accuracy on Test Data in %\")\nplt.title(\"Accuracy Comparison Between DT and RF on Randomly Generated Models\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "DecisionTreeRFBlog/DecisionTreeNotebook.html#improvements",
    "href": "DecisionTreeRFBlog/DecisionTreeNotebook.html#improvements",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Improvements",
    "text": "Improvements\n\nprint(X_train.shape)\nprint(y_train.shape)\n\nparam_grid = {\n    'max_depth': [i for i in range(2, 10)],\n    'min_samples_leaf': [2 ** i for i in range(0, 4)],\n    'criterion': [\"entropy\", \"gini\"]\n}\n\nrf = RandomForestClassifier(random_state=42)\ngrid_search_rf = GridSearchCV(rf, param_grid, cv=StratifiedKFold(n_splits=5), scoring='accuracy')\ngrid_search_rf.fit(X_train.drop(['Gender_Encoded'], axis=1), y_train.values.ravel())\nbest_params_rf = grid_search_rf.best_params_\nprint(\"Best Hyperparameters:\", best_params_rf)\nprint(\"Best Score:\", grid_search_rf.best_score_)\n\nrf = RandomForestClassifier(n_estimators=300, \n                            max_depth=best_params_rf[\"max_depth\"], \n                            min_samples_leaf=best_params_rf[\"min_samples_leaf\"],\n                            criterion=best_params_rf[\"criterion\"])\nrf.fit(X_train.drop(['Gender_Encoded'], axis=1), y_train.values.ravel())\ny_pred = rf.predict(X_test.drop(['Gender_Encoded'], axis=1))\n\n(320, 3)\n(320, 1)\nBest Hyperparameters: {'criterion': 'entropy', 'max_depth': 8, 'min_samples_leaf': 1}\nBest Score: 0.85625\n\n\n\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nconf_df = pd.DataFrame(confusion_matrix, index=[f\"{i}\" for i in range(6)], columns=[f\"{i}\" for i in range(6)])\nheatmap = sns.heatmap(conf_df, annot=True, fmt=\"d\", linewidths=0.35, cmap=\"YlGnBu\")\nplt.title(f\"Model Predictions With {(np.sum(confusion_matrix.diagonal()) / y_test.shape[0]) * 100:.2f}% Accuracy (NO GENDER)\")\n\nText(0.5, 1.0, 'Model Predictions With 90.00% Accuracy (NO GENDER)')\n\n\n\n\n\n\nprint(\"Micro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='micro'))\nprint(\"Macro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='macro'))\n\nMicro F1:  0.9\nMacro F1:  0.8450314415205457"
  },
  {
    "objectID": "DecisionTreeRFBlog/DecisionTreeNotebook.html#learning-curve",
    "href": "DecisionTreeRFBlog/DecisionTreeNotebook.html#learning-curve",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Learning Curve",
    "text": "Learning Curve\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.tree import DecisionTreeClassifier\ntrain_sizes, train_scores, test_scores = learning_curve(rf, X_train, y_train.values.ravel(), cv=StratifiedKFold(n_splits=5))\ndisplay = LearningCurveDisplay(train_sizes=train_sizes,\n    train_scores=train_scores, test_scores=test_scores, score_name=\"Score\")\ndisplay.plot()\nplt.show()"
  },
  {
    "objectID": "ClusteringBlog/index.html",
    "href": "ClusteringBlog/index.html",
    "title": "Spotify Recommendation System With Clustering",
    "section": "",
    "text": "Author: Daniel Hassler\n```{python}\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.model_selection import train_test_split, ParameterGrid\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\n```"
  },
  {
    "objectID": "ClusteringBlog/index.html#data-analysis",
    "href": "ClusteringBlog/index.html#data-analysis",
    "title": "Spotify Recommendation System With Clustering",
    "section": "Data Analysis",
    "text": "Data Analysis\nClustering algorithms can be applied to many real-world applications, including but not limited to security, anomaly detection, document clustering, stock market analysis, image compression, and so much more. The application I decided to approach with clustering is a song recommendation system. I found a dataset on Kaggle containing almost 114,000 songs from the popular music streaming platform Spotify. Each entry in the dataset consists of many features including artists, track_name, track_genre, popularity, danceability, and many more.\nBefore I dive into the visualizaitons, I first dropped duplicates in the dataset to minimize problems with recommendations. Now, below are some visualizations showcasing certain features in a scatterplot. This gives me a rough idea what the dataset looks like with all of these features and genres.\n\n```{python}\noriginal_df = pd.read_csv(\"./dataset-dedup.csv\")\nprint(original_df.columns)\n# original_df = original_df.drop_duplicates(subset=[\"artists\", \"track_name\"], keep=\"first\").reset_index()\nprint(original_df.shape)\n# original_df.to_csv(\"./dataset-dedup.csv\")\nfeatures_x = [\"loudness\", \"popularity\", \"duration_ms\"]\nfeatures_y = [\"popularity\", \"energy\", \"tempo\"]\n\nfor i, (x,y) in enumerate(zip(features_x, features_y)):\n    scatter = sns.scatterplot(x=x, y=y, hue='track_genre', data=original_df, palette=\"viridis\", alpha=0.25)\n    legend_labels = original_df['track_genre'].unique()# [:3]  # Show only the first 3 genres\n    scatter.legend(title='Genre', labels=legend_labels, prop={'size': 1})\n    plt.title(f\"Scatter Plot of {x} vs {y} by genre\")\n    plt.show()\n\nplt.show()\n```\n\nIndex(['Unnamed: 0.1', 'index', 'Unnamed: 0', 'track_id', 'artists',\n       'album_name', 'track_name', 'popularity', 'duration_ms', 'explicit',\n       'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n       'time_signature', 'track_genre'],\n      dtype='object')\n(81344, 23)\n\n\n\n\n\n\n\n\n\n\n\n\n```{python}\nunique_vals = original_df['track_genre'].unique()\nplt.bar(unique_vals, original_df['track_genre'].value_counts())\nplt.title(\"Occurrence of Genre\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Genre\")\n_ = plt.xticks(rotation=\"vertical\", fontsize=4)\n```\n\n\n\n\nBecause a lot of these continuous variables: loudness, popularity, duration_ms overlap by genre significantly, I decided to drop these features during training, as well as many other features like energy, danceability, acousticness, as these metrics are too complex, overlapping, and even subjective. As a Spotify consumer myself, I like when Spotify gives me songs related to the current artist I’m listening to, so I thought important features in this dataset included: artists, track_genre, minimally. Although, I did try other features like key, and tempo on top of that."
  },
  {
    "objectID": "ClusteringBlog/index.html#k-means",
    "href": "ClusteringBlog/index.html#k-means",
    "title": "Spotify Recommendation System With Clustering",
    "section": "K-Means",
    "text": "K-Means\nThe K-Means algorithm clusters data by minimizing a criteria known as intertia, the within-cluster sum-of-squares. The formula for inertia, specified in the K-means documentation for Sklearn, is noted below:\nNoting some of the variables in the summation: n is the number of datapoints, mu is the mean of the cluster, also the cluster_centroid of the cluster C, ||x_i - \\mu||^2 represents the squared euclidean distance between point x_i and the centroid, and min() takes the min of the calculation\n\\[\n\\sum_{i=0}^{n}\\min_{\\mu_j \\in C}(||x_i - \\mu_j||^2)\n\\]\nIt is worth noting that the inertia method has some drawbacks. According to Sklearn, intertia makes the assumption that clusters are convex and isotropic, which may not always be the case. The documentation also states that inertia isn’t a “normalized metric”, so running PCA (principal component analysis) before the K-means clustering is beneficial (which is exactly what I did in later steps).\nA great benefit to K-means is its scalability to large sample sets, which is good for this problem since there are now 81,344 points.\n\nHyperparameter Tuning\nThe biggest hyperparameter for K-means is the number of clusters n_clusters. This hyperparameter is the amount of clusters to generate for the problem. Because the number of clusters largely effects the results of the model, it is important to tune this. In order to chose the best value, I loop through different values up to 80.\n\n```{python}\ninertia = []\n# train_df is the numeric representation of original_df\ntrain_df = original_df.drop(columns=['Unnamed: 0.1', 'index', 'Unnamed: 0', 'track_id',\n       'album_name', 'track_name', 'popularity', 'duration_ms', 'explicit',\n       'danceability', 'energy', 'loudness', 'mode', 'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n       'time_signature'])\n\nfor col in train_df.columns:\n    if not pd.api.types.is_numeric_dtype(train_df[col]):\n        train_df[col] = pd.factorize(original_df[col])[0]\n\nscaler = StandardScaler()\n# df_scaled is the scaled version of train_df\ndf_scaled = scaler.fit_transform(train_df)\npca_num_components = 2\n\n# df_pca to reduce dimensionality\npca = PCA(n_components=pca_num_components).fit_transform(df_scaled)\ndf_pca = pd.DataFrame(pca,columns=['pca1','pca2'])\n\nfor k in range(1, 80, 10):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit_predict(df_pca)\n    inertia.append(kmeans.inertia_)\n```\n\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n```{python}\nplt.plot(range(1, 80, 10), inertia, marker='o')\nplt.title('Elbow Method for Optimal K')\nplt.xlabel('Number of Clusters (K)')\nplt.ylabel('Inertia (Within-Cluster Sum of Squares)')\nplt.show()\n```\n\n\n\n\nThe elbow chart is a great way to visualize intertia vs number of clusters on the dataset. Since our goal is to generalize well, it’s not the best to choose the “lowest” inertia value. It is generally recommended in practice to choose the “elbow point”; I chose 10 as this looks very close to an elbow point for this distribution. Although, one drawback to this approach is its subjectiveness– you might think the elbow point is 12, whereas I think the elbow point is 10.\n\n\nK-Means for Spotify\nAfter taking the resulting elbow point, I run that through my own instance of kmeans, utilizing the Sklearn library, and store the predicted results into the original dataframe.\n\n```{python}\nkmeans = KMeans(n_clusters=10, random_state=42)\noriginal_df['clusters'] = kmeans.fit_predict(df_pca)\n```\n\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n```{python}\nsns.scatterplot(x=\"pca1\", y=\"pca2\", hue=original_df['clusters'], data=df_pca)\nplt.title('K-means Clustering PCA of 3 features [track_genre, artists, key]')\nplt.show()\n```\n\n\n\n\nThis is a PCA visualization of the clusters on the feature set track_genre, artists and key."
  },
  {
    "objectID": "ClusteringBlog/index.html#evaluating-k-means-for-spotify",
    "href": "ClusteringBlog/index.html#evaluating-k-means-for-spotify",
    "title": "Spotify Recommendation System With Clustering",
    "section": "Evaluating K-Means for Spotify",
    "text": "Evaluating K-Means for Spotify\nBelow are some sample mini-clusters. Since the goal of this overall problem is to recommend music based on certain songs, I decided to create a function that grabs an entry from the CSV file, finds the cluster it’s in, and computes the k-nearest neighbors of that song. These nearest neighbors would be the “recommendation” songs, in order.\nThe general idea we should see with these mini-clusters are songs that resemble the query song. In the case of the first example, I ran my function on Daughtry’s song “Home”. The recommended song (top 1) example was another Daughtry song “It’s Not Over”.\nWhen testing out different K-means implementations on different features, I found that simplicity is key. Having a ton of features is great for any dataset, but knowing how they interact with each other and how to simplify the problem makes for better results. I tested many different subsests of features including:\n\nall of the original dataset features (n=20)\nsubset of continuous variables\nsubset of just track_genre and artists\nsubset of track_genre, artists, tempo, and key. All of which are discrete, factual features.\nsubset of track_genre, artists, and key.\n\nMy final result ended up being the last option, although those did not generate the most similar clusters, especially compared to option 3. Although, I chose the last option as I was trying to find similar songs while spanning across other artists. Option 5 seemed to give me similar options across at least one or more genres with different artists. It is worth noting that some of the results gave me the same artists, which is good since those are similar songs too.\n\n```{python}\noriginal_df['Distance_to_Centroid'] = kmeans.transform(df_pca).min(axis=1)\n\ndef get_nearest_entry(idx, k=5):\n    # print(original_df.iloc[idx])\n    # print(train_df.iloc[idx])\n    cluster = kmeans.predict(df_pca.iloc[idx].to_frame().T)[0]\n    cluster_data = original_df[original_df[\"clusters\"] == cluster]\n    cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\n    cluster_data = cluster_data.sort_values(by=\"closest_entries_to_idx\")\n    # print(cluster_data[[\"artists\", \"album_name\", \"track_name\", \"track_genre\"]])\n\n    cluster_data.drop(columns=[\"closest_entries_to_idx\"])\n    print(f\"Top {k} Closest Examples to {cluster_data.loc[idx]['artists']}'s \\\"{cluster_data.loc[idx]['track_name']}\\\"\")\n    print(cluster_data[:k][[\"artists\", \"track_name\", \"track_genre\"]])\n    print(\"\\n\\n\")\n\nget_nearest_entry(35640) # rock song\nget_nearest_entry(16587) # country song\nget_nearest_entry(41220) # rap song\n```\n\nTop 5 Closest Examples to Daughtry's \"September\"\n            artists               track_name     track_genre\n35640      Daughtry                September          grunge\n35381      Daughtry            It's Not Over          grunge\n35839    Stone Sour                 Hesitate          grunge\n55666    Mark Broom                Five/Four  minimal-techno\n40063  TNT;POPR3B3L  I'm Raving - Radio Edit       hardstyle\n\n\n\nTop 5 Closest Examples to Florida Georgia Line's \"Stay\"\n                    artists  \\\n16587  Florida Georgia Line   \n8395    Datsik;Virtual Riot   \n8582            The Prodigy   \n8819            The Prodigy   \n8529            The Prodigy   \n\n                                              track_name track_genre  \n16587                                               Stay     country  \n8395                                               Nasty   breakbeat  \n8582                                               Girls   breakbeat  \n8819                                  We Are The Ruffest   breakbeat  \n8529   Out of Space - Techno Underworld Remix Remastered   breakbeat  \n\n\n\nTop 5 Closest Examples to Future;Lil Uzi Vert's \"Tic Tac\"\n                                          artists              track_name  \\\n41220                         Future;Lil Uzi Vert                 Tic Tac   \n43994  Pritam;Arijit Singh;Shadab;Altamash Faridi  Lambiyaan Si Judaiyaan   \n41226                                    Lil Baby                  All In   \n43981     Pritam;Sukhwinder Singh;Sunidhi Chauhan                Marjaani   \n41207                    Zack Knight;Jasmin Walia         Bom Diggy Diggy   \n\n      track_genre  \n41220     hip-hop  \n43994      indian  \n41226     hip-hop  \n43981      indian  \n41207     hip-hop  \n\n\n\n\n\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_14212\\3657151199.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_14212\\3657151199.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_14212\\3657151199.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\n\n\nOption 3 on the other hand gave me different songs for the same artists, which is fine for a recommendation system, but not what I was exactly going for. Below is a visualization of the clusters with just two features as well as its predictions.\n\n```{python}\noriginal_df = pd.read_csv(\"./dataset-dedup.csv\")\n# train_df is the numeric representation of original_df\ntrain_df = original_df.drop(columns=['Unnamed: 0.1', 'index', 'Unnamed: 0', 'track_id',\n       'album_name', 'track_name', 'popularity', 'duration_ms', 'explicit',\n       'danceability', 'key', 'energy', 'loudness', 'mode', 'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n       'time_signature'])\n\nfor col in train_df.columns:\n    if not pd.api.types.is_numeric_dtype(train_df[col]):\n        train_df[col] = pd.factorize(original_df[col])[0]\n\nscaler = StandardScaler()\n# df_scaled is the scaled version of train_df\ndf_scaled = scaler.fit_transform(train_df)\npca_num_components = 2\n\n# df_pca to reduce dimensionality\npca = PCA(n_components=pca_num_components).fit_transform(df_scaled)\ndf_pca = pd.DataFrame(pca,columns=['pca1','pca2'])\n\nkmeans = KMeans(n_clusters=10, random_state=42)\noriginal_df['clusters'] = kmeans.fit_predict(df_pca)\n\nsns.scatterplot(x=\"pca1\", y=\"pca2\", hue=original_df['clusters'], data=df_pca)\nplt.title('K-means Clustering PCA of 2 features [track_genre, artists]')\nplt.show()\noriginal_df['Distance_to_Centroid'] = kmeans.transform(df_pca).min(axis=1)\nget_nearest_entry(35640) # rock song\nget_nearest_entry(16587) # country song\nget_nearest_entry(41220) # rap song\n```\n\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_14212\\3657151199.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_14212\\3657151199.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_14212\\3657151199.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\n\n\n\n\n\nTop 5 Closest Examples to Daughtry's \"September\"\n        artists            track_name track_genre\n35439  Daughtry  Waiting for Superman      grunge\n35678  Daughtry         Gone Too Soon      grunge\n35802  Daughtry            I'll Fight      grunge\n35640  Daughtry             September      grunge\n35336  Daughtry                  Home      grunge\n\n\n\nTop 5 Closest Examples to Florida Georgia Line's \"Stay\"\n                    artists         track_name track_genre\n16592  Florida Georgia Line  I Love My Country     country\n16587  Florida Georgia Line               Stay     country\n16938  Florida Georgia Line           H.O.L.Y.     country\n16598  Florida Georgia Line           Sun Daze     country\n16975  Florida Georgia Line               Life     country\n\n\n\nTop 5 Closest Examples to Future;Lil Uzi Vert's \"Tic Tac\"\n                   artists              track_name track_genre\n41220  Future;Lil Uzi Vert                 Tic Tac     hip-hop\n39123            Lionheart                  Cursed    hardcore\n39035            Lionheart                LHHC '17    hardcore\n39040              Bodyjar  A Hazy Shade of Winter    hardcore\n39033         Naked Raygun              Rat Patrol    hardcore"
  },
  {
    "objectID": "ClusteringBlog/index.html#conclusion",
    "href": "ClusteringBlog/index.html#conclusion",
    "title": "Spotify Recommendation System With Clustering",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, I used K-means as a clustering algorithm for a song recommendation system. Though K-means does a good job at coming up with clusters and generating similar examples, other clustering algorithms such as DBSCAN may be a suitable option as well. In general, what I like about clustering algorithms for this problem domain, especially K-means, is its free range to determine what logical clusters should look like and its intuitiveness. There’s not only one correct way to do K-means."
  },
  {
    "objectID": "AnomalyDetectionBlog/anomaly.html",
    "href": "AnomalyDetectionBlog/anomaly.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Author: Daniel Hassler\nfrom sklearn.datasets import load_iris\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.model_selection import ParameterGrid\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "AnomalyDetectionBlog/anomaly.html#load-iris-data",
    "href": "AnomalyDetectionBlog/anomaly.html#load-iris-data",
    "title": "Anomaly Detection",
    "section": "Load Iris Data",
    "text": "Load Iris Data\nFor the purpose of anomaly detection and being able to visualize anomalies/outliers, I ran PCA (principal component analysis) to reduce the dimensionality of the iris dataset (without targets) from shape (150, 4) to (150, 2). This allows me to visualize this dataset in two dimensions and makes clustering more efficient and representative of the true data.\n\niris = load_iris()\npca = PCA(n_components=2)\ndata_pca = pca.fit_transform(iris.data, iris.target)\ndf = pd.DataFrame(data_pca, columns=[\"PC1\", \"PC2\"])\ndf['target'] = iris.target\nprint(df)\n\n          PC1       PC2  target\n0   -2.684126  0.319397       0\n1   -2.714142 -0.177001       0\n2   -2.888991 -0.144949       0\n3   -2.745343 -0.318299       0\n4   -2.728717  0.326755       0\n..        ...       ...     ...\n145  1.944110  0.187532       2\n146  1.527167 -0.375317       2\n147  1.764346  0.078859       2\n148  1.900942  0.116628       2\n149  1.390189 -0.282661       2\n\n[150 rows x 3 columns]"
  },
  {
    "objectID": "AnomalyDetectionBlog/anomaly.html#plot-the-current-iris-data",
    "href": "AnomalyDetectionBlog/anomaly.html#plot-the-current-iris-data",
    "title": "Anomaly Detection",
    "section": "Plot the Current Iris Data",
    "text": "Plot the Current Iris Data\n\n_, ax = plt.subplots()\nscatter = ax.scatter(df[\"PC1\"], df[\"PC2\"], c=iris.target, cmap=\"copper\")\nax.set(xlabel=\"PC1\", ylabel=\"PC2\")\n_ = ax.legend(\n    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n)"
  },
  {
    "objectID": "AnomalyDetectionBlog/anomaly.html#initialize-the-dbscan",
    "href": "AnomalyDetectionBlog/anomaly.html#initialize-the-dbscan",
    "title": "Anomaly Detection",
    "section": "Initialize the DBSCAN",
    "text": "Initialize the DBSCAN\nIn order to create realistic clusters with DBSCAN and maximize an optimization, we need to preform hyperparameter tuning. Below is a GridSearch implementation for tweaking and finding the best eps and min_samples hyperparameters.\n\nparams = {\n    'eps': [i / 10 for i in range(1, 15)],\n    'min_samples': [i for i in range(1, 10)]\n}\n\nbest_score = -1\nbest_params = {}\n\nfor param_i in ParameterGrid(params):\n    db = DBSCAN(**param_i)\n    labels = db.fit_predict(data_pca)\n    # minimum of 4 clusters (3 classes + 1 outlier)\n    if len(np.unique(labels)) &lt;= 3:\n        continue\n\n    score = silhouette_score(data_pca, labels)\n    if score &gt; best_score:\n        best_score = score\n        best_params = param_i\n\nprint(\"Best Score: \", best_score)\nprint(\"Best Params: \", best_params)\n\n\n\nBest Score:  0.45189188489361554\nBest Params:  {'eps': 0.3, 'min_samples': 6}\n\n\n\ndb = DBSCAN(**best_params).fit(data_pca)\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint(\"Estimated number of clusters: %d\" % n_clusters_)\nprint(\"Estimated number of noise points: %d\" % n_noise_)\n\nEstimated number of clusters: 3\nEstimated number of noise points: 20"
  },
  {
    "objectID": "AnomalyDetectionBlog/anomaly.html#plot-the-dbscan",
    "href": "AnomalyDetectionBlog/anomaly.html#plot-the-dbscan",
    "title": "Anomaly Detection",
    "section": "Plot the DBSCAN",
    "text": "Plot the DBSCAN\n\ny_pred = db.fit_predict(data_pca)\nfig, ax = plt.subplots()\nscatter = ax.scatter(df[\"PC1\"], df[\"PC2\"], c=y_pred, cmap='copper')\ntext_labels = [\"outlier\", \"setosa\", \"versicolor\", \"virginica\"]\nlegend1 = ax.legend(scatter.legend_elements()[0], text_labels,\n                    loc=\"lower right\", title=\"Classes\")\nax.add_artist(legend1)\nplt.title(\"DBSCAN of Iris Data\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\n\nText(0, 0.5, 'PC2')"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Daniel Hassler, and I am currently a Master’s in Computer Science student at Virginia Tech focusing in AI/ML. My academic areas of interest fall under AI/ML, robotics, and networking.\nThis fall, I am involved in interesting areas of research for a few of my classes. In my “Nautral Language Processing” (NLP) class, I am researching in-context learning for LLMs, and in my “Security in Generative AI” class, I am researching defense mechanisms against deep learning facial recognition attacks utilizing generative AI.\nPrior to being a graduate student at VT, my experience is mostly academic, as I recently graduated from James Madison University with my B.S. in Computer Science (’23). At JMU, I was part of an exciting research project where I got to help design safety features for the JMU Autonomous Golf Cart (JACart) with ML and computer vision technologies."
  },
  {
    "objectID": "AnomalyDetectionBlog/index.html",
    "href": "AnomalyDetectionBlog/index.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Author: Daniel Hassler"
  },
  {
    "objectID": "AnomalyDetectionBlog/index.html#introduction",
    "href": "AnomalyDetectionBlog/index.html#introduction",
    "title": "Anomaly Detection",
    "section": "Introduction",
    "text": "Introduction\nAnomaly detection is a common task for a lot of unsupervised learning settings, but it can also good for supervised settings as well. In the case of anomaly detection here, I will be performing DBSCAN, a clustering algorithm, on the infamous Iris dataset to detect outliers.\nFirst, I import all the necessary libraries for the project:\n\n```{python}\nfrom sklearn.datasets import load_iris\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.model_selection import ParameterGrid\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n```"
  },
  {
    "objectID": "AnomalyDetectionBlog/index.html#data",
    "href": "AnomalyDetectionBlog/index.html#data",
    "title": "Anomaly Detection",
    "section": "Data",
    "text": "Data\nFor the data, I will be using the Iris dataset taken from sklearn.datasets library. This dataset is useful for educational purposes and is an accurate representation of some real world data; it contains 150 samples of iris data and has 4 columns: Sepal Length, Sepal Width, Petal Length and Petal Width.\nFor the purpose of anomaly detection and being able to visualize anomalies/outliers, I ran PCA (principal component analysis) to reduce the dimensionality of the iris dataset (without targets) from shape (150, 4) to (150, 2). PCA finds the eigenvalues and eigenvectors of the covariance matrix of the entire dataset, and the algorithm takes the top n_components, in this case two, to represent the data in two dimensions. This allows me to visualize this dataset in two dimensions and makes clustering more efficient and representative.\n\n```{python}\niris = load_iris()\npca = PCA(n_components=2)\ndata_pca = pca.fit_transform(iris.data, iris.target)\ndf = pd.DataFrame(data_pca, columns=[\"PC1\", \"PC2\"])\ndf['target'] = iris.target\nprint(df)\n```\n\n          PC1       PC2  target\n0   -2.684126  0.319397       0\n1   -2.714142 -0.177001       0\n2   -2.888991 -0.144949       0\n3   -2.745343 -0.318299       0\n4   -2.728717  0.326755       0\n..        ...       ...     ...\n145  1.944110  0.187532       2\n146  1.527167 -0.375317       2\n147  1.764346  0.078859       2\n148  1.900942  0.116628       2\n149  1.390189 -0.282661       2\n\n[150 rows x 3 columns]\n\n\nNext, I plot the reduced dimensionality representation of the Iris dataset:\n\n```{python}\n_, ax = plt.subplots()\nscatter = ax.scatter(df[\"PC1\"], df[\"PC2\"], c=iris.target, cmap=\"copper\")\nax.set(xlabel=\"PC1\", ylabel=\"PC2\")\n_ = ax.legend(\n    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n)\n```"
  },
  {
    "objectID": "AnomalyDetectionBlog/index.html#dbscan-clustering",
    "href": "AnomalyDetectionBlog/index.html#dbscan-clustering",
    "title": "Anomaly Detection",
    "section": "DBSCAN Clustering",
    "text": "DBSCAN Clustering\nDensity-Based Spatial Clustering Of Applications With Noise (DBSCAN) is a clustering algorithm that groups each point into a neighborhood, given a radius (eps) and a minimum number of points (min_samples). This is more representative for applications with real-life data, especially since they can contain noise.\n\nHyperparameter Tuning\nIn order to create realistic clusters with DBSCAN and maximize an optimization, we need to preform hyperparameter tuning. Below is a GridSearch implementation for tweaking and finding the best eps and min_samples hyperparameters.\n\n```{python}\nparams = {\n    'eps': [i / 10 for i in range(1, 15)],\n    'min_samples': [i for i in range(1, 10)]\n}\n\nbest_score = -1\nbest_params = {}\n\nfor param_i in ParameterGrid(params):\n    db = DBSCAN(**param_i)\n    labels = db.fit_predict(data_pca)\n    # minimum of 4 clusters (3 classes + 1 outlier)\n    if len(np.unique(labels)) &lt;= 3:\n        continue\n    curr_score = silhouette_score(data_pca, labels)\n    if curr_score &gt; best_score:\n        best_score = curr_score\n        best_params = param_i\n\nprint(\"Best Score: \", best_score)\nprint(\"Best Params: \", best_params)\n```\n\nBest Score:  0.45189188489361554\nBest Params:  {'eps': 0.3, 'min_samples': 6}\n\n\n\n\nDBSCAN Initialization and Visualization\nNext, I plugged in the “best” hyperparameters to the DBSCAN object generated from the GridSearch, and visualized the DBSCAN clusters with the outliers in the next codeblock.\n\n```{python}\ndb = DBSCAN(**best_params).fit(data_pca)\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint(\"Estimated number of clusters: %d\" % n_clusters_)\nprint(\"Estimated number of noise points: %d\" % n_noise_)\n```\n\nEstimated number of clusters: 3\nEstimated number of noise points: 20\n\n\n\n```{python}\ny_pred = db.fit_predict(data_pca)\nfig, ax = plt.subplots()\nscatter = ax.scatter(df[\"PC1\"], df[\"PC2\"], c=y_pred, cmap='copper')\ntext_labels = [\"outlier\", \"setosa\", \"versicolor\", \"virginica\"]\nlegend1 = ax.legend(scatter.legend_elements()[0], text_labels,\n                    loc=\"lower right\", title=\"Classes\")\nax.add_artist(legend1)\nplt.title(\"DBSCAN of Iris Data\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\n```\n\nText(0, 0.5, 'PC2')\n\n\n\n\n\nGiven the best hyperparameters, with realistic limitations of at least 4 clusters (3 classes, 1 outlier), the clusters look roughly similar to the expected classifications. The great advantage to DBSCAN is the ability to come up with these clusters without knowledge of the original labels (unsupervised), and based on the visualization of all this, it’s roughly representative of the actual data.\n\n\nOutliers\nThe outliers from the above visualization are represented in black. Based on the configuration of the DBSCAN object, it produced three main clusters, one for each label, and a predicted outlier one. It is important to note that the outliers in the graph are past the farthest ends of the main clusters, which is truly representative of outliers/anomalies. To emphasize further, the strength and quantity of such outliers in a DBSCAN cluster is heavily dependent on its hyperparameter setup."
  },
  {
    "objectID": "AnomalyDetectionBlog/index.html#improvements",
    "href": "AnomalyDetectionBlog/index.html#improvements",
    "title": "Anomaly Detection",
    "section": "Improvements",
    "text": "Improvements\nThere are some improvements we can make to DBSCAN. One improvement would include more data samples, as more data CAN further improve the clusters and limit some outliers. Another improvement we can make is exploring the hyperparameters further with finer eps values."
  },
  {
    "objectID": "ClusteringBlog/spotify.html",
    "href": "ClusteringBlog/spotify.html",
    "title": "Spotify Recommendation System",
    "section": "",
    "text": "Author: Daniel Hassler\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.model_selection import train_test_split, ParameterGrid\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score"
  },
  {
    "objectID": "ClusteringBlog/spotify.html#data-analysis",
    "href": "ClusteringBlog/spotify.html#data-analysis",
    "title": "Spotify Recommendation System",
    "section": "Data Analysis",
    "text": "Data Analysis\nLook at -&gt; * https://github.com/ageron/handson-ml/blob/master/08_dimensionality_reduction.ipynb * https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset/code\n\noriginal_df = pd.read_csv(\"./dataset-dedup.csv\")\nprint(original_df.columns)\n# original_df = original_df.drop_duplicates(subset=[\"artists\", \"track_name\"], keep=\"first\").reset_index()\nprint(original_df.shape)\n# original_df.to_csv(\"./dataset-dedup.csv\")\nfeatures_x = [\"loudness\", \"popularity\", \"duration_ms\"]\nfeatures_y = [\"popularity\", \"energy\", \"tempo\"]\n\nfor i, (x,y) in enumerate(zip(features_x, features_y)):\n    scatter = sns.scatterplot(x=x, y=y, hue='track_genre', data=original_df, palette=\"viridis\", alpha=0.25)\n    legend_labels = original_df['track_genre'].unique()# [:3]  # Show only the first 3 genres\n    scatter.legend(title='Genre', labels=legend_labels, prop={'size': 1})\n    plt.title(f\"Scatter Plot of {x} vs {y} by genre\")\n    plt.show()\n\nplt.show()\n\nIndex(['Unnamed: 0.1', 'index', 'Unnamed: 0', 'track_id', 'artists',\n       'album_name', 'track_name', 'popularity', 'duration_ms', 'explicit',\n       'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n       'time_signature', 'track_genre'],\n      dtype='object')\n(81344, 23)\n\n\n\n\n\n\n\n\n\n\n\n\nunique_vals = original_df['track_genre'].unique()\nplt.bar(unique_vals, original_df['track_genre'].value_counts())\nplt.title(\"Occurrence of Genre\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Genre\")\n_ = plt.xticks(rotation=\"vertical\", fontsize=4)"
  },
  {
    "objectID": "ClusteringBlog/spotify.html#k-means-clustering",
    "href": "ClusteringBlog/spotify.html#k-means-clustering",
    "title": "Spotify Recommendation System",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\n\nHyperparameter Tuning for K-Means\n\ninertia = []\n# train_df is the numeric representation of original_df\ntrain_df = original_df.drop(columns=['Unnamed: 0.1', 'index', 'Unnamed: 0', 'track_id',\n       'album_name', 'track_name', 'popularity', 'duration_ms', 'explicit',\n       'danceability', 'energy', 'loudness', 'mode', 'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n       'time_signature'])\n\nfor col in train_df.columns:\n    if not pd.api.types.is_numeric_dtype(train_df[col]):\n        train_df[col] = pd.factorize(original_df[col])[0]\n\nscaler = StandardScaler()\n# df_scaled is the scaled version of train_df\ndf_scaled = scaler.fit_transform(train_df)\npca_num_components = 2\n\n# df_pca to reduce dimensionality\npca = PCA(n_components=pca_num_components).fit_transform(df_scaled)\ndf_pca = pd.DataFrame(pca,columns=['pca1','pca2'])\n\nfor k in range(1, 80, 10):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit_predict(df_pca)\n    inertia.append(kmeans.inertia_)\n\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\nPlotting the Elbow Chart\n\n# Plot the elbow curve\nplt.plot(range(1, 80, 10), inertia, marker='o')\nplt.title('Elbow Method for Optimal K')\nplt.xlabel('Number of Clusters (K)')\nplt.ylabel('Inertia (Within-Cluster Sum of Squares)')\nplt.show()\n\n\n\n\n\n\nChoosing K-Means with “Elbow” Point\n\nkmeans = KMeans(n_clusters=10, random_state=42)\noriginal_df['clusters'] = kmeans.fit_predict(df_pca)\n\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\nsns.scatterplot(x=\"pca1\", y=\"pca2\", hue=original_df['clusters'], data=df_pca)\nplt.title('K-means Clustering PCA of 3 features [track_genre, artists, key]')\nplt.show()"
  },
  {
    "objectID": "ClusteringBlog/spotify.html#evaluation",
    "href": "ClusteringBlog/spotify.html#evaluation",
    "title": "Spotify Recommendation System",
    "section": "Evaluation",
    "text": "Evaluation\n\noriginal_df['Distance_to_Centroid'] = kmeans.transform(df_pca).min(axis=1)\n\n\ndef get_nearest_entry(idx, k=5):\n    # print(original_df.iloc[idx])\n    # print(train_df.iloc[idx])\n    cluster = kmeans.predict(df_pca.iloc[idx].to_frame().T)[0]\n    cluster_data = original_df[original_df[\"clusters\"] == cluster]\n    cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\n    cluster_data = cluster_data.sort_values(by=\"closest_entries_to_idx\")\n    # print(cluster_data[[\"artists\", \"album_name\", \"track_name\", \"track_genre\"]])\n\n    cluster_data.drop(columns=[\"closest_entries_to_idx\"])\n    print(f\"Top {k} Closest Examples to {cluster_data.loc[idx]['artists']}'s \\\"{cluster_data.loc[idx]['track_name']}\\\"\")\n    print(cluster_data[:k][[\"artists\", \"track_name\", \"track_genre\"]])\n    print(\"\\n\\n\")\n\nget_nearest_entry(35640) # rock song\nget_nearest_entry(16587) # country song\nget_nearest_entry(41220) # rap song\n\nTop 5 Closest Examples to Daughtry's \"September\"\n            artists               track_name     track_genre\n35640      Daughtry                September          grunge\n35381      Daughtry            It's Not Over          grunge\n35839    Stone Sour                 Hesitate          grunge\n55666    Mark Broom                Five/Four  minimal-techno\n40063  TNT;POPR3B3L  I'm Raving - Radio Edit       hardstyle\n\n\n\nTop 5 Closest Examples to Florida Georgia Line's \"Stay\"\n                    artists  \\\n16587  Florida Georgia Line   \n8395    Datsik;Virtual Riot   \n8582            The Prodigy   \n8819            The Prodigy   \n8529            The Prodigy   \n\n                                              track_name track_genre  \n16587                                               Stay     country  \n8395                                               Nasty   breakbeat  \n8582                                               Girls   breakbeat  \n8819                                  We Are The Ruffest   breakbeat  \n8529   Out of Space - Techno Underworld Remix Remastered   breakbeat  \n\n\n\nTop 5 Closest Examples to Future;Lil Uzi Vert's \"Tic Tac\"\n                                          artists              track_name  \\\n41220                         Future;Lil Uzi Vert                 Tic Tac   \n43994  Pritam;Arijit Singh;Shadab;Altamash Faridi  Lambiyaan Si Judaiyaan   \n41226                                    Lil Baby                  All In   \n43981     Pritam;Sukhwinder Singh;Sunidhi Chauhan                Marjaani   \n41207                    Zack Knight;Jasmin Walia         Bom Diggy Diggy   \n\n      track_genre  \n41220     hip-hop  \n43994      indian  \n41226     hip-hop  \n43981      indian  \n41207     hip-hop  \n\n\n\n\n\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_7112\\101083667.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_7112\\101083667.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_7112\\101083667.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\n\n\n\noriginal_df = pd.read_csv(\"./dataset-dedup.csv\")\n# train_df is the numeric representation of original_df\ntrain_df = original_df.drop(columns=['Unnamed: 0.1', 'index', 'Unnamed: 0', 'track_id',\n       'album_name', 'track_name', 'popularity', 'duration_ms', 'explicit',\n       'danceability', 'key', 'energy', 'loudness', 'mode', 'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n       'time_signature'])\n\nfor col in train_df.columns:\n    if not pd.api.types.is_numeric_dtype(train_df[col]):\n        train_df[col] = pd.factorize(original_df[col])[0]\n\nscaler = StandardScaler()\n# df_scaled is the scaled version of train_df\ndf_scaled = scaler.fit_transform(train_df)\npca_num_components = 2\n\n# df_pca to reduce dimensionality\npca = PCA(n_components=pca_num_components).fit_transform(df_scaled)\ndf_pca = pd.DataFrame(pca,columns=['pca1','pca2'])\n\nkmeans = KMeans(n_clusters=10, random_state=42)\noriginal_df['clusters'] = kmeans.fit_predict(df_pca)\n\nsns.scatterplot(x=\"pca1\", y=\"pca2\", hue=original_df['clusters'], data=df_pca)\nplt.title('K-means Clustering PCA of 2 features [track_genre, artists]')\nplt.show()\noriginal_df['Distance_to_Centroid'] = kmeans.transform(df_pca).min(axis=1)\nget_nearest_entry(35640) # rock song\nget_nearest_entry(16587) # country song\nget_nearest_entry(41220) # rap song\n\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_7112\\101083667.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_7112\\101083667.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_7112\\101083667.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\n\n\n\n\n\nTop 5 Closest Examples to Daughtry's \"September\"\n        artists            track_name track_genre\n35439  Daughtry  Waiting for Superman      grunge\n35678  Daughtry         Gone Too Soon      grunge\n35802  Daughtry            I'll Fight      grunge\n35640  Daughtry             September      grunge\n35336  Daughtry                  Home      grunge\n\n\n\nTop 5 Closest Examples to Florida Georgia Line's \"Stay\"\n                    artists         track_name track_genre\n16592  Florida Georgia Line  I Love My Country     country\n16587  Florida Georgia Line               Stay     country\n16938  Florida Georgia Line           H.O.L.Y.     country\n16598  Florida Georgia Line           Sun Daze     country\n16975  Florida Georgia Line               Life     country\n\n\n\nTop 5 Closest Examples to Future;Lil Uzi Vert's \"Tic Tac\"\n                   artists              track_name track_genre\n41220  Future;Lil Uzi Vert                 Tic Tac     hip-hop\n39123            Lionheart                  Cursed    hardcore\n39035            Lionheart                LHHC '17    hardcore\n39040              Bodyjar  A Hazy Shade of Winter    hardcore\n39033         Naked Raygun              Rat Patrol    hardcore"
  },
  {
    "objectID": "DecisionTreeRFBlog/index.html",
    "href": "DecisionTreeRFBlog/index.html",
    "title": "Comparing Decision Tree and Random Forest Classifier",
    "section": "",
    "text": "Author: Daniel Hassler"
  },
  {
    "objectID": "DecisionTreeRFBlog/index.html#sample-data-used-in-classification",
    "href": "DecisionTreeRFBlog/index.html#sample-data-used-in-classification",
    "title": "Comparing Decision Tree and Random Forest Classifier",
    "section": "Sample Data Used in Classification",
    "text": "Sample Data Used in Classification\nTo compare a DecisionTree and a RandomForestClassifier, the first step I took was to gather some data and run some visualizations and analysis. Through Kaggle, I was able to obtain a small dataset on person features and their BMI (Body Mass Index) data. The data consists of just around 400 samples with features: gender, height, and weight, and the goal is to predict BMI.\n\n```{python}\nimport numpy as np\nimport sklearn\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, learning_curve, LearningCurveDisplay\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n```\n\n\n```{python}\ndata = pd.read_csv(\"./datasets/bmi_train.csv\")\ncategory_mapping = {'Male': 0, 'Female': 1}\ndata['Gender_Encoded'] = data['Gender'].map(category_mapping) # converts categorical data to numeric data.\nX = data.drop(['Gender','Index'], axis=1)\ny = data.drop(['Gender', 'Gender_Encoded', 'Height', 'Weight'], axis=1)\nprint(\"All X shape: \", X.shape)\nprint(\"All y shape: \", y.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nprint(\"X_train shape: \", X_train.shape)\nprint(\"y_train shape: \", y_train.shape)\nprint(\"X_test shape: \", X_test.shape)\nprint(\"y_test shape: \", y_test.shape)\n```\n\nAll X shape:  (400, 3)\nAll y shape:  (400, 1)\nX_train shape:  (320, 3)\ny_train shape:  (320, 1)\nX_test shape:  (80, 3)\ny_test shape:  (80, 1)\n\n\nIn the above code snippet, I first populated my data into a Pandas dataframe and then split up the data into a “training” and “testing” datasets. I decided to go with an 80/20% split between train and test (with its corresponding labels), as that seems to be the most standard approach in the industry. The significant benefit here is that I possess labeled data on both sets, a challenge in practice. This enables me to make comparisons between predictions and outcomes on my data, eliminating the need to procure any additional “test” data.\n\n```{python}\nplt.figure(figsize=(6, 6))\nsns.scatterplot(data=data, x='Height', y='Weight', hue='Index', palette='deep')\nplt.title('Scatter Plot of Height vs Weight')\nplt.show()\n```\n\n\n\n\nNext, I created a scatterplot showing the distribution of the entire dataset (n=400) to find linear associations. Based on the scatterplot above, I was roughly able to see that there was a class imbalance.\n\n```{python}\n# Class imbalance, more obesity.\nunique_values, counts = np.unique(y, return_counts=True)\nplt.bar(unique_values, counts)\nplt.title(\"BMI Classes in the Entire Dataset\")\nplt.xlabel(\"BMI Class\")\nplt.ylabel(\"Occurences in Entire Dataset\")\nplt.show()\n```\n\n\n\n\nThe labels are all discrete and sequential, consisting of whole numbers between 0 and 5, further enforcing my intuition for using a classifier approach. A “0” in my case represents someone with an exeptionally low BMI, whereas a “5” depicts an exceptionally high BMI. Based on the distribution of the data, there appears to be a huge class imbalance, heavily favoring the amount of exceptionally high instances in the dataset; this was something I needed to keep in mind when building the classifiers for this dataset.\n\n```{python}\ncorrelation_matrix = data.corr()\n\n# Display a heatmap of the correlation matrix\nplt.figure(figsize=(10, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('Correlation Heatmap')\nplt.show()\n```\n\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_18988\\1253239144.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  correlation_matrix = data.corr()\n\n\n\n\n\nThe correlation matrix depicts the correlation between features (height, weight, gender, BMI) in the dataset. It uses the pearson’s correlation coefficient to compute this: \\[\nr =\n  \\frac{ \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y}) }{\n        \\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^{n}(y_i-\\bar{y})^2}}\n\\]\nBased on the features presented, most are not correlated strongly, but there is a glaring strong correlation between weight and BMI. It is also important to note that gender doesn’t influence classification results, as the key factors to determining BMI is height and weight."
  },
  {
    "objectID": "DecisionTreeRFBlog/index.html#decisiontreeclassifier",
    "href": "DecisionTreeRFBlog/index.html#decisiontreeclassifier",
    "title": "Comparing Decision Tree and Random Forest Classifier",
    "section": "DecisionTreeClassifier",
    "text": "DecisionTreeClassifier\nIn order to start the model building process, I decided to tune the hyperparamters first by running a GridSearch\n\n```{python}\nparam_grid = {\n    'max_depth': [i for i in range(2, 6)],\n    'min_samples_leaf': [2 ** i for i in range(0, 6)],\n    'criterion': [\"entropy\", \"gini\"]\n}\n\ndt = DecisionTreeClassifier(random_state=42)\n\ngrid_search_dt = GridSearchCV(dt, param_grid, cv=StratifiedKFold(n_splits=5), scoring='accuracy')\ngrid_search_dt.fit(X_train, y_train)\nbest_params_dt = grid_search_dt.best_params_\nprint(\"Best Hyperparameters:\", best_params_dt)\nprint(\"Best Score:\", grid_search_dt.best_score_)\n```\n\nBest Hyperparameters: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1}\nBest Score: 0.784375\n\n\nI recognized that max_depth was an important hyperparameter for the DecisionTree (DT), as the depth of the tree heavily influences overfitting, but other hyperparameters are important as well, such as:\n\nmin_samples_leaf: the minimum amount of samples needed in a leaf node of the DT. For example, when min_samples_leaf is set to 10, that means a node won’t split if it has fewer than 10 samples. When this number is higher, the model can create a more generalized tree, although, when the number is smaller, it’ll create more specific splits, resulting in a more complex tree (more potential for overfitting).\ncriterion: this hyperparameter chooses whether to use entropy or Gini index as a way to calculate dissimilarity in a node. I found that in most cases, entropy outpreformed the Gini index. \\[\nEntropy(C) = -\\sum_{c=1}^Cp(c)\\log(p(c))\n\\]\n\n\\[\nGini(C) = 1 - \\sum_{c=1}^Cp(c)^2\n\\]\nNow that I’ve determined the necessary hyperparameters for this classifier, I initialize the GridSearchCV object to analyze every combination of the above hyperparameters. Within its search, it goes through an important cross-validation step (cv) that splits the training data into multiple folds and iterates through each fold for each hyperparameter combination.\nThere were a few options I could’ve chose from for the cv parameter in GridSearchCV, but in order to account for class imbalance like I stated earlier, I decided to go with a StratifiedKFold cross-validator. StratifiedKFold accounts for class label imbalance by keeping an equal precentage of classes for training and testing represented in the dataset. Below is a picture representing this:\n\n\n```{python}\ndt = DecisionTreeClassifier(max_depth=best_params_dt[\"max_depth\"], min_samples_leaf=best_params_dt[\"min_samples_leaf\"], criterion=best_params_dt[\"criterion\"])\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\n```\n\nI then created a DecisionTreeClassifier with the ‘best’ tuned hyperparameters from the above grid search and populated the y_pred array with the predictions from the test dataset. After that, I plotted the tree out using Sklearn’s plot_tree method.\n\n```{python}\nplt.figure(figsize=(10, 10))\nplot_tree(dt, feature_names=X_train.columns.tolist(), class_names=['0', '1', '2','3','4','5'], filled=True)\nplt.show()\n```\n\n\n\n\nAfter plotting the tree, I created a confusion matrix, showing where my predictions fell. Currently, the model sits around 75-86% accurate due to the above hyperparameter values and the randomly generated tree with those hyperparameter values. Not bad for a small dataset with class imbalance.\n\n```{python}\nprint(\"Micro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='micro'))\nprint(\"Macro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='macro'))\n```\n\nMicro F1:  0.6875\nMacro F1:  0.6401709401709401\n\n\n\n```{python}\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nconf_df = pd.DataFrame(confusion_matrix, index=[f\"{i}\" for i in range(6)], columns=[f\"{i}\" for i in range(6)])\nheatmap = sns.heatmap(conf_df, annot=True, fmt=\"d\", linewidths=0.35, cmap=\"YlGnBu\")\nplt.title(f\"Model Predictions With {(np.sum(confusion_matrix.diagonal()) / y_test.shape[0]) * 100:.2f}% Accuracy\")\n```\n\nText(0.5, 1.0, 'Model Predictions With 68.75% Accuracy')"
  },
  {
    "objectID": "DecisionTreeRFBlog/index.html#randomforestclassifer-ensemble-approach",
    "href": "DecisionTreeRFBlog/index.html#randomforestclassifer-ensemble-approach",
    "title": "Comparing Decision Tree and Random Forest Classifier",
    "section": "RandomForestClassifer (Ensemble approach)",
    "text": "RandomForestClassifer (Ensemble approach)\nAs above with the DecisionTreeClassifer, I first started to implement the RandomForestClassifier by tuning the hyperparameter values. Since a RandomForest is just a collection of DecisionTrees, RandomForestClassifiers, like a DecisionTreeClassifier, have mostly the same hyperparameters, but the RandomForestClassifier has an extra one for the amount of DecisionTrees that should be included in the forest (n_estimators).\nThough this step wasn’t as necessary, since I already did the hyperparameter tuning part for the DecisionTree, but I decided to include it again for the RandomForest with the number of estimators.\nIt is important to note that the n_estimators hyperparameter won’t cause the model to overfit. In fact, it actually does better at generalization when increasing the number of estimators due to the diversity of opinions the model presents for each unique DecisionTree. The only way overfitting can happen in a RandomForest depends on how the underlying DecisionTrees are set up, not the quantity of them.\n\n```{python}\nparam_grid = {\n    'max_depth': [i for i in range(2, 6)],\n    'min_samples_leaf': [2 ** i for i in range(0, 6)],\n    'criterion': [\"entropy\", \"gini\"]\n}\n\nrf = RandomForestClassifier(random_state=42)\n\nprint(y_train.shape)\ngrid_search_rf = GridSearchCV(rf, param_grid, cv=StratifiedKFold(n_splits=5), scoring='accuracy')\ngrid_search_rf.fit(X_train, y_train.values.ravel())\nbest_params_rf = grid_search_rf.best_params_\nprint(\"Best Hyperparameters:\", best_params_rf)\nprint(\"Best Score:\", grid_search_rf.best_score_)\n```\n\n(320, 1)\nBest Hyperparameters: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1}\nBest Score: 0.809375\n\n\n\n```{python}\nrf = RandomForestClassifier(n_estimators=300, \n                            max_depth=best_params_rf[\"max_depth\"], \n                            min_samples_leaf=best_params_rf[\"min_samples_leaf\"],\n                            criterion=best_params_rf[\"criterion\"])\nrf.fit(X_train, y_train.values.ravel())\ny_pred = rf.predict(X_test)\n```\n\nThe above code snippet creates the RandomForestClassifier with the same hyperparameters as the DecisionTree, in addition to the number of estimators (number of decision trees in the forest), trains the classifier, then stores a prediction array.\nHere is a visualizaiton of a subset of DecisionTrees in this RandomForest:\n\n```{python}\n    fig, axes = plt.subplots(nrows = 1,ncols = 5,figsize = (10,3), dpi=250)\n    for index in range(5):\n        tree.plot_tree(rf.estimators_[index],\n                    feature_names = X_train.columns.tolist(), \n                    class_names= [f\"{i}\" for i in range(6)],\n                    filled = True,\n                    ax = axes[index])\n\n        axes[index].set_title('Estimator: ' + str(index + 1), fontsize = 10)\n    plt.show()\n```\n\n\n\n\nAfter running the model, I checked the accuracy output of the prediction array and found that the RandomForestClassifier was able to increase the accuracy of the predictions by a considerable amount on average.\n\n```{python}\nprint(\"Micro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='micro'))\nprint(\"Macro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='macro'))\n```\n\nMicro F1:  0.7875\nMacro F1:  0.738680094980758\n\n\n\n```{python}\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nconf_df = pd.DataFrame(confusion_matrix, index=[f\"{i}\" for i in range(6)], columns=[f\"{i}\" for i in range(6)])\nheatmap = sns.heatmap(conf_df, annot=True, fmt=\"d\", linewidths=0.35, cmap=\"YlGnBu\")\nplt.title(f\"Model Predictions With {(np.sum(confusion_matrix.diagonal()) / y_test.shape[0]) * 100:.2f}% Accuracy\")\n```\n\nText(0.5, 1.0, 'Model Predictions With 78.75% Accuracy')\n\n\n\n\n\nFinally, I decided to calculate the accuracy preformance on multiple samples of RandomForestClassifiers and DecisionTrees at the same time and plot them out in a line chart.\n\n```{python}\n'''\nPlot a graph that compares the two models, randomly generated with tuned hyperparameter models\n'''\ndt_results = []\nrf_results = []\nn_samples = 40\nindexes = [i for i in range(n_samples)]\nfor i in indexes:\n    dt = DecisionTreeClassifier(max_depth=best_params_dt[\"max_depth\"], \n                   min_samples_leaf=best_params_dt[\"min_samples_leaf\"], criterion=best_params_dt[\"criterion\"])\n    dt.fit(X_train, y_train)\n    y_pred_dt = dt.predict(X_test)\n    \n    rf = RandomForestClassifier(n_estimators=300, \n                            max_depth=best_params_rf[\"max_depth\"], \n                            min_samples_leaf=best_params_rf[\"min_samples_leaf\"],\n                            criterion=best_params_rf[\"criterion\"])\n    rf.fit(X_train, y_train.values.ravel())\n    y_pred_rf = rf.predict(X_test)\n    \n    confusion_matrix_dt = sklearn.metrics.confusion_matrix(y_test, y_pred_dt)\n    confusion_matrix_rf = sklearn.metrics.confusion_matrix(y_test, y_pred_rf)\n    \n    dt_results.append((np.sum(confusion_matrix_dt.diagonal()) / y_test.shape[0]) * 100)\n    rf_results.append((np.sum(confusion_matrix_rf.diagonal()) / y_test.shape[0]) * 100)\n\nplt.plot(indexes, dt_results, label=\"DT results\")\nplt.plot(indexes, rf_results, label=\"RF results\")\nplt.xlabel(\"Sample\")\nplt.ylabel(\"Accuracy on Test Data in %\")\nplt.title(\"Accuracy Comparison Between DT and RF on Randomly Generated Models\")\nplt.legend()\nplt.show()\n```"
  },
  {
    "objectID": "DecisionTreeRFBlog/index.html#model-improvements",
    "href": "DecisionTreeRFBlog/index.html#model-improvements",
    "title": "Comparing Decision Tree and Random Forest Classifier",
    "section": "Model Improvements",
    "text": "Model Improvements\nNow that I’ve determined RandomForestClassifier as an overall better approach for this problem, I’ve included more ways to improve the current implementation.\nEarlier, I stated that gender may be a redudndant feature based on the correlation matrix, so I decided to drop that in the dataset when training the model.\n\n```{python}\nprint(X_train.shape)\nprint(y_train.shape)\n\nparam_grid = {\n    'max_depth': [i for i in range(2, 6)],\n    'min_samples_leaf': [2 ** i for i in range(0, 6)],\n    'criterion': [\"entropy\", \"gini\"]\n}\n\nrf = RandomForestClassifier(random_state=42)\ngrid_search_rf = GridSearchCV(rf, param_grid, cv=StratifiedKFold(n_splits=5), scoring='accuracy')\ngrid_search_rf.fit(X_train.drop(['Gender_Encoded'], axis=1), y_train.values.ravel())\nbest_params_rf = grid_search_rf.best_params_\nprint(\"Best Hyperparameters:\", best_params_rf)\nprint(\"Best Score:\", grid_search_rf.best_score_)\n\nrf = RandomForestClassifier(n_estimators=300, \n                            max_depth=best_params_rf[\"max_depth\"], \n                            min_samples_leaf=best_params_rf[\"min_samples_leaf\"],\n                            criterion=best_params_rf[\"criterion\"])\nrf.fit(X_train.drop(['Gender_Encoded'], axis=1), y_train.values.ravel())\ny_pred = rf.predict(X_test.drop(['Gender_Encoded'], axis=1))\n```\n\n(320, 3)\n(320, 1)\nBest Hyperparameters: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1}\nBest Score: 0.8375\n\n\n\n```{python}\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nconf_df = pd.DataFrame(confusion_matrix, index=[f\"{i}\" for i in range(6)], columns=[f\"{i}\" for i in range(6)])\nheatmap = sns.heatmap(conf_df, annot=True, fmt=\"d\", linewidths=0.35, cmap=\"YlGnBu\")\nplt.title(f\"Model Predictions With {(np.sum(confusion_matrix.diagonal()) / y_test.shape[0]) * 100:.2f}% Accuracy (NO GENDER)\")\n```\n\nText(0.5, 1.0, 'Model Predictions With 78.75% Accuracy (NO GENDER)')\n\n\n\n\n\n\n```{python}\nprint(\"Micro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='micro'))\nprint(\"Macro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='macro'))\n```\n\nMicro F1:  0.7875\nMacro F1:  0.7154144982465785\n\n\nIt appears that removing that feature, on average, didn’t hurt the preformance of the overall model.\nFinally, below is a learning curve showing accuracy results in respect to the number of samples in the training set. This plot is heavily dependent on the random state of the generated RandomForestClassifier and its underlying DecisionTrees. Sometimes the model is overfitting, so I tried minimizing the hyperparameter values to make sure it mostly doesn’t.\n\n```{python}\ntrain_sizes, train_scores, test_scores = learning_curve(rf, X_train, y_train.values.ravel(), cv=StratifiedKFold(n_splits=5))\ndisplay = LearningCurveDisplay(train_sizes=train_sizes,\n    train_scores=train_scores, test_scores=test_scores, score_name=\"Score\")\ndisplay.plot()\nplt.title(\"Learning Curve on RandomForestClassifer (NO GENDER)\")\nplt.show()\n```"
  },
  {
    "objectID": "DecisionTreeRFBlog/index.html#results-and-conclusions",
    "href": "DecisionTreeRFBlog/index.html#results-and-conclusions",
    "title": "Comparing Decision Tree and Random Forest Classifier",
    "section": "Results and Conclusions",
    "text": "Results and Conclusions\nAfter doing simple experimentation with these models, I have found that, on average, the RandomForestClassifier outpreforms just a singular DecisionTreeClassifier. There are several advantages to having a forest of DecisionTrees rather than a singular tree:\n\nMore generalizability due to the ensemble approach to this problem\nLimits overfitting compared to a DT\nDT has high variance and instability, so having a forest of those trees in a more collective approach would help get more opinions at least.\n\nThough there is more resource complexity with a forest, the benefits of using that over a DT is worth the tradeoff."
  },
  {
    "objectID": "NaiveBayesBlog/NaiveBayesNotebook.html",
    "href": "NaiveBayesBlog/NaiveBayesNotebook.html",
    "title": "Probability Theory with Naive Bayes",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport mnist\n\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn.metrics import confusion_matrix"
  },
  {
    "objectID": "NaiveBayesBlog/NaiveBayesNotebook.html#mnist-data",
    "href": "NaiveBayesBlog/NaiveBayesNotebook.html#mnist-data",
    "title": "Probability Theory with Naive Bayes",
    "section": "MNIST Data",
    "text": "MNIST Data\n\n# mnist.init()\nX_train, y_train, X_test, y_test = mnist.load()\nprint(\"X_train len: \", len(X_train))\nprint(\"X_test len: \", len(X_test))\nprint(\"X_train shape: \", X_train.shape)\nprint(\"X_test shape: \", X_test.shape)\n\nplt.subplot(1, 2, 1)\nplt.title(\"Entry in Train Set\")\nimg = X_train[0,:].reshape(28,28) # First image in the training set.\nplt.imshow(img,cmap='gray')\n\nplt.subplot(1, 2, 2)\nplt.title(\"Entry in Test Set\")\nimg = X_test[0,:].reshape(28,28) # First image in the training set.\nplt.imshow(img,cmap='gray')\nplt.show() # Show the image\n\nX_train len:  60000\nX_test len:  10000\nX_train shape:  (60000, 784)\nX_test shape:  (10000, 784)\n\n\n\n\n\n\nVisualize Every Number\n\nunique_values, indices = np.unique(y_train, return_index=True)\nprint(unique_values, indices)\n\nfor i, label_index in enumerate(indices):\n    plt.subplot(1, len(indices), i + 1)\n    img = X_train[label_index,:].reshape(28,28)\n    plt.imshow(img,cmap='gray')\nplt.show()\n    \n\n[0 1 2 3 4 5 6 7 8 9] [ 1  3  5  7  2  0 13 15 17  4]"
  },
  {
    "objectID": "NaiveBayesBlog/NaiveBayesNotebook.html#naive-bayes-model",
    "href": "NaiveBayesBlog/NaiveBayesNotebook.html#naive-bayes-model",
    "title": "Probability Theory with Naive Bayes",
    "section": "Naive Bayes Model",
    "text": "Naive Bayes Model\n\nunique, counts = np.unique(y_train, return_counts=True)\nsum_counts = np.sum(counts)\npriors = np.divide(counts, sum_counts)\n\nnb = GaussianNB(priors=priors, var_smoothing=0.1)\nnb.fit(X_train, y_train)\ny_pred = nb.predict(X_test)\nprint(y_pred)\nprint(y_test)\n\n# print(\"Priors: \", nb.priors)\n\n[7 2 1 ... 9 8 6]\n[7 2 1 ... 4 5 6]\n\n\n\nAccuracy Results\n\nconfusion = confusion_matrix(y_test, y_pred)\nconf_df = pd.DataFrame(confusion, index=[f\"{i}\" for i in range(10)], columns=[f\"{i}\" for i in range(10)])\nheatmap = sns.heatmap(conf_df, annot=True, fmt=\"d\", linewidths=0.35, cmap=\"YlGnBu\")\nplt.title(f\"Model Predictions With {(np.sum(confusion.diagonal()) / y_test.shape[0]) * 100:.2f}% Accuracy\")\n\nText(0.5, 1.0, 'Model Predictions With 81.40% Accuracy')\n\n\n\n\n\n\nmean_pixel_values = nb.theta_\nplt.figure(figsize=(5,10))\nfor i, digit in enumerate(range(10)):\n    plt.subplot(len(indices) // 2, 2, i + 1)\n    plt.title(f\"Digit {digit}\")\n    plt.axis('off')\n    img = mean_pixel_values[digit].reshape(28,28)\n    plt.imshow(img)\nplt.plot()\n\n[]"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "RegressionBlog/regression.html",
    "href": "RegressionBlog/regression.html",
    "title": "Salary Prediction Using Regression on MLB Data",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.svm import SVR\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error"
  },
  {
    "objectID": "RegressionBlog/regression.html#data-analysis",
    "href": "RegressionBlog/regression.html#data-analysis",
    "title": "Salary Prediction Using Regression on MLB Data",
    "section": "Data Analysis",
    "text": "Data Analysis\nBefore plugging in the data into our SVM, it is important to evaluate redundant information to limit multicollinearity. We can find collinearities through the correlation matrix.\n\ndf = pd.read_csv(\"./Hitters.csv\")\nnum_cols = [col for col in df.columns if df[col].dtypes != \"O\"]\ncorr = df[num_cols].corr()\nsns.set(rc={'figure.figsize': (15, 15)})\nsns.heatmap(corr, cmap=\"RdBu\", annot=True)\nplt.show()\n\n\n\n\n\nRun PCA on highly-correlated features\n\npca_custat = PCA(n_components=1)\npca_indstat = PCA(n_components=1)\npca_fieldstat = PCA(n_components=1)\n\ncustat_df = df[[\"CAtBat\", \"CHits\", \"CHmRun\", \"CRuns\", \"CRBI\", \"CWalks\"]]\nindstat_df = df[[\"AtBat\", \"Hits\", \"HmRun\", \"Runs\", \"RBI\", \"Walks\"]]\nfieldstat_df = df[[\"Assists\", \"Errors\"]]\n\ncustat_df_pca = pca_custat.fit_transform(custat_df)\nindstat_df_pca = pca_indstat.fit_transform(indstat_df)\nfieldstat_df_pca = pca_fieldstat.fit_transform(fieldstat_df)\n\ndf_reduced = df.drop(columns=[\"AtBat\", \"Hits\", \"HmRun\", \"Runs\", \"RBI\", \"Walks\", \"CAtBat\", \"CHits\", \"CHmRun\", \"CRuns\", \"CRBI\", \"CWalks\", \"Assists\", \"Errors\"])\ndf_reduced = df_reduced.drop(columns=[\"League\", \"NewLeague\", \"Division\", \"Years\"])\ndf_reduced[\"I_PC1\"] = indstat_df_pca\ndf_reduced[\"C_PC2\"] = custat_df_pca\ndf_reduced[\"F_PC3\"] = fieldstat_df_pca\n\n# label_mapping_an = {'A': 0, 'N': 1}\n# label_mapping_div = {'E': 0, 'W': 1}\n\n\n# # Apply label encoding to the 'Category' column\n# df_reduced['League'] = df_reduced['League'].map(label_mapping_an)\n# df_reduced['NewLeague'] = df_reduced['NewLeague'].map(label_mapping_an)\n# df_reduced['Division'] = df_reduced['Division'].map(label_mapping_div)\ndf_reduced.dropna(axis=0, inplace=True)\n\nprint(df_reduced)\n\n     PutOuts  Salary       I_PC1        C_PC2       F_PC3\n1        632   475.0   72.590869   801.168761  -63.814023\n2        880   480.0 -108.738125 -1059.201154  -24.703570\n3        200   500.0 -124.894681  3171.420761  -96.026925\n4        805    91.5   63.304301 -2384.553500  -67.009725\n5        282   750.0 -221.371087  1777.108228  314.474814\n..       ...     ...         ...          ...         ...\n317      325   700.0 -117.568310    61.626177  -98.025844\n318      313   875.0 -122.303569  3049.912291  274.332014\n319       37   385.0  -96.674458 -1030.222990    6.049449\n320     1314   960.0 -201.645999   587.940371   24.204149\n321      408  1000.0 -255.582377  2378.079204 -103.023139\n\n[263 rows x 5 columns]\n\n\n\nnum_cols = [col for col in df_reduced.columns if df_reduced[col].dtypes != \"O\"]\ncorr = df_reduced[num_cols].corr()\nsns.set(rc={'figure.figsize': (15, 15)})\nsns.heatmap(corr, cmap=\"RdBu\", annot=True)\nplt.show()\n\n\n\n\nwe’ve reduced the data from 20 columns to 6 columns, which is important, as collinearity can negatively effect the preformance of regression models. Apart from the collinearity effect, I decided to get rid of discretely labeled binary relationships (labels 0 or 1), as this makes the linear regression model more complex and doesn’t effect the model preformance that much.\n\n\nCreate Training and Test Sets\nSince we’re trying to predict salary, I extract “salary” column from the dataframe, storing it into the label, and dropping that column from the feature data.\n\ndf = df_reduced\ny = df[\"Salary\"]\nX = df.drop(columns=\"Salary\")\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n\nprint(f\"Salary STD: ${np.std(y) * 1000:,.2f}\")\nprint(f\"Salary Mean: ${np.mean(y) * 1000:,.2f}\")\nprint(f\"Salary Low: ${np.min(y) * 1000:,.2f}\")\nprint(f\"Salary High: ${np.max(y) * 1000:,.2f}\")\n\nSalary STD: $450,260.22\nSalary Mean: $535,925.88\nSalary Low: $67,500.00\nSalary High: $2,460,000.00"
  },
  {
    "objectID": "RegressionBlog/regression.html#linear-support-vector-regression-kernellinear",
    "href": "RegressionBlog/regression.html#linear-support-vector-regression-kernellinear",
    "title": "Salary Prediction Using Regression on MLB Data",
    "section": "Linear Support Vector Regression (kernel=“linear”)",
    "text": "Linear Support Vector Regression (kernel=“linear”)\n\nsc = StandardScaler()\n\nX_train_scaled = sc.fit_transform(X_train, y_train)\nX_test_scaled = sc.fit_transform(X_test, y_test)\n# pca_all = PCA(n_components=1)\n# X_train_scaled_pca = pca_all.fit_transform(X_train_scaled)\n# X_test_scaled_pca = pca_all.fit_transform(X_test_scaled)\n\nsvr_lin = SVR(kernel=\"linear\", C=1, gamma=\"auto\")\nsvr_lin.fit(X_train_scaled, y_train)\ny_pred = svr_lin.predict(X_test_scaled)\n\n\nVisualize Multiple Linear Regression\n\nplt.figure(figsize=(16, 12))\nindependent_variables = X_train.columns\ndependent_variable = \"Salary\"\nX_test_numpy = X_test.to_numpy()\nfor i, col in enumerate(independent_variables, 1):\n    plt.subplot(3, 3, i)\n    sns.regplot(x=X_train[col],y=y_train,ci=None,color ='red')\n    sns.scatterplot(data=X_train, x=col, y=y_train, color='blue', label='Training Points')\n    sns.scatterplot(data=X_test, x=col, y=y_test, color='green', label='Testing Points')\n    sns.scatterplot(data=X_test, x=col, y=y_pred, color='red', label='Predicted Points')\n    plt.title(f'{dependent_variable} vs. {col}')\n    plt.xlabel(col)\n    plt.ylabel(dependent_variable)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nresult_df = pd.DataFrame(columns=[\"id\", \"actual\", \"predicted\"])\n\nfor i, actual, predicted in zip(y_test.index, y_test, y_pred):\n    entry = [i, actual, predicted]\n    df_entry = pd.DataFrame(entry, index=[\"id\", \"actual\", \"predicted\"]).T\n    result_df = pd.concat((result_df, df_entry))\n#print(result_df)\ndifference = abs(result_df[\"actual\"] - result_df[\"predicted\"])\nprint(f\"Cumulative Difference: ${np.sum(difference) * 1000:,.2f}\")\nprint(f\"Min Difference: ${np.min(difference) * 1000:,.2f}\")\nprint(f\"Max Difference: ${np.max(difference) * 1000:,.2f}\")\nprint(f\"Average Difference: ${np.mean(difference) * 1000:,.2f}\")\nprint(f\"Std Difference: ${np.std(difference) * 1000:,.2f}\")\nprint(f\"Mean Squared Error: ${mean_squared_error(y_test, y_pred):,.2f}\")\n\nCumulative Difference: $10,792,084.13\nMin Difference: $19,061.95\nMax Difference: $1,968,301.30\nAverage Difference: $269,802.10\nStd Difference: $361,894.28\nMean Squared Error: $203,760.65\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nplt.figure(figsize=(8,8))\nresiduals = result_df['actual'] - result_df['predicted']\n\nplt.scatter(result_df['id'], residuals)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('ID')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.show()"
  },
  {
    "objectID": "RegressionBlog/regression.html#non-linear-support-vector-regression-kernelpoly",
    "href": "RegressionBlog/regression.html#non-linear-support-vector-regression-kernelpoly",
    "title": "Salary Prediction Using Regression on MLB Data",
    "section": "Non-Linear Support Vector Regression (kernel=“poly”)",
    "text": "Non-Linear Support Vector Regression (kernel=“poly”)\n\nsvr_poly = SVR(kernel=\"poly\", degree=2, C=75, gamma=\"scale\")\nsvr_poly.fit(X_train_scaled, y_train)\ny_pred = svr_poly.predict(X_test_scaled)\n\nresult_df = pd.DataFrame(columns=[\"id\", \"actual\", \"predicted\"])\n\nfor i, actual, predicted in zip(y_test.index, y_test, y_pred):\n    entry = [i, actual, predicted]\n    df_entry = pd.DataFrame(entry, index=[\"id\", \"actual\", \"predicted\"]).T\n    result_df = pd.concat((result_df, df_entry))\n    \n#print(result_df)\ndifference = abs(result_df[\"actual\"] - result_df[\"predicted\"])\nprint(f\"Cumulative Difference: ${np.sum(difference) * 1000:,.2f}\")\nprint(f\"Min Difference: ${np.min(difference) * 1000:,.2f}\")\nprint(f\"Max Difference: ${np.max(difference) * 1000:,.2f}\")\nprint(f\"Average Difference: ${np.mean(difference) * 1000:,.2f}\")\nprint(f\"Std Difference: ${np.std(difference) * 1000:,.2f}\")\nprint(f\"Mean Squared Error: ${mean_squared_error(y_test, y_pred):,.2f}\")\n\nCumulative Difference: $13,385,640.30\nMin Difference: $798.03\nMax Difference: $1,581,505.51\nAverage Difference: $334,641.01\nStd Difference: $330,986.90\nMean Squared Error: $221,536.94\n\n\n\nplt.figure(figsize=(16, 12))\nindependent_variables = X_train.columns\ndependent_variable = \"Salary\"\nX_test_numpy = X_test.to_numpy()\nfor i, col in enumerate(independent_variables, 1):\n    plt.subplot(3, 3, i)\n    sns.regplot(x=X_train[col],y=y_train,ci=None,color ='red', order=svr_poly.degree)\n    sns.scatterplot(data=X_train, x=col, y=y_train, color='blue', label='Training Points')\n    sns.scatterplot(data=X_test, x=col, y=y_test, color='green', label='Testing Points')\n    sns.scatterplot(data=X_test, x=col, y=y_pred, color='red', label='Predicted Points')\n    plt.title(f'{dependent_variable} vs. {col}')\n    plt.xlabel(col)\n    plt.ylabel(dependent_variable)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nplt.figure(figsize=(8,8))\nresiduals = result_df['actual'] - result_df['predicted']\n\nplt.scatter(result_df['id'], residuals)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('ID')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.show()"
  },
  {
    "objectID": "posts/RegressionBlog/index.html",
    "href": "posts/RegressionBlog/index.html",
    "title": "Comparing Multiple Linear and Non-Linear Regression on MLB Data",
    "section": "",
    "text": "Author: Daniel Hassler\n```{python}\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.svm import SVR\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n```"
  },
  {
    "objectID": "posts/RegressionBlog/index.html#data",
    "href": "posts/RegressionBlog/index.html#data",
    "title": "Comparing Multiple Linear and Non-Linear Regression on MLB Data",
    "section": "Data",
    "text": "Data\nWith Kaggle, I was able to find an MLB (Major League Baseball) dataset consisting of player data from the 1986-1987 seasons (https://www.kaggle.com/datasets/mathchi/hitters-baseball-data). The data itself has many features consisting of individual stats for the season, cumulative career stats, fielding stats, and salary. In total, there are 20 features with 322 entries before preprocessing.\nThe goal of this notebook is to showcase linear and non-linear regression as a way of predicting salary (in thousands), a continuous variable, for my dataset. But before I run through the regression process, I have to clean the data first and figure out correlations.\nOne negative influence on regression models is collinearity in the feature space. Collinearity is problematic for several reasons, including overfitting, interpretability, and inefficiency. When there are many features that are highly correlated, this will create a strong negative impact on the performance. So before plugging into the model, I analyzed the correlation matrix consisting of correlations between all the features.\n\n```{python}\ndf = pd.read_csv(\"./Hitters.csv\")\nnum_cols = [col for col in df.columns if df[col].dtypes != \"O\"]\ncorr = df[num_cols].corr()\nsns.set(rc={'figure.figsize': (15, 15)})\nsns.heatmap(corr, cmap=\"RdBu\", annot=True)\nplt.show()\n```\n\n\n\n\nBased on the above correlation matrix, I can see there’s a high correlation between all the individual season player stats (AtBat, Hits, HmRun, Runs, RBI, and Walks), cumulative player stats (CAtBat, CHits, CHmRun, CRuns, CRBI, Years, and CWalks), and finally some fielding stats (Assists, Errors).\nOne way to remove these strong correlations is to run a dimensionality reduction technique. In this case, I will be using PCA (principal component analysis) seperately on those three highly correlated areas: individual stats, cumulative stats, and fielding.\n\n```{python}\npca_custat = PCA(n_components=1)\npca_indstat = PCA(n_components=1)\npca_fieldstat = PCA(n_components=1)\n\ncustat_df = df[[\"CAtBat\", \"CHits\", \"CHmRun\", \"CRuns\", \"CRBI\", \"CWalks\"]]\nindstat_df = df[[\"AtBat\", \"Hits\", \"HmRun\", \"Runs\", \"RBI\", \"Walks\"]]\nfieldstat_df = df[[\"Assists\", \"Errors\"]]\n\ncustat_df_pca = pca_custat.fit_transform(custat_df)\nindstat_df_pca = pca_indstat.fit_transform(indstat_df)\nfieldstat_df_pca = pca_fieldstat.fit_transform(fieldstat_df)\n\ndf_reduced = df.drop(columns=[\"AtBat\", \"Hits\", \"HmRun\", \"Runs\", \"RBI\", \"Walks\", \"CAtBat\", \"CHits\", \"CHmRun\", \"CRuns\", \"CRBI\", \"CWalks\", \"Assists\", \"Errors\"])\ndf_reduced = df_reduced.drop(columns=[\"League\", \"NewLeague\", \"Division\", \"Years\"])\ndf_reduced[\"I_PC1\"] = indstat_df_pca\ndf_reduced[\"C_PC2\"] = custat_df_pca\ndf_reduced[\"F_PC3\"] = fieldstat_df_pca\n\ndf_reduced.dropna(axis=0, inplace=True)\n```\n\n\n```{python}\nnum_cols = [col for col in df_reduced.columns if df_reduced[col].dtypes != \"O\"]\ncorr = df_reduced[num_cols].corr()\nsns.set(rc={'figure.figsize': (15, 15)})\nsns.heatmap(corr, cmap=\"RdBu\", annot=True)\nplt.show()\n```\n\n\n\n\nAfter applying PCA and removing other features, I’ve reduced the data from 20 columns to just 5 columns, which is important, as collinearity can negatively effect the performance of regression models. Apart from the collinearity effect, I decided to get rid of discretely labeled binary relationships (labels 0 or 1), as this makes the linear regression model more complex and can only negatively impact the performance. I also discarded years as a feature because it was highly correlated with the PCA of the cumulative stats (r=0.91).\n\nCreating the Training and Test Sets\nSince we’re trying to predict salary, I extract salary column from the df_reduced, storing it into labels array y, and dropping that column from the feature data. The X data feature space has dropped from 5 columns to 4 columns, so there are 4 total features. For splitting the data into test and train, I am using sklearn’s train_test_split().\n\n```{python}\ndf = df_reduced\ny = df[\"Salary\"]\nX = df.drop(columns=\"Salary\")\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n\nprint(f\"Salary STD: ${np.std(y) * 1000:,.2f}\")\nprint(f\"Salary Mean: ${np.mean(y) * 1000:,.2f}\")\nprint(f\"Salary Low: ${np.min(y) * 1000:,.2f}\")\nprint(f\"Salary High: ${np.max(y) * 1000:,.2f}\")\n```\n\nSalary STD: $450,260.22\nSalary Mean: $535,925.88\nSalary Low: $67,500.00\nSalary High: $2,460,000.00"
  },
  {
    "objectID": "posts/RegressionBlog/index.html#multiple-linear-support-vector-regression-kernellinear",
    "href": "posts/RegressionBlog/index.html#multiple-linear-support-vector-regression-kernellinear",
    "title": "Comparing Multiple Linear and Non-Linear Regression on MLB Data",
    "section": "Multiple Linear Support Vector Regression (kernel=“linear”)",
    "text": "Multiple Linear Support Vector Regression (kernel=“linear”)\nNow, I will test out multiple linear regression using Sklearn’s SVR (support vector regression) class from the SVM library. Before passing the data into the regression model, I scaled the data using StandardScaler() as this is important for faster computation with regression. For the hyperparameters, I toyed with different C values, which influences the degree of regularization applied to the SVR model. A smaller C value leads to a simpler model, but a larger C value would fit to the training data more closely, which can potentially overfit if the number is too high. For the C value on the linear kernel, my model showed good preformance at C=1.\n\n```{python}\nsc = StandardScaler()\n\nX_train_scaled = sc.fit_transform(X_train, y_train)\nX_test_scaled = sc.fit_transform(X_test, y_test)\n# pca_all = PCA(n_components=1)\n# X_train_scaled_pca = pca_all.fit_transform(X_train_scaled)\n# X_test_scaled_pca = pca_all.fit_transform(X_test_scaled)\n\nsvr_lin = SVR(kernel=\"linear\", C=1, gamma=\"auto\")\nsvr_lin.fit(X_train_scaled, y_train)\ny_pred = svr_lin.predict(X_test_scaled)\n```\n\n\nMultiple Linear Regression Visualization\n\n```{python}\nplt.figure(figsize=(16, 12))\nindependent_variables = X_train.columns\ndependent_variable = \"Salary\"\nX_test_numpy = X_test.to_numpy()\nfor i, col in enumerate(independent_variables, 1):\n    plt.subplot(3, 3, i)\n    sns.regplot(x=X_train[col],y=y_train,ci=None,color ='red')\n    sns.scatterplot(data=X_train, x=col, y=y_train, color='blue', label='Training Points')\n    sns.scatterplot(data=X_test, x=col, y=y_test, color='green', label='Testing Points')\n    sns.scatterplot(data=X_test, x=col, y=y_pred, color='red', label='Predicted Points')\n    plt.title(f'{dependent_variable} vs. {col}')\n    plt.xlabel(col)\n    plt.ylabel(dependent_variable)\n\nplt.tight_layout()\nplt.show()\n```\n\n\n\n\nHere is a visualization showing linear regression applied on all of the features in the feature space, which make up the overall prediction. For each feature, the red line represents the function applied, which in this case is linear because I’m using a linear kernel, and the points represent all the different datapoints in the dataset. X_train points are in blue, X_test points are in green with their actual label y_test, and predicted points are in red (the X_test dataset on the y_pred).\nBased on the above visualization, it appears that linear regression works very well for all the features, although in any case, outliers are a problem. It is also worth noting that changing the C value does change the results, so modifying that may improve preformance with more fine-tuning. Below, I have outputed some metrics on the model:\n\n```{python}\nresult_df = pd.DataFrame(columns=[\"id\", \"actual\", \"predicted\"])\n\nfor i, actual, predicted in zip(y_test.index, y_test, y_pred):\n    entry = [i, actual, predicted]\n    df_entry = pd.DataFrame(entry, index=[\"id\", \"actual\", \"predicted\"]).T\n    result_df = pd.concat((result_df, df_entry))\n#print(result_df)\ndifference = abs(result_df[\"actual\"] - result_df[\"predicted\"])\nprint(f\"Cumulative Difference: ${np.sum(difference) * 1000:,.2f}\")\nprint(f\"Min Difference: ${np.min(difference) * 1000:,.2f}\")\nprint(f\"Max Difference: ${np.max(difference) * 1000:,.2f}\")\nprint(f\"Average Difference: ${np.mean(difference) * 1000:,.2f}\")\nprint(f\"Std Difference: ${np.std(difference) * 1000:,.2f}\")\nprint(f\"Mean Squared Error: ${mean_squared_error(y_test, y_pred):,.2f}\")\n```\n\nCumulative Difference: $10,792,084.13\nMin Difference: $19,061.95\nMax Difference: $1,968,301.30\nAverage Difference: $269,802.10\nStd Difference: $361,894.28\nMean Squared Error: $203,760.65\n\n\nBased on this data, the total difference (sum of all the differences) between the actual and predicted outputs could be better, as the average difference between the labels is around 270 = $270,000. The MSE is a common metric used in these types of problems, and my MSE(mean squared error) score is around 205 ($205,000), which is a respectable MSE value for this dataset due to its high standard deviation at ($450,260.22).\n\n```{python}\nplt.figure(figsize=(8,8))\nresiduals = result_df['actual'] - result_df['predicted']\n\nplt.scatter(result_df['id'], residuals)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('ID')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.show()\n```\n\n\n\n\nAbove is a visualization showing how far the differences are for each value in the test set (the residuals). 0 means no difference between the actual and the predicted, any number below 0 means the predicted value was higher than the actual value, and any number above 0 means the predicted value was lower than the actual value."
  },
  {
    "objectID": "posts/RegressionBlog/index.html#multiple-non-linear-support-vector-regression-kernelpoly",
    "href": "posts/RegressionBlog/index.html#multiple-non-linear-support-vector-regression-kernelpoly",
    "title": "Comparing Multiple Linear and Non-Linear Regression on MLB Data",
    "section": "Multiple Non-Linear Support Vector Regression (kernel=“poly”)",
    "text": "Multiple Non-Linear Support Vector Regression (kernel=“poly”)\nNow I run SVR on a non-linear kernel and assess its comparison to a linear kernel. Since real world data has a lot of non-linearity, this comparison is worth attempting.\n\n```{python}\nsvr_poly = SVR(kernel=\"poly\", degree=2, C=75, gamma=\"scale\")\nsvr_poly.fit(X_train_scaled, y_train)\ny_pred = svr_poly.predict(X_test_scaled)\n\nresult_df = pd.DataFrame(columns=[\"id\", \"actual\", \"predicted\"])\n\nfor i, actual, predicted in zip(y_test.index, y_test, y_pred):\n    entry = [i, actual, predicted]\n    df_entry = pd.DataFrame(entry, index=[\"id\", \"actual\", \"predicted\"]).T\n    result_df = pd.concat((result_df, df_entry))\n    \n#print(result_df)\ndifference = abs(result_df[\"actual\"] - result_df[\"predicted\"])\nprint(f\"Cumulative Difference: ${np.sum(difference) * 1000:,.2f}\")\nprint(f\"Min Difference: ${np.min(difference) * 1000:,.2f}\")\nprint(f\"Max Difference: ${np.max(difference) * 1000:,.2f}\")\nprint(f\"Average Difference: ${np.mean(difference) * 1000:,.2f}\")\nprint(f\"Std Difference: ${np.std(difference) * 1000:,.2f}\")\nprint(f\"Mean Squared Error: ${mean_squared_error(y_test, y_pred):,.2f}\")\n```\n\nCumulative Difference: $13,385,640.30\nMin Difference: $798.03\nMax Difference: $1,581,505.51\nAverage Difference: $334,641.01\nStd Difference: $330,986.90\nMean Squared Error: $221,536.94\n\n\nBased on the model run through with a polynomial kernel, the results are overall noticeably worse than the linear kernel, but not by a whole lot. Although, it is worth noting that the C value is crucial in this result. I was trying to balance the trade-off between conforming to the function and simplicity. For the polynomial degree, I decide to go with 2, as 1 is linear and 3 didn’t preform as anticipated, as the function plotted didn’t represent some of the features as well as 2.\n\nMultiple Non-Linear Regression Visualization\n\n```{python}\nplt.figure(figsize=(16, 12))\nindependent_variables = X_train.columns\ndependent_variable = \"Salary\"\nX_test_numpy = X_test.to_numpy()\nfor i, col in enumerate(independent_variables, 1):\n    plt.subplot(3, 3, i)\n    sns.regplot(x=X_train[col],y=y_train,ci=None,color ='red', order=svr_poly.degree)\n    sns.scatterplot(data=X_train, x=col, y=y_train, color='blue', label='Training Points')\n    sns.scatterplot(data=X_test, x=col, y=y_test, color='green', label='Testing Points')\n    sns.scatterplot(data=X_test, x=col, y=y_pred, color='red', label='Predicted Points')\n    plt.title(f'{dependent_variable} vs. {col}')\n    plt.xlabel(col)\n    plt.ylabel(dependent_variable)\n\nplt.tight_layout()\nplt.show()\n```\n\n\n\n\nBased on the above visualzation, using a polynomial function with degree 2, shows interesting results. For the Salary vs PutOuts plot, the data plotted resembles a linear kernel, but in actuality it’s a very zoomed in polynomial kernel. The Salary vs I_PC1 showed a curve which I expected. It starts off at a peak and then lowers like a parabolic function (degree 2). The Salary vs C_PC2 plot is interesting in the sense that it’s a negative parabola; I would say this is not a fully representative curve as it seems to be fitting to the outlier at the end of the plot. Finally, the Salary vs F_PC3 plot seems to be similar to the first plot as it resembles more of a linear kernel.\n\n```{python}\nplt.figure(figsize=(8,8))\nresiduals = result_df['actual'] - result_df['predicted']\n\nplt.scatter(result_df['id'], residuals)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('ID')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.show()\n```\n\n\n\n\nAbove is the residual plot for this model with the same setup as the linear kernel one."
  },
  {
    "objectID": "posts/RegressionBlog/index.html#discussion-and-improvements",
    "href": "posts/RegressionBlog/index.html#discussion-and-improvements",
    "title": "Comparing Multiple Linear and Non-Linear Regression on MLB Data",
    "section": "Discussion and Improvements",
    "text": "Discussion and Improvements\nIn actuality, it appears to me that the poly non-linear kernel represents the data better for certain features even though it doesn’t perform as well against the linear kernel. I believe the tradeoff to this approach highly depends on the C value for both approaches and requires more hyperparameter awareness and optimization.\nIn terms of features, I believe all of these features have strong presuasion in determing an MLB player’s salary, but there is one flaw. Some players, regardless of preformance, are more “famous” than other players. It is likely that more famous players have higher salaries simply because they generate more revenue for the teams they play for, but that doesn’t necessarily mean the player’s popularity is correlated with skill. This is the reason why I believe there are outliers in this dataset. If there were a way to accurately determine popularity, that would be a key feature in predicting salary as well for this particular domain. Speaking of outliers, I would also like to point out that outliers can have a significant impact on these regression models. Depending on the case, deleting outliers may be a valid option, but that should come with caution as deleting data can result in loss of valuable information."
  },
  {
    "objectID": "posts/NaiveBayesBlog/index.html",
    "href": "posts/NaiveBayesBlog/index.html",
    "title": "Probability Theory with Naive Bayes Application",
    "section": "",
    "text": "Author: Daniel Hassler\n```{python}\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport mnist\n\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn.metrics import confusion_matrix\n```"
  },
  {
    "objectID": "posts/NaiveBayesBlog/index.html#data",
    "href": "posts/NaiveBayesBlog/index.html#data",
    "title": "Probability Theory with Naive Bayes Application",
    "section": "Data",
    "text": "Data\nBefore diving into the probability theory and using Naive Bayes, I will first introduce the dataset I am using for this application.\nThe dataset I am using to explain probability theory with Naive Bayes is the MNIST dataset, a large dataset containing pictures/drawings of digits 0-9. There are 60,000 training images and 10,000 testing images in my particular dataset.\nBelow, is a visualization of one entry in the training set and one entry in the test set to show what these digits look like.\n\n```{python}\n# mnist.init()\nX_train, y_train, X_test, y_test = mnist.load()\nprint(\"X_train len: \", len(X_train))\nprint(\"X_test len: \", len(X_test))\nprint(\"X_train shape: \", X_train.shape)\nprint(\"X_test shape: \", X_test.shape)\n\nplt.subplot(1, 2, 1)\nplt.title(\"Entry in Train Set\")\nimg = X_train[0,:].reshape(28,28) # First image in the training set.\nplt.imshow(img,cmap='gray')\n\nplt.subplot(1, 2, 2)\nplt.title(\"Entry in Test Set\")\nimg = X_test[0,:].reshape(28,28) # First image in the test set.\nplt.imshow(img,cmap='gray')\nplt.show() # Show the image\n```\n\nX_train len:  60000\nX_test len:  10000\nX_train shape:  (60000, 784)\nX_test shape:  (10000, 784)\n\n\n\n\n\nHere is a visualization showing a unique entry in the X_train data for each digit.\n\n```{python}\nunique_values, indices = np.unique(y_train, return_index=True)\n\nfor i, label_index in enumerate(indices):\n    plt.subplot(1, len(indices), i + 1)\n    img = X_train[label_index,:].reshape(28,28)\n    plt.imshow(img,cmap='gray')\nplt.show()\n```"
  },
  {
    "objectID": "posts/NaiveBayesBlog/index.html#naive-bayes-background",
    "href": "posts/NaiveBayesBlog/index.html#naive-bayes-background",
    "title": "Probability Theory with Naive Bayes Application",
    "section": "Naive Bayes Background",
    "text": "Naive Bayes Background\nA fundamental concept in probability theory is Bayes’ Theorem. The theorem is used to update the belief of an event occuring given new evidence: \\[\nP(A|B) =\n  \\frac{ P(B|A)P(A)}{\n       P(B)}\n\\]\nIn the theorem above, P(A|B) is represented as the probability of event A occurring given event B occurred. P(B|A) is the probability of B occurring given that event A occured. P(A) and P(B) are independent events.\nNaive Bayes is a machine learning algorithm that relies on the concept of Bayes Theorem and idea of conditional independence. In an application using Naive Bayes, we are assuming that the features are independent of each other (naive assumption), which is rarely ever true in real world scenarios, but it is a valid benchmark and has some benefits. This is the overall idea of the application of Bayes’ Theorem with Naive Bayes: \\[\nP(C|X) =\n  \\frac{ P(X|C)P(C)}{\n       P(X)}\n\\]\nThe goal is to find the probability of class C given observation X. In our case with the MNIST dataset, X is the feature set that represents every pixel in the 28x28 images (784 total features) and C is a representation of all the classes, digits 0-9. The naive assumption with the MNIST dataset is treating the pixels as independent observations.\nFirst, we can get the prior probabilities, represented as P(C) from the training set itself. This is the probability occurrence of each class in the dataset. I calculated this by this equation, where N_c is the number of occurrences of class c and N is the sum of all classes occurrences. \\[\nP(C=c) = \\frac{N_c}{N}\n\\]\nWe can then get the likelihood probability P(X|C), the probability of the feature (pixel) given class C. We can get this directly from the training data itself. We can get this by observing the data or by calculating the probability density function if we’re assuming the data flows like a Gaussian distribution.\nP(C|X), the posterior probability, represents the probability of class C given feature X. Based on this, in our prediction stage with Naive Bayes, we are taking the class with the max probability to get the classification."
  },
  {
    "objectID": "posts/NaiveBayesBlog/index.html#naive-bayes-classification",
    "href": "posts/NaiveBayesBlog/index.html#naive-bayes-classification",
    "title": "Probability Theory with Naive Bayes Application",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\nFor the purposes of explaining probability theory with NB, visualizing data, speed consideration, and understanding of naive bayes on the MNIST dataset, I decided to go with sklearn’s GaussianNB model, which is a commonly used baseline model for a lot of distributions that follow a Gaussian distribution. By chosing this model, I am assuming that my MNIST data follows a Gaussian distribution, but the data itself doesn’t directly follow this assumption. So, compared to other methods like CNNs (see improvements), we will see a performance degredation with this model, as MNIST pixels are not normally distributed. Although, the performance of this model we will see is “reasonable” and better than expected, especially since we can still assume conditional independence between the features, which is a large assumption for Naive Bayes.\nFor the hyperparameter values, I gathered the dataset’s prior distribution by simply taking the frequency of the dataset and dividing it by the sum of all the frequencies, and then passed it into the GaussianNB model. Another parameter I had to tune is the var_smoothing value. This value is used to prevent any division by zero during probability estimation. By default, sklearn sets this value to 1e-09, but the effectiveness of this value depends on the dataset, so in the end, I found 0.1 to have the best accuracy performance.\n\n```{python}\nunique, counts = np.unique(y_train, return_counts=True)\nsum_counts = np.sum(counts)\npriors = np.divide(counts, sum_counts)\n\nnb = GaussianNB(priors=priors, var_smoothing=0.1)\nnb.fit(X_train, y_train)\ny_pred = nb.predict(X_test)\nprint(\"Priors: \", nb.priors)\n```\n\nPriors:  [0.09871667 0.11236667 0.0993     0.10218333 0.09736667 0.09035\n 0.09863333 0.10441667 0.09751667 0.09915   ]"
  },
  {
    "objectID": "posts/NaiveBayesBlog/index.html#results",
    "href": "posts/NaiveBayesBlog/index.html#results",
    "title": "Probability Theory with Naive Bayes Application",
    "section": "Results",
    "text": "Results\nIn order to evaluate the preformance of the GaussianNB classifier, I ran a confusion matrix to visualize where the predictions fall. As you can see, we have around 81% accuracy on our given classifier, which is about what we expected for this dataset.\n\n```{python}\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nconf_df = pd.DataFrame(confusion_matrix, index=[f\"{i}\" for i in range(10)], columns=[f\"{i}\" for i in range(10)])\nheatmap = sns.heatmap(conf_df, annot=True, fmt=\"d\", linewidths=0.35, cmap=\"YlGnBu\")\nplt.title(f\"Model Predictions With {(np.sum(confusion_matrix.diagonal()) / y_test.shape[0]) * 100:.2f}% Accuracy\")\n```\n\nText(0.5, 1.0, 'Model Predictions With 81.40% Accuracy')\n\n\n\n\n\nNext, I used the GaussianNB theta value to extract the mean_pixel_values, which stores the estimated mean of each pixel for every class. This visualization plots a heatmap of each pixel, where the pixels that are highlighted in yellow have the highest mean for that digit and the dark blue being the lowest.\n\n```{python}\nmean_pixel_values = nb.theta_\nplt.figure(figsize=(5,10))\nfor i, digit in enumerate(range(10)):\n    plt.subplot(len(indices) // 2, 2, i + 1)\n    plt.title(f\"Digit {digit}\")\n    plt.axis('off')\n    img = mean_pixel_values[digit].reshape(28,28)\n    plt.imshow(img)\nplt.plot()\n```\n\n[]"
  },
  {
    "objectID": "posts/NaiveBayesBlog/index.html#improvements",
    "href": "posts/NaiveBayesBlog/index.html#improvements",
    "title": "Probability Theory with Naive Bayes Application",
    "section": "Improvements",
    "text": "Improvements\nThough we achieve decent accuracy with the MNIST dataset using GaussianNB classifier, this model has one big flaw for image classification; it only looks at the discretized values for specific points in the image. What would happen if I shifted the “0” or “9” to a different section of the image (not centered)? it would not be able to classify this case effectively.\nA way to fix the above limitation is using convolutional neural networks (CNNs), which is a deep learning classifier used in a lot of computer vision and even NLP related applications. Its main feature is using the idea of a “sliding window” to find more meaningful representations, which means the location of the object we’re classifying is less important."
  },
  {
    "objectID": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html",
    "href": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "",
    "text": "import numpy as np\nimport sklearn\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, LearningCurveDisplay\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn import tree\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#imports",
    "href": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#imports",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "",
    "text": "import numpy as np\nimport sklearn\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, LearningCurveDisplay\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn import tree\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#class-distribution",
    "href": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#class-distribution",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Class Distribution",
    "text": "Class Distribution\n\n# Class imbalance, more obesity.\nunique_values, counts = np.unique(y, return_counts=True)\nplt.bar(unique_values, counts)\nplt.title(\"BMI Classes in the Entire Dataset\")\nplt.xlabel(\"BMI Class\")\nplt.ylabel(\"Occurences in Entire Dataset\")\nplt.show()"
  },
  {
    "objectID": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#feature-correlation",
    "href": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#feature-correlation",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Feature Correlation",
    "text": "Feature Correlation\n\n\ncorrelation_matrix = data.corr()\n\n# Display a heatmap of the correlation matrix\nplt.figure(figsize=(10, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('Correlation Heatmap')\nplt.show()\n\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_33884\\1179218035.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  correlation_matrix = data.corr()"
  },
  {
    "objectID": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#scatter-plot-of-data",
    "href": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#scatter-plot-of-data",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Scatter Plot of Data",
    "text": "Scatter Plot of Data\n\nplt.figure(figsize=(6, 6))\nsns.scatterplot(data=data, x='Height', y='Weight', hue='Index', palette='deep')\nplt.title('Scatter Plot of Height vs Weight')\nplt.show()"
  },
  {
    "objectID": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#distribution-of-features",
    "href": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#distribution-of-features",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Distribution of Features",
    "text": "Distribution of Features\n\n# Compare height and weight between male and female genders using box plots\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=data, x='Gender', y='Height')\nplt.title('Comparison of Height between Genders')\nplt.xlabel('Gender')\nplt.ylabel('Height')\nplt.show()\n\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=data, x='Gender', y='Weight')\nplt.title('Comparison of Weight between Genders')\nplt.xlabel('Gender')\nplt.ylabel('Weight')\nplt.show()"
  },
  {
    "objectID": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#hyperparameter-tuning-dt",
    "href": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#hyperparameter-tuning-dt",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Hyperparameter Tuning DT",
    "text": "Hyperparameter Tuning DT\n\nparam_grid = {\n    'max_depth': [i for i in range(2, 9)],\n    'min_samples_leaf': [2 ** i for i in range(0, 4)],\n    'criterion': [\"entropy\", \"gini\"]\n}\n\ndt = DecisionTreeClassifier(random_state=42)\n\ngrid_search_dt = GridSearchCV(dt, param_grid, cv=StratifiedKFold(n_splits=3), scoring='accuracy')\ngrid_search_dt.fit(X_train, y_train)\nbest_params_dt = grid_search_dt.best_params_\nprint(\"Best Hyperparameters:\", best_params_dt)\nprint(\"Best Score:\", grid_search_dt.best_score_)\n\nBest Hyperparameters: {'criterion': 'entropy', 'max_depth': 8, 'min_samples_leaf': 1}\nBest Score: 0.8281314289073061"
  },
  {
    "objectID": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#decision-tree-classifier",
    "href": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#decision-tree-classifier",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Decision Tree Classifier",
    "text": "Decision Tree Classifier\n\ndt = DecisionTreeClassifier(max_depth=best_params_dt[\"max_depth\"], min_samples_leaf=best_params_dt[\"min_samples_leaf\"], criterion=best_params_dt[\"criterion\"])\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)"
  },
  {
    "objectID": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#decision-tree-visualization",
    "href": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#decision-tree-visualization",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Decision Tree Visualization",
    "text": "Decision Tree Visualization\n\nplt.figure(figsize=(30, 20))\nplot_tree(dt, feature_names=X_train.columns.tolist(), class_names=['0', '1', '2','3','4','5'], filled=True)\n\n[Text(0.5918141592920354, 0.9444444444444444, 'Weight &lt;= 94.5\\nentropy = 2.189\\nsamples = 320\\nvalue = [11, 17, 39, 47, 76, 130]\\nclass = 5'),\n Text(0.3915929203539823, 0.8333333333333334, 'Weight &lt;= 69.5\\nentropy = 2.381\\nsamples = 126\\nvalue = [11, 17, 39, 31, 21, 7]\\nclass = 2'),\n Text(0.23893805309734514, 0.7222222222222222, 'Height &lt;= 167.5\\nentropy = 2.037\\nsamples = 56\\nvalue = [11, 17, 18, 9, 1, 0]\\nclass = 2'),\n Text(0.1415929203539823, 0.6111111111111112, 'Height &lt;= 152.5\\nentropy = 1.371\\nsamples = 25\\nvalue = [1, 0, 14, 9, 1, 0]\\nclass = 2'),\n Text(0.07079646017699115, 0.5, 'Weight &lt;= 57.5\\nentropy = 1.189\\nsamples = 12\\nvalue = [0, 0, 3, 8, 1, 0]\\nclass = 3'),\n Text(0.035398230088495575, 0.3888888888888889, 'Height &lt;= 144.0\\nentropy = 0.811\\nsamples = 4\\nvalue = [0, 0, 3, 1, 0, 0]\\nclass = 2'),\n Text(0.017699115044247787, 0.2777777777777778, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 1, 0, 0]\\nclass = 3'),\n Text(0.05309734513274336, 0.2777777777777778, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 0, 3, 0, 0, 0]\\nclass = 2'),\n Text(0.10619469026548672, 0.3888888888888889, 'Weight &lt;= 68.0\\nentropy = 0.544\\nsamples = 8\\nvalue = [0, 0, 0, 7, 1, 0]\\nclass = 3'),\n Text(0.08849557522123894, 0.2777777777777778, 'entropy = 0.0\\nsamples = 7\\nvalue = [0, 0, 0, 7, 0, 0]\\nclass = 3'),\n Text(0.12389380530973451, 0.2777777777777778, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0]\\nclass = 4'),\n Text(0.21238938053097345, 0.5, 'Weight &lt;= 66.0\\nentropy = 0.773\\nsamples = 13\\nvalue = [1, 0, 11, 1, 0, 0]\\nclass = 2'),\n Text(0.17699115044247787, 0.3888888888888889, 'Height &lt;= 162.5\\nentropy = 0.439\\nsamples = 11\\nvalue = [1, 0, 10, 0, 0, 0]\\nclass = 2'),\n Text(0.1592920353982301, 0.2777777777777778, 'entropy = 0.0\\nsamples = 7\\nvalue = [0, 0, 7, 0, 0, 0]\\nclass = 2'),\n Text(0.19469026548672566, 0.2777777777777778, 'Weight &lt;= 57.5\\nentropy = 0.811\\nsamples = 4\\nvalue = [1, 0, 3, 0, 0, 0]\\nclass = 2'),\n Text(0.17699115044247787, 0.16666666666666666, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0, 0, 0, 0, 0]\\nclass = 0'),\n Text(0.21238938053097345, 0.16666666666666666, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 0, 3, 0, 0, 0]\\nclass = 2'),\n Text(0.24778761061946902, 0.3888888888888889, 'Height &lt;= 163.5\\nentropy = 1.0\\nsamples = 2\\nvalue = [0, 0, 1, 1, 0, 0]\\nclass = 2'),\n Text(0.23008849557522124, 0.2777777777777778, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 1, 0, 0]\\nclass = 3'),\n Text(0.26548672566371684, 0.2777777777777778, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 1, 0, 0, 0]\\nclass = 2'),\n Text(0.336283185840708, 0.6111111111111112, 'Weight &lt;= 55.0\\nentropy = 1.383\\nsamples = 31\\nvalue = [10, 17, 4, 0, 0, 0]\\nclass = 1'),\n Text(0.3008849557522124, 0.5, 'Height &lt;= 180.0\\nentropy = 0.918\\nsamples = 15\\nvalue = [10, 5, 0, 0, 0, 0]\\nclass = 0'),\n Text(0.2831858407079646, 0.3888888888888889, 'entropy = 0.0\\nsamples = 5\\nvalue = [0, 5, 0, 0, 0, 0]\\nclass = 1'),\n Text(0.3185840707964602, 0.3888888888888889, 'entropy = 0.0\\nsamples = 10\\nvalue = [10, 0, 0, 0, 0, 0]\\nclass = 0'),\n Text(0.37168141592920356, 0.5, 'Height &lt;= 182.0\\nentropy = 0.811\\nsamples = 16\\nvalue = [0, 12, 4, 0, 0, 0]\\nclass = 1'),\n Text(0.35398230088495575, 0.3888888888888889, 'Weight &lt;= 59.5\\nentropy = 0.985\\nsamples = 7\\nvalue = [0, 3, 4, 0, 0, 0]\\nclass = 2'),\n Text(0.336283185840708, 0.2777777777777778, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 3, 0, 0, 0, 0]\\nclass = 1'),\n Text(0.37168141592920356, 0.2777777777777778, 'entropy = 0.0\\nsamples = 4\\nvalue = [0, 0, 4, 0, 0, 0]\\nclass = 2'),\n Text(0.3893805309734513, 0.3888888888888889, 'entropy = 0.0\\nsamples = 9\\nvalue = [0, 9, 0, 0, 0, 0]\\nclass = 1'),\n Text(0.5442477876106194, 0.7222222222222222, 'Height &lt;= 168.5\\nentropy = 1.894\\nsamples = 70\\nvalue = [0, 0, 21, 22, 20, 7]\\nclass = 3'),\n Text(0.4778761061946903, 0.6111111111111112, 'Height &lt;= 148.5\\nentropy = 1.408\\nsamples = 33\\nvalue = [0, 0, 0, 7, 19, 7]\\nclass = 4'),\n Text(0.4424778761061947, 0.5, 'Weight &lt;= 84.5\\nentropy = 0.996\\nsamples = 13\\nvalue = [0, 0, 0, 0, 6, 7]\\nclass = 5'),\n Text(0.4247787610619469, 0.3888888888888889, 'Height &lt;= 141.5\\nentropy = 0.592\\nsamples = 7\\nvalue = [0, 0, 0, 0, 6, 1]\\nclass = 4'),\n Text(0.40707964601769914, 0.2777777777777778, 'Height &lt;= 140.5\\nentropy = 1.0\\nsamples = 2\\nvalue = [0, 0, 0, 0, 1, 1]\\nclass = 4'),\n Text(0.3893805309734513, 0.16666666666666666, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0]\\nclass = 4'),\n Text(0.4247787610619469, 0.16666666666666666, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 1]\\nclass = 5'),\n Text(0.4424778761061947, 0.2777777777777778, 'entropy = 0.0\\nsamples = 5\\nvalue = [0, 0, 0, 0, 5, 0]\\nclass = 4'),\n Text(0.46017699115044247, 0.3888888888888889, 'entropy = 0.0\\nsamples = 6\\nvalue = [0, 0, 0, 0, 0, 6]\\nclass = 5'),\n Text(0.5132743362831859, 0.5, 'Weight &lt;= 80.5\\nentropy = 0.934\\nsamples = 20\\nvalue = [0, 0, 0, 7, 13, 0]\\nclass = 4'),\n Text(0.49557522123893805, 0.3888888888888889, 'Height &lt;= 159.0\\nentropy = 0.946\\nsamples = 11\\nvalue = [0, 0, 0, 7, 4, 0]\\nclass = 3'),\n Text(0.4778761061946903, 0.2777777777777778, 'Weight &lt;= 72.0\\nentropy = 0.722\\nsamples = 5\\nvalue = [0, 0, 0, 1, 4, 0]\\nclass = 4'),\n Text(0.46017699115044247, 0.16666666666666666, 'Gender_Encoded &lt;= 0.5\\nentropy = 1.0\\nsamples = 2\\nvalue = [0, 0, 0, 1, 1, 0]\\nclass = 3'),\n Text(0.4424778761061947, 0.05555555555555555, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 1, 0, 0]\\nclass = 3'),\n Text(0.4778761061946903, 0.05555555555555555, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0]\\nclass = 4'),\n Text(0.49557522123893805, 0.16666666666666666, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 0, 0, 0, 3, 0]\\nclass = 4'),\n Text(0.5132743362831859, 0.2777777777777778, 'entropy = 0.0\\nsamples = 6\\nvalue = [0, 0, 0, 6, 0, 0]\\nclass = 3'),\n Text(0.5309734513274337, 0.3888888888888889, 'entropy = 0.0\\nsamples = 9\\nvalue = [0, 0, 0, 0, 9, 0]\\nclass = 4'),\n Text(0.6106194690265486, 0.6111111111111112, 'Weight &lt;= 80.5\\nentropy = 1.133\\nsamples = 37\\nvalue = [0, 0, 21, 15, 1, 0]\\nclass = 2'),\n Text(0.5929203539823009, 0.5, 'entropy = 0.0\\nsamples = 11\\nvalue = [0, 0, 11, 0, 0, 0]\\nclass = 2'),\n Text(0.6283185840707964, 0.5, 'Height &lt;= 188.5\\nentropy = 1.169\\nsamples = 26\\nvalue = [0, 0, 10, 15, 1, 0]\\nclass = 3'),\n Text(0.6106194690265486, 0.3888888888888889, 'Weight &lt;= 89.0\\nentropy = 1.049\\nsamples = 21\\nvalue = [0, 0, 5, 15, 1, 0]\\nclass = 3'),\n Text(0.5663716814159292, 0.2777777777777778, 'Height &lt;= 176.0\\nentropy = 0.98\\nsamples = 12\\nvalue = [0, 0, 5, 7, 0, 0]\\nclass = 3'),\n Text(0.5309734513274337, 0.16666666666666666, 'Gender_Encoded &lt;= 0.5\\nentropy = 0.811\\nsamples = 4\\nvalue = [0, 0, 3, 1, 0, 0]\\nclass = 2'),\n Text(0.5132743362831859, 0.05555555555555555, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 1, 0, 0]\\nclass = 3'),\n Text(0.5486725663716814, 0.05555555555555555, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 0, 3, 0, 0, 0]\\nclass = 2'),\n Text(0.6017699115044248, 0.16666666666666666, 'Height &lt;= 183.0\\nentropy = 0.811\\nsamples = 8\\nvalue = [0, 0, 2, 6, 0, 0]\\nclass = 3'),\n Text(0.584070796460177, 0.05555555555555555, 'entropy = 0.0\\nsamples = 5\\nvalue = [0, 0, 0, 5, 0, 0]\\nclass = 3'),\n Text(0.6194690265486725, 0.05555555555555555, 'entropy = 0.918\\nsamples = 3\\nvalue = [0, 0, 2, 1, 0, 0]\\nclass = 2'),\n Text(0.6548672566371682, 0.2777777777777778, 'Height &lt;= 173.0\\nentropy = 0.503\\nsamples = 9\\nvalue = [0, 0, 0, 8, 1, 0]\\nclass = 3'),\n Text(0.6371681415929203, 0.16666666666666666, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0]\\nclass = 4'),\n Text(0.672566371681416, 0.16666666666666666, 'entropy = 0.0\\nsamples = 8\\nvalue = [0, 0, 0, 8, 0, 0]\\nclass = 3'),\n Text(0.6460176991150443, 0.3888888888888889, 'entropy = 0.0\\nsamples = 5\\nvalue = [0, 0, 5, 0, 0, 0]\\nclass = 2'),\n Text(0.7920353982300885, 0.8333333333333334, 'Height &lt;= 171.5\\nentropy = 1.229\\nsamples = 194\\nvalue = [0, 0, 0, 16, 55, 123]\\nclass = 5'),\n Text(0.7345132743362832, 0.7222222222222222, 'Weight &lt;= 116.0\\nentropy = 0.363\\nsamples = 101\\nvalue = [0, 0, 0, 0, 7, 94]\\nclass = 5'),\n Text(0.7168141592920354, 0.6111111111111112, 'Height &lt;= 164.0\\nentropy = 0.797\\nsamples = 29\\nvalue = [0, 0, 0, 0, 7, 22]\\nclass = 5'),\n Text(0.6991150442477876, 0.5, 'Weight &lt;= 95.5\\nentropy = 0.258\\nsamples = 23\\nvalue = [0, 0, 0, 0, 1, 22]\\nclass = 5'),\n Text(0.6814159292035398, 0.3888888888888889, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0]\\nclass = 4'),\n Text(0.7168141592920354, 0.3888888888888889, 'entropy = 0.0\\nsamples = 22\\nvalue = [0, 0, 0, 0, 0, 22]\\nclass = 5'),\n Text(0.7345132743362832, 0.5, 'entropy = 0.0\\nsamples = 6\\nvalue = [0, 0, 0, 0, 6, 0]\\nclass = 4'),\n Text(0.7522123893805309, 0.6111111111111112, 'entropy = 0.0\\nsamples = 72\\nvalue = [0, 0, 0, 0, 0, 72]\\nclass = 5'),\n Text(0.8495575221238938, 0.7222222222222222, 'Weight &lt;= 126.5\\nentropy = 1.454\\nsamples = 93\\nvalue = [0, 0, 0, 16, 48, 29]\\nclass = 4'),\n Text(0.7876106194690266, 0.6111111111111112, 'Weight &lt;= 114.5\\nentropy = 0.918\\nsamples = 48\\nvalue = [0, 0, 0, 16, 32, 0]\\nclass = 4'),\n Text(0.7699115044247787, 0.5, 'Height &lt;= 181.5\\nentropy = 1.0\\nsamples = 32\\nvalue = [0, 0, 0, 16, 16, 0]\\nclass = 3'),\n Text(0.7522123893805309, 0.3888888888888889, 'entropy = 0.0\\nsamples = 13\\nvalue = [0, 0, 0, 0, 13, 0]\\nclass = 4'),\n Text(0.7876106194690266, 0.3888888888888889, 'Weight &lt;= 105.5\\nentropy = 0.629\\nsamples = 19\\nvalue = [0, 0, 0, 16, 3, 0]\\nclass = 3'),\n Text(0.7699115044247787, 0.2777777777777778, 'entropy = 0.0\\nsamples = 10\\nvalue = [0, 0, 0, 10, 0, 0]\\nclass = 3'),\n Text(0.8053097345132744, 0.2777777777777778, 'Height &lt;= 190.0\\nentropy = 0.918\\nsamples = 9\\nvalue = [0, 0, 0, 6, 3, 0]\\nclass = 3'),\n Text(0.7876106194690266, 0.16666666666666666, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 0, 0, 0, 3, 0]\\nclass = 4'),\n Text(0.8230088495575221, 0.16666666666666666, 'entropy = 0.0\\nsamples = 6\\nvalue = [0, 0, 0, 6, 0, 0]\\nclass = 3'),\n Text(0.8053097345132744, 0.5, 'entropy = 0.0\\nsamples = 16\\nvalue = [0, 0, 0, 0, 16, 0]\\nclass = 4'),\n Text(0.911504424778761, 0.6111111111111112, 'Height &lt;= 185.5\\nentropy = 0.939\\nsamples = 45\\nvalue = [0, 0, 0, 0, 16, 29]\\nclass = 5'),\n Text(0.8761061946902655, 0.5, 'Weight &lt;= 139.5\\nentropy = 0.529\\nsamples = 25\\nvalue = [0, 0, 0, 0, 3, 22]\\nclass = 5'),\n Text(0.8584070796460177, 0.3888888888888889, 'Height &lt;= 178.5\\nentropy = 0.881\\nsamples = 10\\nvalue = [0, 0, 0, 0, 3, 7]\\nclass = 5'),\n Text(0.8407079646017699, 0.2777777777777778, 'entropy = 0.0\\nsamples = 5\\nvalue = [0, 0, 0, 0, 0, 5]\\nclass = 5'),\n Text(0.8761061946902655, 0.2777777777777778, 'Height &lt;= 184.5\\nentropy = 0.971\\nsamples = 5\\nvalue = [0, 0, 0, 0, 3, 2]\\nclass = 4'),\n Text(0.8584070796460177, 0.16666666666666666, 'Height &lt;= 180.0\\nentropy = 0.811\\nsamples = 4\\nvalue = [0, 0, 0, 0, 3, 1]\\nclass = 4'),\n Text(0.8407079646017699, 0.05555555555555555, 'entropy = 1.0\\nsamples = 2\\nvalue = [0, 0, 0, 0, 1, 1]\\nclass = 4'),\n Text(0.8761061946902655, 0.05555555555555555, 'entropy = 0.0\\nsamples = 2\\nvalue = [0, 0, 0, 0, 2, 0]\\nclass = 4'),\n Text(0.8938053097345132, 0.16666666666666666, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 1]\\nclass = 5'),\n Text(0.8938053097345132, 0.3888888888888889, 'entropy = 0.0\\nsamples = 15\\nvalue = [0, 0, 0, 0, 0, 15]\\nclass = 5'),\n Text(0.9469026548672567, 0.5, 'Weight &lt;= 139.0\\nentropy = 0.934\\nsamples = 20\\nvalue = [0, 0, 0, 0, 13, 7]\\nclass = 4'),\n Text(0.9292035398230089, 0.3888888888888889, 'entropy = 0.0\\nsamples = 9\\nvalue = [0, 0, 0, 0, 9, 0]\\nclass = 4'),\n Text(0.9646017699115044, 0.3888888888888889, 'Height &lt;= 196.5\\nentropy = 0.946\\nsamples = 11\\nvalue = [0, 0, 0, 0, 4, 7]\\nclass = 5'),\n Text(0.9469026548672567, 0.2777777777777778, 'Height &lt;= 194.0\\nentropy = 0.544\\nsamples = 8\\nvalue = [0, 0, 0, 0, 1, 7]\\nclass = 5'),\n Text(0.9292035398230089, 0.16666666666666666, 'entropy = 0.0\\nsamples = 5\\nvalue = [0, 0, 0, 0, 0, 5]\\nclass = 5'),\n Text(0.9646017699115044, 0.16666666666666666, 'Weight &lt;= 150.0\\nentropy = 0.918\\nsamples = 3\\nvalue = [0, 0, 0, 0, 1, 2]\\nclass = 5'),\n Text(0.9469026548672567, 0.05555555555555555, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0]\\nclass = 4'),\n Text(0.9823008849557522, 0.05555555555555555, 'entropy = 0.0\\nsamples = 2\\nvalue = [0, 0, 0, 0, 0, 2]\\nclass = 5'),\n Text(0.9823008849557522, 0.2777777777777778, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 0, 0, 0, 3, 0]\\nclass = 4')]"
  },
  {
    "objectID": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#accuracy-results-on-test-data",
    "href": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#accuracy-results-on-test-data",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Accuracy Results on Test Data",
    "text": "Accuracy Results on Test Data\n\nprint(\"Micro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='micro'))\nprint(\"Macro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='macro'))\nprint(\"Precision: \", sklearn.metrics.precision_score(y_test, y_pred, average=\"weighted\"))\nprint(\"Recall: \", sklearn.metrics.recall_score(y_test, y_pred, average='weighted'))\n\n# Calculate AUC-PRC for each class\nauc_prc_scores = []\nfor class_index in range(y_pred.shape[0]):\n    precision, recall, _ = sklearn.metrics.precision_recall_curve(y_test == class_index, y_pred == class_index)\n    auc_prc_scores.append(sklearn.metrics.auc(recall, precision))\n\n# Compute the summary metric (e.g., micro-average)\nmicro_avg_auc_prc = sklearn.metrics.auc(recall, precision)\n\nprint(\"AUC-PRC for Each Class:\", auc_prc_scores)\nprint(\"Micro-Average AUC-PRC:\", micro_avg_auc_prc)\n\nMicro F1:  0.8000000000000002\nMacro F1:  0.7033683460612853\nPrecision:  0.8400535714285715\nRecall:  0.8\nAUC-PRC for Each Class: [0.75, 0.625, 0.7875, 0.8284340659340659, 0.8070833333333334, 0.915301724137931, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\nMicro-Average AUC-PRC: 0.5\n\n\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n\n\n\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nconf_df = pd.DataFrame(confusion_matrix, index=[f\"{i}\" for i in range(6)], columns=[f\"{i}\" for i in range(6)])\nheatmap = sns.heatmap(conf_df, annot=True, fmt=\"d\", linewidths=0.35, cmap=\"YlGnBu\")\nplt.title(f\"Model Predictions With {(np.sum(confusion_matrix.diagonal()) / y_test.shape[0]) * 100:.2f}% Accuracy\")\n\nText(0.5, 1.0, 'Model Predictions With 80.00% Accuracy')"
  },
  {
    "objectID": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#hyperparamter-tuning",
    "href": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#hyperparamter-tuning",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Hyperparamter Tuning",
    "text": "Hyperparamter Tuning\n\nparam_grid = {\n    'max_depth': [i for i in range(2, 9)],\n    'min_samples_leaf': [2 ** i for i in range(0, 4)],\n    'criterion': [\"entropy\", \"gini\"]\n}\n\nrf = RandomForestClassifier(random_state=42)\n\nprint(y_train.shape)\ngrid_search_rf = GridSearchCV(rf, param_grid, cv=StratifiedKFold(n_splits=3), scoring='accuracy')\ngrid_search_rf.fit(X_train, y_train.values.ravel())\nbest_params_rf = grid_search_rf.best_params_\nprint(\"Best Hyperparameters:\", best_params_rf)\nprint(\"Best Score:\", grid_search_rf.best_score_)\n\n(320, 1)\nBest Hyperparameters: {'criterion': 'gini', 'max_depth': 8, 'min_samples_leaf': 1}\nBest Score: 0.8125844942103098"
  },
  {
    "objectID": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#randomforest-classifier",
    "href": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#randomforest-classifier",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "RandomForest Classifier",
    "text": "RandomForest Classifier\n\nrf = RandomForestClassifier(n_estimators=300, \n                            max_depth=best_params_rf[\"max_depth\"], \n                            min_samples_leaf=best_params_rf[\"min_samples_leaf\"],\n                            criterion=best_params_rf[\"criterion\"])\nrf.fit(X_train, y_train.values.ravel())\ny_pred = rf.predict(X_test)"
  },
  {
    "objectID": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#plotting-the-randomforest-trees",
    "href": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#plotting-the-randomforest-trees",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Plotting the RandomForest Trees",
    "text": "Plotting the RandomForest Trees\n\n    fig, axes = plt.subplots(nrows = 1,ncols = 5,figsize = (10,3), dpi=250)\n    for index in range(5):\n        tree.plot_tree(rf.estimators_[index],\n                    feature_names = X_train.columns.tolist(), \n                    class_names= [f\"{i}\" for i in range(6)],\n                    filled = True,\n                    ax = axes[index])\n\n        axes[index].set_title('Estimator: ' + str(index + 1), fontsize = 10)\n#     fig.savefig(f'rf_{dt.n_estimators}trees.png')\n    plt.show()\n\n\n\n\n\nprint(\"Micro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='micro'))\nprint(\"Macro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='macro'))\n\nMicro F1:  0.8875\nMacro F1:  0.8523938236704195\n\n\n\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nconf_df = pd.DataFrame(confusion_matrix, index=[f\"{i}\" for i in range(6)], columns=[f\"{i}\" for i in range(6)])\nheatmap = sns.heatmap(conf_df, annot=True, fmt=\"d\", linewidths=0.35, cmap=\"YlGnBu\")\nplt.title(f\"Model Predictions With {(np.sum(confusion_matrix.diagonal()) / y_test.shape[0]) * 100:.2f}% Accuracy\")\n\nText(0.5, 1.0, 'Model Predictions With 88.75% Accuracy')\n\n\n\n\n\n\n'''\nPlot a graph that compares the two models, randomly generated with tuned hyperparameter models\n'''\ndt_results = []\nrf_results = []\nn_samples = 40\nindexes = [i for i in range(n_samples)]\nfor i in indexes:\n    dt = DecisionTreeClassifier(max_depth=8, min_samples_leaf=2, min_samples_split=4)\n    dt.fit(X_train, y_train)\n    y_pred_dt = dt.predict(X_test)\n    \n    rf = RandomForestClassifier(n_estimators=325, max_depth=8)\n    rf.fit(X_train, y_train.values.ravel())\n    y_pred_rf = rf.predict(X_test)\n    \n    confusion_matrix_dt = sklearn.metrics.confusion_matrix(y_test, y_pred_dt)\n    confusion_matrix_rf = sklearn.metrics.confusion_matrix(y_test, y_pred_rf)\n    \n    dt_results.append((np.sum(confusion_matrix_dt.diagonal()) / y_test.shape[0]) * 100)\n    rf_results.append((np.sum(confusion_matrix_rf.diagonal()) / y_test.shape[0]) * 100)\n\nplt.plot(indexes, dt_results, label=\"DT results\")\nplt.plot(indexes, rf_results, label=\"RF results\")\nplt.xlabel(\"Sample\")\nplt.ylabel(\"Accuracy on Test Data in %\")\nplt.title(\"Accuracy Comparison Between DT and RF on Randomly Generated Models\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#improvements",
    "href": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#improvements",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Improvements",
    "text": "Improvements\n\nprint(X_train.shape)\nprint(y_train.shape)\n\nparam_grid = {\n    'max_depth': [i for i in range(2, 10)],\n    'min_samples_leaf': [2 ** i for i in range(0, 4)],\n    'criterion': [\"entropy\", \"gini\"]\n}\n\nrf = RandomForestClassifier(random_state=42)\ngrid_search_rf = GridSearchCV(rf, param_grid, cv=StratifiedKFold(n_splits=5), scoring='accuracy')\ngrid_search_rf.fit(X_train.drop(['Gender_Encoded'], axis=1), y_train.values.ravel())\nbest_params_rf = grid_search_rf.best_params_\nprint(\"Best Hyperparameters:\", best_params_rf)\nprint(\"Best Score:\", grid_search_rf.best_score_)\n\nrf = RandomForestClassifier(n_estimators=300, \n                            max_depth=best_params_rf[\"max_depth\"], \n                            min_samples_leaf=best_params_rf[\"min_samples_leaf\"],\n                            criterion=best_params_rf[\"criterion\"])\nrf.fit(X_train.drop(['Gender_Encoded'], axis=1), y_train.values.ravel())\ny_pred = rf.predict(X_test.drop(['Gender_Encoded'], axis=1))\n\n(320, 3)\n(320, 1)\nBest Hyperparameters: {'criterion': 'entropy', 'max_depth': 8, 'min_samples_leaf': 1}\nBest Score: 0.85625\n\n\n\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nconf_df = pd.DataFrame(confusion_matrix, index=[f\"{i}\" for i in range(6)], columns=[f\"{i}\" for i in range(6)])\nheatmap = sns.heatmap(conf_df, annot=True, fmt=\"d\", linewidths=0.35, cmap=\"YlGnBu\")\nplt.title(f\"Model Predictions With {(np.sum(confusion_matrix.diagonal()) / y_test.shape[0]) * 100:.2f}% Accuracy (NO GENDER)\")\n\nText(0.5, 1.0, 'Model Predictions With 90.00% Accuracy (NO GENDER)')\n\n\n\n\n\n\nprint(\"Micro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='micro'))\nprint(\"Macro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='macro'))\n\nMicro F1:  0.9\nMacro F1:  0.8450314415205457"
  },
  {
    "objectID": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#learning-curve",
    "href": "posts/DecisionTreeRFBlog/DecisionTreeNotebook.html#learning-curve",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Learning Curve",
    "text": "Learning Curve\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.tree import DecisionTreeClassifier\ntrain_sizes, train_scores, test_scores = learning_curve(rf, X_train, y_train.values.ravel(), cv=StratifiedKFold(n_splits=5))\ndisplay = LearningCurveDisplay(train_sizes=train_sizes,\n    train_scores=train_scores, test_scores=test_scores, score_name=\"Score\")\ndisplay.plot()\nplt.show()"
  },
  {
    "objectID": "posts/ClusteringBlog/index.html",
    "href": "posts/ClusteringBlog/index.html",
    "title": "Spotify Recommendation System With Clustering",
    "section": "",
    "text": "Author: Daniel Hassler\n```{python}\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.model_selection import train_test_split, ParameterGrid\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\n```"
  },
  {
    "objectID": "posts/ClusteringBlog/index.html#data-analysis",
    "href": "posts/ClusteringBlog/index.html#data-analysis",
    "title": "Spotify Recommendation System With Clustering",
    "section": "Data Analysis",
    "text": "Data Analysis\nClustering algorithms can be applied to many real-world applications, including but not limited to security, anomaly detection, document clustering, stock market analysis, image compression, and so much more. The application I decided to approach with clustering is a song recommendation system. I found a dataset on Kaggle containing almost 114,000 songs from the popular music streaming platform Spotify. Each entry in the dataset consists of many features including artists, track_name, track_genre, popularity, danceability, and many more.\nBefore I dive into the visualizaitons, I first dropped duplicates in the dataset to minimize problems with recommendations. Now, below are some visualizations showcasing certain features in a scatterplot. This gives me a rough idea what the dataset looks like with all of these features and genres.\n\n```{python}\noriginal_df = pd.read_csv(\"./dataset-dedup.csv\")\nprint(original_df.columns)\n# original_df = original_df.drop_duplicates(subset=[\"artists\", \"track_name\"], keep=\"first\").reset_index()\nprint(original_df.shape)\n# original_df.to_csv(\"./dataset-dedup.csv\")\nfeatures_x = [\"loudness\", \"popularity\", \"duration_ms\"]\nfeatures_y = [\"popularity\", \"energy\", \"tempo\"]\n\nfor i, (x,y) in enumerate(zip(features_x, features_y)):\n    scatter = sns.scatterplot(x=x, y=y, hue='track_genre', data=original_df, palette=\"viridis\", alpha=0.25)\n    legend_labels = original_df['track_genre'].unique()# [:3]  # Show only the first 3 genres\n    scatter.legend(title='Genre', labels=legend_labels, prop={'size': 1})\n    plt.title(f\"Scatter Plot of {x} vs {y} by genre\")\n    plt.show()\n\nplt.show()\n```\n\nIndex(['Unnamed: 0.1', 'index', 'Unnamed: 0', 'track_id', 'artists',\n       'album_name', 'track_name', 'popularity', 'duration_ms', 'explicit',\n       'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n       'time_signature', 'track_genre'],\n      dtype='object')\n(81344, 23)\n\n\n\n\n\n\n\n\n\n\n\n\n```{python}\nunique_vals = original_df['track_genre'].unique()\nplt.bar(unique_vals, original_df['track_genre'].value_counts())\nplt.title(\"Occurrence of Genre\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Genre\")\n_ = plt.xticks(rotation=\"vertical\", fontsize=4)\n```\n\n\n\n\nBecause a lot of these continuous variables: loudness, popularity, duration_ms overlap by genre significantly, I decided to drop these features during training, as well as many other features like energy, danceability, acousticness, as these metrics are too complex, overlapping, and even subjective. As a Spotify consumer myself, I like when Spotify gives me songs related to the current artist I’m listening to, so I thought important features in this dataset included: artists, track_genre, minimally. Although, I did try other features like key, and tempo on top of that."
  },
  {
    "objectID": "posts/ClusteringBlog/index.html#k-means",
    "href": "posts/ClusteringBlog/index.html#k-means",
    "title": "Spotify Recommendation System With Clustering",
    "section": "K-Means",
    "text": "K-Means\nThe K-Means algorithm clusters data by minimizing a criteria known as intertia, the within-cluster sum-of-squares. The formula for inertia, specified in the K-means documentation for Sklearn, is noted below:\nNoting some of the variables in the summation: n is the number of datapoints, mu is the mean of the cluster, also the cluster_centroid of the cluster C, ||x_i - \\mu||^2 represents the squared euclidean distance between point x_i and the centroid, and min() takes the min of the calculation\n\\[\n\\sum_{i=0}^{n}\\min_{\\mu_j \\in C}(||x_i - \\mu_j||^2)\n\\]\nIt is worth noting that the inertia method has some drawbacks. According to Sklearn, intertia makes the assumption that clusters are convex and isotropic, which may not always be the case. The documentation also states that inertia isn’t a “normalized metric”, so running PCA (principal component analysis) before the K-means clustering is beneficial (which is exactly what I did in later steps).\nA great benefit to K-means is its scalability to large sample sets, which is good for this problem since there are now 81,344 points.\n\nHyperparameter Tuning\nThe biggest hyperparameter for K-means is the number of clusters n_clusters. This hyperparameter is the amount of clusters to generate for the problem. Because the number of clusters largely effects the results of the model, it is important to tune this. In order to chose the best value, I loop through different values up to 80.\n\n```{python}\ninertia = []\n# train_df is the numeric representation of original_df\ntrain_df = original_df.drop(columns=['Unnamed: 0.1', 'index', 'Unnamed: 0', 'track_id',\n       'album_name', 'track_name', 'popularity', 'duration_ms', 'explicit',\n       'danceability', 'energy', 'loudness', 'mode', 'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n       'time_signature'])\n\nfor col in train_df.columns:\n    if not pd.api.types.is_numeric_dtype(train_df[col]):\n        train_df[col] = pd.factorize(original_df[col])[0]\n\nscaler = StandardScaler()\n# df_scaled is the scaled version of train_df\ndf_scaled = scaler.fit_transform(train_df)\npca_num_components = 2\n\n# df_pca to reduce dimensionality\npca = PCA(n_components=pca_num_components).fit_transform(df_scaled)\ndf_pca = pd.DataFrame(pca,columns=['pca1','pca2'])\n\nfor k in range(1, 80, 10):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit_predict(df_pca)\n    inertia.append(kmeans.inertia_)\n```\n\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n```{python}\nplt.plot(range(1, 80, 10), inertia, marker='o')\nplt.title('Elbow Method for Optimal K')\nplt.xlabel('Number of Clusters (K)')\nplt.ylabel('Inertia (Within-Cluster Sum of Squares)')\nplt.show()\n```\n\n\n\n\nThe elbow chart is a great way to visualize intertia vs number of clusters on the dataset. Since our goal is to generalize well, it’s not the best to choose the “lowest” inertia value. It is generally recommended in practice to choose the “elbow point”; I chose 10 as this looks very close to an elbow point for this distribution. Although, one drawback to this approach is its subjectiveness– you might think the elbow point is 12, whereas I think the elbow point is 10.\n\n\nK-Means for Spotify\nAfter taking the resulting elbow point, I run that through my own instance of kmeans, utilizing the Sklearn library, and store the predicted results into the original dataframe.\n\n```{python}\nkmeans = KMeans(n_clusters=10, random_state=42)\noriginal_df['clusters'] = kmeans.fit_predict(df_pca)\n```\n\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n```{python}\nsns.scatterplot(x=\"pca1\", y=\"pca2\", hue=original_df['clusters'], data=df_pca)\nplt.title('K-means Clustering PCA of 3 features [track_genre, artists, key]')\nplt.show()\n```\n\n\n\n\nThis is a PCA visualization of the clusters on the feature set track_genre, artists and key."
  },
  {
    "objectID": "posts/ClusteringBlog/index.html#evaluating-k-means-for-spotify",
    "href": "posts/ClusteringBlog/index.html#evaluating-k-means-for-spotify",
    "title": "Spotify Recommendation System With Clustering",
    "section": "Evaluating K-Means for Spotify",
    "text": "Evaluating K-Means for Spotify\nBelow are some sample mini-clusters. Since the goal of this overall problem is to recommend music based on certain songs, I decided to create a function that grabs an entry from the CSV file, finds the cluster it’s in, and computes the k-nearest neighbors of that song. These nearest neighbors would be the “recommendation” songs, in order.\nThe general idea we should see with these mini-clusters are songs that resemble the query song. In the case of the first example, I ran my function on Daughtry’s song “Home”. The recommended song (top 1) example was another Daughtry song “It’s Not Over”.\nWhen testing out different K-means implementations on different features, I found that simplicity is key. Having a ton of features is great for any dataset, but knowing how they interact with each other and how to simplify the problem makes for better results. I tested many different subsests of features including:\n\nall of the original dataset features (n=20)\nsubset of continuous variables\nsubset of just track_genre and artists\nsubset of track_genre, artists, tempo, and key. All of which are discrete, factual features.\nsubset of track_genre, artists, and key.\n\nMy final result ended up being the last option, although those did not generate the most similar clusters, especially compared to option 3. Although, I chose the last option as I was trying to find similar songs while spanning across other artists. Option 5 seemed to give me similar options across at least one or more genres with different artists. It is worth noting that some of the results gave me the same artists, which is good since those are similar songs too.\n\n```{python}\noriginal_df['Distance_to_Centroid'] = kmeans.transform(df_pca).min(axis=1)\n\ndef get_nearest_entry(idx, k=5):\n    # print(original_df.iloc[idx])\n    # print(train_df.iloc[idx])\n    cluster = kmeans.predict(df_pca.iloc[idx].to_frame().T)[0]\n    cluster_data = original_df[original_df[\"clusters\"] == cluster]\n    cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\n    cluster_data = cluster_data.sort_values(by=\"closest_entries_to_idx\")\n    # print(cluster_data[[\"artists\", \"album_name\", \"track_name\", \"track_genre\"]])\n\n    cluster_data.drop(columns=[\"closest_entries_to_idx\"])\n    print(f\"Top {k} Closest Examples to {cluster_data.loc[idx]['artists']}'s \\\"{cluster_data.loc[idx]['track_name']}\\\"\")\n    print(cluster_data[:k][[\"artists\", \"track_name\", \"track_genre\"]])\n    print(\"\\n\\n\")\n\nget_nearest_entry(35640) # rock song\nget_nearest_entry(16587) # country song\nget_nearest_entry(41220) # rap song\n```\n\nTop 5 Closest Examples to Daughtry's \"September\"\n            artists               track_name     track_genre\n35640      Daughtry                September          grunge\n35381      Daughtry            It's Not Over          grunge\n35839    Stone Sour                 Hesitate          grunge\n55666    Mark Broom                Five/Four  minimal-techno\n40063  TNT;POPR3B3L  I'm Raving - Radio Edit       hardstyle\n\n\n\nTop 5 Closest Examples to Florida Georgia Line's \"Stay\"\n                    artists  \\\n16587  Florida Georgia Line   \n8395    Datsik;Virtual Riot   \n8582            The Prodigy   \n8819            The Prodigy   \n8529            The Prodigy   \n\n                                              track_name track_genre  \n16587                                               Stay     country  \n8395                                               Nasty   breakbeat  \n8582                                               Girls   breakbeat  \n8819                                  We Are The Ruffest   breakbeat  \n8529   Out of Space - Techno Underworld Remix Remastered   breakbeat  \n\n\n\nTop 5 Closest Examples to Future;Lil Uzi Vert's \"Tic Tac\"\n                                          artists              track_name  \\\n41220                         Future;Lil Uzi Vert                 Tic Tac   \n43994  Pritam;Arijit Singh;Shadab;Altamash Faridi  Lambiyaan Si Judaiyaan   \n41226                                    Lil Baby                  All In   \n43981     Pritam;Sukhwinder Singh;Sunidhi Chauhan                Marjaani   \n41207                    Zack Knight;Jasmin Walia         Bom Diggy Diggy   \n\n      track_genre  \n41220     hip-hop  \n43994      indian  \n41226     hip-hop  \n43981      indian  \n41207     hip-hop  \n\n\n\n\n\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_16264\\3657151199.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_16264\\3657151199.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_16264\\3657151199.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\n\n\nOption 3 on the other hand gave me different songs for the same artists, which is fine for a recommendation system, but not what I was exactly going for. Below is a visualization of the clusters with just two features as well as its predictions.\n\n```{python}\noriginal_df = pd.read_csv(\"./dataset-dedup.csv\")\n# train_df is the numeric representation of original_df\ntrain_df = original_df.drop(columns=['Unnamed: 0.1', 'index', 'Unnamed: 0', 'track_id',\n       'album_name', 'track_name', 'popularity', 'duration_ms', 'explicit',\n       'danceability', 'key', 'energy', 'loudness', 'mode', 'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n       'time_signature'])\n\nfor col in train_df.columns:\n    if not pd.api.types.is_numeric_dtype(train_df[col]):\n        train_df[col] = pd.factorize(original_df[col])[0]\n\nscaler = StandardScaler()\n# df_scaled is the scaled version of train_df\ndf_scaled = scaler.fit_transform(train_df)\npca_num_components = 2\n\n# df_pca to reduce dimensionality\npca = PCA(n_components=pca_num_components).fit_transform(df_scaled)\ndf_pca = pd.DataFrame(pca,columns=['pca1','pca2'])\n\nkmeans = KMeans(n_clusters=10, random_state=42)\noriginal_df['clusters'] = kmeans.fit_predict(df_pca)\n\nsns.scatterplot(x=\"pca1\", y=\"pca2\", hue=original_df['clusters'], data=df_pca)\nplt.title('K-means Clustering PCA of 2 features [track_genre, artists]')\nplt.show()\noriginal_df['Distance_to_Centroid'] = kmeans.transform(df_pca).min(axis=1)\nget_nearest_entry(35640) # rock song\nget_nearest_entry(16587) # country song\nget_nearest_entry(41220) # rap song\n```\n\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_16264\\3657151199.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_16264\\3657151199.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_16264\\3657151199.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\n\n\n\n\n\nTop 5 Closest Examples to Daughtry's \"September\"\n        artists            track_name track_genre\n35439  Daughtry  Waiting for Superman      grunge\n35678  Daughtry         Gone Too Soon      grunge\n35802  Daughtry            I'll Fight      grunge\n35640  Daughtry             September      grunge\n35336  Daughtry                  Home      grunge\n\n\n\nTop 5 Closest Examples to Florida Georgia Line's \"Stay\"\n                    artists         track_name track_genre\n16592  Florida Georgia Line  I Love My Country     country\n16587  Florida Georgia Line               Stay     country\n16938  Florida Georgia Line           H.O.L.Y.     country\n16598  Florida Georgia Line           Sun Daze     country\n16975  Florida Georgia Line               Life     country\n\n\n\nTop 5 Closest Examples to Future;Lil Uzi Vert's \"Tic Tac\"\n                   artists              track_name track_genre\n41220  Future;Lil Uzi Vert                 Tic Tac     hip-hop\n39123            Lionheart                  Cursed    hardcore\n39035            Lionheart                LHHC '17    hardcore\n39040              Bodyjar  A Hazy Shade of Winter    hardcore\n39033         Naked Raygun              Rat Patrol    hardcore"
  },
  {
    "objectID": "posts/ClusteringBlog/index.html#conclusion",
    "href": "posts/ClusteringBlog/index.html#conclusion",
    "title": "Spotify Recommendation System With Clustering",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, I used K-means as a clustering algorithm for a song recommendation system. Though K-means does a good job at coming up with clusters and generating similar examples, other clustering algorithms such as DBSCAN may be a suitable option as well. In general, what I like about clustering algorithms for this problem domain, especially K-means, is its free range to determine what logical clusters should look like and its intuitiveness. There’s not only one correct way to do K-means."
  },
  {
    "objectID": "posts/AnomalyDetectionBlog/anomaly.html",
    "href": "posts/AnomalyDetectionBlog/anomaly.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Author: Daniel Hassler\nfrom sklearn.datasets import load_iris\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.model_selection import ParameterGrid\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/AnomalyDetectionBlog/anomaly.html#load-iris-data",
    "href": "posts/AnomalyDetectionBlog/anomaly.html#load-iris-data",
    "title": "Anomaly Detection",
    "section": "Load Iris Data",
    "text": "Load Iris Data\nFor the purpose of anomaly detection and being able to visualize anomalies/outliers, I ran PCA (principal component analysis) to reduce the dimensionality of the iris dataset (without targets) from shape (150, 4) to (150, 2). This allows me to visualize this dataset in two dimensions and makes clustering more efficient and representative of the true data.\n\niris = load_iris()\npca = PCA(n_components=2)\ndata_pca = pca.fit_transform(iris.data, iris.target)\ndf = pd.DataFrame(data_pca, columns=[\"PC1\", \"PC2\"])\ndf['target'] = iris.target\nprint(df)\n\n          PC1       PC2  target\n0   -2.684126  0.319397       0\n1   -2.714142 -0.177001       0\n2   -2.888991 -0.144949       0\n3   -2.745343 -0.318299       0\n4   -2.728717  0.326755       0\n..        ...       ...     ...\n145  1.944110  0.187532       2\n146  1.527167 -0.375317       2\n147  1.764346  0.078859       2\n148  1.900942  0.116628       2\n149  1.390189 -0.282661       2\n\n[150 rows x 3 columns]"
  },
  {
    "objectID": "posts/AnomalyDetectionBlog/anomaly.html#plot-the-current-iris-data",
    "href": "posts/AnomalyDetectionBlog/anomaly.html#plot-the-current-iris-data",
    "title": "Anomaly Detection",
    "section": "Plot the Current Iris Data",
    "text": "Plot the Current Iris Data\n\n_, ax = plt.subplots()\nscatter = ax.scatter(df[\"PC1\"], df[\"PC2\"], c=iris.target, cmap=\"copper\")\nax.set(xlabel=\"PC1\", ylabel=\"PC2\")\n_ = ax.legend(\n    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n)"
  },
  {
    "objectID": "posts/AnomalyDetectionBlog/anomaly.html#initialize-the-dbscan",
    "href": "posts/AnomalyDetectionBlog/anomaly.html#initialize-the-dbscan",
    "title": "Anomaly Detection",
    "section": "Initialize the DBSCAN",
    "text": "Initialize the DBSCAN\nIn order to create realistic clusters with DBSCAN and maximize an optimization, we need to preform hyperparameter tuning. Below is a GridSearch implementation for tweaking and finding the best eps and min_samples hyperparameters.\n\nparams = {\n    'eps': [i / 10 for i in range(1, 15)],\n    'min_samples': [i for i in range(1, 10)]\n}\n\nbest_score = -1\nbest_params = {}\n\nfor param_i in ParameterGrid(params):\n    db = DBSCAN(**param_i)\n    labels = db.fit_predict(data_pca)\n    # minimum of 4 clusters (3 classes + 1 outlier)\n    if len(np.unique(labels)) &lt;= 3:\n        continue\n\n    score = silhouette_score(data_pca, labels)\n    if score &gt; best_score:\n        best_score = score\n        best_params = param_i\n\nprint(\"Best Score: \", best_score)\nprint(\"Best Params: \", best_params)\n\n\n\nBest Score:  0.45189188489361554\nBest Params:  {'eps': 0.3, 'min_samples': 6}\n\n\n\ndb = DBSCAN(**best_params).fit(data_pca)\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint(\"Estimated number of clusters: %d\" % n_clusters_)\nprint(\"Estimated number of noise points: %d\" % n_noise_)\n\nEstimated number of clusters: 3\nEstimated number of noise points: 20"
  },
  {
    "objectID": "posts/AnomalyDetectionBlog/anomaly.html#plot-the-dbscan",
    "href": "posts/AnomalyDetectionBlog/anomaly.html#plot-the-dbscan",
    "title": "Anomaly Detection",
    "section": "Plot the DBSCAN",
    "text": "Plot the DBSCAN\n\ny_pred = db.fit_predict(data_pca)\nfig, ax = plt.subplots()\nscatter = ax.scatter(df[\"PC1\"], df[\"PC2\"], c=y_pred, cmap='copper')\ntext_labels = [\"outlier\", \"setosa\", \"versicolor\", \"virginica\"]\nlegend1 = ax.legend(scatter.legend_elements()[0], text_labels,\n                    loc=\"lower right\", title=\"Classes\")\nax.add_artist(legend1)\nplt.title(\"DBSCAN of Iris Data\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\n\nText(0, 0.5, 'PC2')"
  },
  {
    "objectID": "posts/AnomalyDetectionBlog/index.html",
    "href": "posts/AnomalyDetectionBlog/index.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Author: Daniel Hassler"
  },
  {
    "objectID": "posts/AnomalyDetectionBlog/index.html#introduction",
    "href": "posts/AnomalyDetectionBlog/index.html#introduction",
    "title": "Anomaly Detection",
    "section": "Introduction",
    "text": "Introduction\nAnomaly detection is a common task for a lot of unsupervised learning settings, but it can also good for supervised settings as well. In the case of anomaly detection here, I will be performing DBSCAN, a clustering algorithm, on the infamous Iris dataset to detect outliers.\nFirst, I import all the necessary libraries for the project:\n\n```{python}\nfrom sklearn.datasets import load_iris\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.model_selection import ParameterGrid\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n```"
  },
  {
    "objectID": "posts/AnomalyDetectionBlog/index.html#data",
    "href": "posts/AnomalyDetectionBlog/index.html#data",
    "title": "Anomaly Detection",
    "section": "Data",
    "text": "Data\nFor the data, I will be using the Iris dataset taken from sklearn.datasets library. This dataset is useful for educational purposes and is an accurate representation of some real world data; it contains 150 samples of iris data and has 4 columns: Sepal Length, Sepal Width, Petal Length and Petal Width.\nFor the purpose of anomaly detection and being able to visualize anomalies/outliers, I ran PCA (principal component analysis) to reduce the dimensionality of the iris dataset (without targets) from shape (150, 4) to (150, 2). PCA finds the eigenvalues and eigenvectors of the covariance matrix of the entire dataset, and the algorithm takes the top n_components, in this case two, to represent the data in two dimensions. This allows me to visualize this dataset in two dimensions and makes clustering more efficient and representative.\n\n```{python}\niris = load_iris()\npca = PCA(n_components=2)\ndata_pca = pca.fit_transform(iris.data, iris.target)\ndf = pd.DataFrame(data_pca, columns=[\"PC1\", \"PC2\"])\ndf['target'] = iris.target\nprint(df)\n```\n\n          PC1       PC2  target\n0   -2.684126  0.319397       0\n1   -2.714142 -0.177001       0\n2   -2.888991 -0.144949       0\n3   -2.745343 -0.318299       0\n4   -2.728717  0.326755       0\n..        ...       ...     ...\n145  1.944110  0.187532       2\n146  1.527167 -0.375317       2\n147  1.764346  0.078859       2\n148  1.900942  0.116628       2\n149  1.390189 -0.282661       2\n\n[150 rows x 3 columns]\n\n\nNext, I plot the reduced dimensionality representation of the Iris dataset:\n\n```{python}\n_, ax = plt.subplots()\nscatter = ax.scatter(df[\"PC1\"], df[\"PC2\"], c=iris.target, cmap=\"copper\")\nax.set(xlabel=\"PC1\", ylabel=\"PC2\")\n_ = ax.legend(\n    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n)\n```"
  },
  {
    "objectID": "posts/AnomalyDetectionBlog/index.html#dbscan-clustering",
    "href": "posts/AnomalyDetectionBlog/index.html#dbscan-clustering",
    "title": "Anomaly Detection",
    "section": "DBSCAN Clustering",
    "text": "DBSCAN Clustering\nDensity-Based Spatial Clustering Of Applications With Noise (DBSCAN) is a clustering algorithm that groups each point into a neighborhood, given a radius (eps) and a minimum number of points (min_samples). This is more representative for applications with real-life data, especially since they can contain noise.\n\nHyperparameter Tuning\nIn order to create realistic clusters with DBSCAN and maximize an optimization, we need to preform hyperparameter tuning. Below is a GridSearch implementation for tweaking and finding the best eps and min_samples hyperparameters.\n\n```{python}\nparams = {\n    'eps': [i / 10 for i in range(1, 15)],\n    'min_samples': [i for i in range(1, 10)]\n}\n\nbest_score = -1\nbest_params = {}\n\nfor param_i in ParameterGrid(params):\n    db = DBSCAN(**param_i)\n    labels = db.fit_predict(data_pca)\n    # minimum of 4 clusters (3 classes + 1 outlier)\n    if len(np.unique(labels)) &lt;= 3:\n        continue\n    curr_score = silhouette_score(data_pca, labels)\n    if curr_score &gt; best_score:\n        best_score = curr_score\n        best_params = param_i\n\nprint(\"Best Score: \", best_score)\nprint(\"Best Params: \", best_params)\n```\n\nBest Score:  0.45189188489361554\nBest Params:  {'eps': 0.3, 'min_samples': 6}\n\n\n\n\nDBSCAN Initialization and Visualization\nNext, I plugged in the “best” hyperparameters to the DBSCAN object generated from the GridSearch, and visualized the DBSCAN clusters with the outliers in the next codeblock.\n\n```{python}\ndb = DBSCAN(**best_params).fit(data_pca)\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint(\"Estimated number of clusters: %d\" % n_clusters_)\nprint(\"Estimated number of noise points: %d\" % n_noise_)\n```\n\nEstimated number of clusters: 3\nEstimated number of noise points: 20\n\n\n\n```{python}\ny_pred = db.fit_predict(data_pca)\nfig, ax = plt.subplots()\nscatter = ax.scatter(df[\"PC1\"], df[\"PC2\"], c=y_pred, cmap='copper')\ntext_labels = [\"outlier\", \"setosa\", \"versicolor\", \"virginica\"]\nlegend1 = ax.legend(scatter.legend_elements()[0], text_labels,\n                    loc=\"lower right\", title=\"Classes\")\nax.add_artist(legend1)\nplt.title(\"DBSCAN of Iris Data\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\n```\n\nText(0, 0.5, 'PC2')\n\n\n\n\n\nGiven the best hyperparameters, with realistic limitations of at least 4 clusters (3 classes, 1 outlier), the clusters look roughly similar to the expected classifications. The great advantage to DBSCAN is the ability to come up with these clusters without knowledge of the original labels (unsupervised), and based on the visualization of all this, it’s roughly representative of the actual data.\n\n\nOutliers\nThe outliers from the above visualization are represented in black. Based on the configuration of the DBSCAN object, it produced three main clusters, one for each label, and a predicted outlier one. It is important to note that the outliers in the graph are past the farthest ends of the main clusters, which is truly representative of outliers/anomalies. To emphasize further, the strength and quantity of such outliers in a DBSCAN cluster is heavily dependent on its hyperparameter setup."
  },
  {
    "objectID": "posts/AnomalyDetectionBlog/index.html#improvements",
    "href": "posts/AnomalyDetectionBlog/index.html#improvements",
    "title": "Anomaly Detection",
    "section": "Improvements",
    "text": "Improvements\nThere are some improvements we can make to DBSCAN. One improvement would include more data samples, as more data CAN further improve the clusters and limit some outliers. Another improvement we can make is exploring the hyperparameters further with finer eps values."
  },
  {
    "objectID": "posts/ClusteringBlog/spotify.html",
    "href": "posts/ClusteringBlog/spotify.html",
    "title": "Spotify Recommendation System",
    "section": "",
    "text": "Author: Daniel Hassler\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.model_selection import train_test_split, ParameterGrid\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score"
  },
  {
    "objectID": "posts/ClusteringBlog/spotify.html#data-analysis",
    "href": "posts/ClusteringBlog/spotify.html#data-analysis",
    "title": "Spotify Recommendation System",
    "section": "Data Analysis",
    "text": "Data Analysis\nLook at -&gt; * https://github.com/ageron/handson-ml/blob/master/08_dimensionality_reduction.ipynb * https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset/code\n\noriginal_df = pd.read_csv(\"./dataset-dedup.csv\")\nprint(original_df.columns)\n# original_df = original_df.drop_duplicates(subset=[\"artists\", \"track_name\"], keep=\"first\").reset_index()\nprint(original_df.shape)\n# original_df.to_csv(\"./dataset-dedup.csv\")\nfeatures_x = [\"loudness\", \"popularity\", \"duration_ms\"]\nfeatures_y = [\"popularity\", \"energy\", \"tempo\"]\n\nfor i, (x,y) in enumerate(zip(features_x, features_y)):\n    scatter = sns.scatterplot(x=x, y=y, hue='track_genre', data=original_df, palette=\"viridis\", alpha=0.25)\n    legend_labels = original_df['track_genre'].unique()# [:3]  # Show only the first 3 genres\n    scatter.legend(title='Genre', labels=legend_labels, prop={'size': 1})\n    plt.title(f\"Scatter Plot of {x} vs {y} by genre\")\n    plt.show()\n\nplt.show()\n\nIndex(['Unnamed: 0.1', 'index', 'Unnamed: 0', 'track_id', 'artists',\n       'album_name', 'track_name', 'popularity', 'duration_ms', 'explicit',\n       'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n       'time_signature', 'track_genre'],\n      dtype='object')\n(81344, 23)\n\n\n\n\n\n\n\n\n\n\n\n\nunique_vals = original_df['track_genre'].unique()\nplt.bar(unique_vals, original_df['track_genre'].value_counts())\nplt.title(\"Occurrence of Genre\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Genre\")\n_ = plt.xticks(rotation=\"vertical\", fontsize=4)"
  },
  {
    "objectID": "posts/ClusteringBlog/spotify.html#k-means-clustering",
    "href": "posts/ClusteringBlog/spotify.html#k-means-clustering",
    "title": "Spotify Recommendation System",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\n\nHyperparameter Tuning for K-Means\n\ninertia = []\n# train_df is the numeric representation of original_df\ntrain_df = original_df.drop(columns=['Unnamed: 0.1', 'index', 'Unnamed: 0', 'track_id',\n       'album_name', 'track_name', 'popularity', 'duration_ms', 'explicit',\n       'danceability', 'energy', 'loudness', 'mode', 'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n       'time_signature'])\n\nfor col in train_df.columns:\n    if not pd.api.types.is_numeric_dtype(train_df[col]):\n        train_df[col] = pd.factorize(original_df[col])[0]\n\nscaler = StandardScaler()\n# df_scaled is the scaled version of train_df\ndf_scaled = scaler.fit_transform(train_df)\npca_num_components = 2\n\n# df_pca to reduce dimensionality\npca = PCA(n_components=pca_num_components).fit_transform(df_scaled)\ndf_pca = pd.DataFrame(pca,columns=['pca1','pca2'])\n\nfor k in range(1, 80, 10):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit_predict(df_pca)\n    inertia.append(kmeans.inertia_)\n\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\nPlotting the Elbow Chart\n\n# Plot the elbow curve\nplt.plot(range(1, 80, 10), inertia, marker='o')\nplt.title('Elbow Method for Optimal K')\nplt.xlabel('Number of Clusters (K)')\nplt.ylabel('Inertia (Within-Cluster Sum of Squares)')\nplt.show()\n\n\n\n\n\n\nChoosing K-Means with “Elbow” Point\n\nkmeans = KMeans(n_clusters=10, random_state=42)\noriginal_df['clusters'] = kmeans.fit_predict(df_pca)\n\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\nsns.scatterplot(x=\"pca1\", y=\"pca2\", hue=original_df['clusters'], data=df_pca)\nplt.title('K-means Clustering PCA of 3 features [track_genre, artists, key]')\nplt.show()"
  },
  {
    "objectID": "posts/ClusteringBlog/spotify.html#evaluation",
    "href": "posts/ClusteringBlog/spotify.html#evaluation",
    "title": "Spotify Recommendation System",
    "section": "Evaluation",
    "text": "Evaluation\n\noriginal_df['Distance_to_Centroid'] = kmeans.transform(df_pca).min(axis=1)\n\n\ndef get_nearest_entry(idx, k=5):\n    # print(original_df.iloc[idx])\n    # print(train_df.iloc[idx])\n    cluster = kmeans.predict(df_pca.iloc[idx].to_frame().T)[0]\n    cluster_data = original_df[original_df[\"clusters\"] == cluster]\n    cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\n    cluster_data = cluster_data.sort_values(by=\"closest_entries_to_idx\")\n    # print(cluster_data[[\"artists\", \"album_name\", \"track_name\", \"track_genre\"]])\n\n    cluster_data.drop(columns=[\"closest_entries_to_idx\"])\n    print(f\"Top {k} Closest Examples to {cluster_data.loc[idx]['artists']}'s \\\"{cluster_data.loc[idx]['track_name']}\\\"\")\n    print(cluster_data[:k][[\"artists\", \"track_name\", \"track_genre\"]])\n    print(\"\\n\\n\")\n\nget_nearest_entry(35640) # rock song\nget_nearest_entry(16587) # country song\nget_nearest_entry(41220) # rap song\n\nTop 5 Closest Examples to Daughtry's \"September\"\n            artists               track_name     track_genre\n35640      Daughtry                September          grunge\n35381      Daughtry            It's Not Over          grunge\n35839    Stone Sour                 Hesitate          grunge\n55666    Mark Broom                Five/Four  minimal-techno\n40063  TNT;POPR3B3L  I'm Raving - Radio Edit       hardstyle\n\n\n\nTop 5 Closest Examples to Florida Georgia Line's \"Stay\"\n                    artists  \\\n16587  Florida Georgia Line   \n8395    Datsik;Virtual Riot   \n8582            The Prodigy   \n8819            The Prodigy   \n8529            The Prodigy   \n\n                                              track_name track_genre  \n16587                                               Stay     country  \n8395                                               Nasty   breakbeat  \n8582                                               Girls   breakbeat  \n8819                                  We Are The Ruffest   breakbeat  \n8529   Out of Space - Techno Underworld Remix Remastered   breakbeat  \n\n\n\nTop 5 Closest Examples to Future;Lil Uzi Vert's \"Tic Tac\"\n                                          artists              track_name  \\\n41220                         Future;Lil Uzi Vert                 Tic Tac   \n43994  Pritam;Arijit Singh;Shadab;Altamash Faridi  Lambiyaan Si Judaiyaan   \n41226                                    Lil Baby                  All In   \n43981     Pritam;Sukhwinder Singh;Sunidhi Chauhan                Marjaani   \n41207                    Zack Knight;Jasmin Walia         Bom Diggy Diggy   \n\n      track_genre  \n41220     hip-hop  \n43994      indian  \n41226     hip-hop  \n43981      indian  \n41207     hip-hop  \n\n\n\n\n\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_7112\\101083667.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_7112\\101083667.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_7112\\101083667.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\n\n\n\noriginal_df = pd.read_csv(\"./dataset-dedup.csv\")\n# train_df is the numeric representation of original_df\ntrain_df = original_df.drop(columns=['Unnamed: 0.1', 'index', 'Unnamed: 0', 'track_id',\n       'album_name', 'track_name', 'popularity', 'duration_ms', 'explicit',\n       'danceability', 'key', 'energy', 'loudness', 'mode', 'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n       'time_signature'])\n\nfor col in train_df.columns:\n    if not pd.api.types.is_numeric_dtype(train_df[col]):\n        train_df[col] = pd.factorize(original_df[col])[0]\n\nscaler = StandardScaler()\n# df_scaled is the scaled version of train_df\ndf_scaled = scaler.fit_transform(train_df)\npca_num_components = 2\n\n# df_pca to reduce dimensionality\npca = PCA(n_components=pca_num_components).fit_transform(df_scaled)\ndf_pca = pd.DataFrame(pca,columns=['pca1','pca2'])\n\nkmeans = KMeans(n_clusters=10, random_state=42)\noriginal_df['clusters'] = kmeans.fit_predict(df_pca)\n\nsns.scatterplot(x=\"pca1\", y=\"pca2\", hue=original_df['clusters'], data=df_pca)\nplt.title('K-means Clustering PCA of 2 features [track_genre, artists]')\nplt.show()\noriginal_df['Distance_to_Centroid'] = kmeans.transform(df_pca).min(axis=1)\nget_nearest_entry(35640) # rock song\nget_nearest_entry(16587) # country song\nget_nearest_entry(41220) # rap song\n\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_7112\\101083667.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_7112\\101083667.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_7112\\101083667.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\n\n\n\n\n\nTop 5 Closest Examples to Daughtry's \"September\"\n        artists            track_name track_genre\n35439  Daughtry  Waiting for Superman      grunge\n35678  Daughtry         Gone Too Soon      grunge\n35802  Daughtry            I'll Fight      grunge\n35640  Daughtry             September      grunge\n35336  Daughtry                  Home      grunge\n\n\n\nTop 5 Closest Examples to Florida Georgia Line's \"Stay\"\n                    artists         track_name track_genre\n16592  Florida Georgia Line  I Love My Country     country\n16587  Florida Georgia Line               Stay     country\n16938  Florida Georgia Line           H.O.L.Y.     country\n16598  Florida Georgia Line           Sun Daze     country\n16975  Florida Georgia Line               Life     country\n\n\n\nTop 5 Closest Examples to Future;Lil Uzi Vert's \"Tic Tac\"\n                   artists              track_name track_genre\n41220  Future;Lil Uzi Vert                 Tic Tac     hip-hop\n39123            Lionheart                  Cursed    hardcore\n39035            Lionheart                LHHC '17    hardcore\n39040              Bodyjar  A Hazy Shade of Winter    hardcore\n39033         Naked Raygun              Rat Patrol    hardcore"
  },
  {
    "objectID": "posts/DecisionTreeRFBlog/index.html",
    "href": "posts/DecisionTreeRFBlog/index.html",
    "title": "Comparing Decision Tree and Random Forest Classifier",
    "section": "",
    "text": "Author: Daniel Hassler"
  },
  {
    "objectID": "posts/DecisionTreeRFBlog/index.html#sample-data-used-in-classification",
    "href": "posts/DecisionTreeRFBlog/index.html#sample-data-used-in-classification",
    "title": "Comparing Decision Tree and Random Forest Classifier",
    "section": "Sample Data Used in Classification",
    "text": "Sample Data Used in Classification\nTo compare a DecisionTree and a RandomForestClassifier, the first step I took was to gather some data and run some visualizations and analysis. Through Kaggle, I was able to obtain a small dataset on person features and their BMI (Body Mass Index) data. The data consists of just around 400 samples with features: gender, height, and weight, and the goal is to predict BMI.\n\n```{python}\nimport numpy as np\nimport sklearn\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, learning_curve, LearningCurveDisplay\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n```\n\n\n```{python}\ndata = pd.read_csv(\"./datasets/bmi_train.csv\")\ncategory_mapping = {'Male': 0, 'Female': 1}\ndata['Gender_Encoded'] = data['Gender'].map(category_mapping) # converts categorical data to numeric data.\nX = data.drop(['Gender','Index'], axis=1)\ny = data.drop(['Gender', 'Gender_Encoded', 'Height', 'Weight'], axis=1)\nprint(\"All X shape: \", X.shape)\nprint(\"All y shape: \", y.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nprint(\"X_train shape: \", X_train.shape)\nprint(\"y_train shape: \", y_train.shape)\nprint(\"X_test shape: \", X_test.shape)\nprint(\"y_test shape: \", y_test.shape)\n```\n\nAll X shape:  (400, 3)\nAll y shape:  (400, 1)\nX_train shape:  (320, 3)\ny_train shape:  (320, 1)\nX_test shape:  (80, 3)\ny_test shape:  (80, 1)\n\n\nIn the above code snippet, I first populated my data into a Pandas dataframe and then split up the data into a “training” and “testing” datasets. I decided to go with an 80/20% split between train and test (with its corresponding labels), as that seems to be the most standard approach in the industry. The significant benefit here is that I possess labeled data on both sets, a challenge in practice. This enables me to make comparisons between predictions and outcomes on my data, eliminating the need to procure any additional “test” data.\n\n```{python}\nplt.figure(figsize=(6, 6))\nsns.scatterplot(data=data, x='Height', y='Weight', hue='Index', palette='deep')\nplt.title('Scatter Plot of Height vs Weight')\nplt.show()\n```\n\n\n\n\nNext, I created a scatterplot showing the distribution of the entire dataset (n=400) to find linear associations. Based on the scatterplot above, I was roughly able to see that there was a class imbalance.\n\n```{python}\n# Class imbalance, more obesity.\nunique_values, counts = np.unique(y, return_counts=True)\nplt.bar(unique_values, counts)\nplt.title(\"BMI Classes in the Entire Dataset\")\nplt.xlabel(\"BMI Class\")\nplt.ylabel(\"Occurences in Entire Dataset\")\nplt.show()\n```\n\n\n\n\nThe labels are all discrete and sequential, consisting of whole numbers between 0 and 5, further enforcing my intuition for using a classifier approach. A “0” in my case represents someone with an exeptionally low BMI, whereas a “5” depicts an exceptionally high BMI. Based on the distribution of the data, there appears to be a huge class imbalance, heavily favoring the amount of exceptionally high instances in the dataset; this was something I needed to keep in mind when building the classifiers for this dataset.\n\n```{python}\ncorrelation_matrix = data.corr()\n\n# Display a heatmap of the correlation matrix\nplt.figure(figsize=(10, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('Correlation Heatmap')\nplt.show()\n```\n\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_16796\\1253239144.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  correlation_matrix = data.corr()\n\n\n\n\n\nThe correlation matrix depicts the correlation between features (height, weight, gender, BMI) in the dataset. It uses the pearson’s correlation coefficient to compute this: \\[\nr =\n  \\frac{ \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y}) }{\n        \\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^{n}(y_i-\\bar{y})^2}}\n\\]\nBased on the features presented, most are not correlated strongly, but there is a glaring strong correlation between weight and BMI. It is also important to note that gender doesn’t influence classification results, as the key factors to determining BMI is height and weight."
  },
  {
    "objectID": "posts/DecisionTreeRFBlog/index.html#decisiontreeclassifier",
    "href": "posts/DecisionTreeRFBlog/index.html#decisiontreeclassifier",
    "title": "Comparing Decision Tree and Random Forest Classifier",
    "section": "DecisionTreeClassifier",
    "text": "DecisionTreeClassifier\nIn order to start the model building process, I decided to tune the hyperparamters first by running a GridSearch\n\n```{python}\nparam_grid = {\n    'max_depth': [i for i in range(2, 6)],\n    'min_samples_leaf': [2 ** i for i in range(0, 6)],\n    'criterion': [\"entropy\", \"gini\"]\n}\n\ndt = DecisionTreeClassifier(random_state=42)\n\ngrid_search_dt = GridSearchCV(dt, param_grid, cv=StratifiedKFold(n_splits=5), scoring='accuracy')\ngrid_search_dt.fit(X_train, y_train)\nbest_params_dt = grid_search_dt.best_params_\nprint(\"Best Hyperparameters:\", best_params_dt)\nprint(\"Best Score:\", grid_search_dt.best_score_)\n```\n\nBest Hyperparameters: {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 8}\nBest Score: 0.78125\n\n\nI recognized that max_depth was an important hyperparameter for the DecisionTree (DT), as the depth of the tree heavily influences overfitting, but other hyperparameters are important as well, such as:\n\nmin_samples_leaf: the minimum amount of samples needed in a leaf node of the DT. For example, when min_samples_leaf is set to 10, that means a node won’t split if it has fewer than 10 samples. When this number is higher, the model can create a more generalized tree, although, when the number is smaller, it’ll create more specific splits, resulting in a more complex tree (more potential for overfitting).\ncriterion: this hyperparameter chooses whether to use entropy or Gini index as a way to calculate dissimilarity in a node. I found that in most cases, entropy outpreformed the Gini index. \\[\nEntropy(C) = -\\sum_{c=1}^Cp(c)\\log(p(c))\n\\]\n\n\\[\nGini(C) = 1 - \\sum_{c=1}^Cp(c)^2\n\\]\nNow that I’ve determined the necessary hyperparameters for this classifier, I initialize the GridSearchCV object to analyze every combination of the above hyperparameters. Within its search, it goes through an important cross-validation step (cv) that splits the training data into multiple folds and iterates through each fold for each hyperparameter combination.\nThere were a few options I could’ve chose from for the cv parameter in GridSearchCV, but in order to account for class imbalance like I stated earlier, I decided to go with a StratifiedKFold cross-validator. StratifiedKFold accounts for class label imbalance by keeping an equal precentage of classes for training and testing represented in the dataset. Below is a picture representing this:\n\n\n```{python}\ndt = DecisionTreeClassifier(max_depth=best_params_dt[\"max_depth\"], min_samples_leaf=best_params_dt[\"min_samples_leaf\"], criterion=best_params_dt[\"criterion\"])\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\n```\n\nI then created a DecisionTreeClassifier with the ‘best’ tuned hyperparameters from the above grid search and populated the y_pred array with the predictions from the test dataset. After that, I plotted the tree out using Sklearn’s plot_tree method.\n\n```{python}\nplt.figure(figsize=(10, 10))\nplot_tree(dt, feature_names=X_train.columns.tolist(), class_names=['0', '1', '2','3','4','5'], filled=True)\nplt.show()\n```\n\n\n\n\nAfter plotting the tree, I created a confusion matrix, showing where my predictions fell. Currently, the model sits around 75-86% accurate due to the above hyperparameter values and the randomly generated tree with those hyperparameter values. Not bad for a small dataset with class imbalance.\n\n```{python}\nprint(\"Micro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='micro'))\nprint(\"Macro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='macro'))\n```\n\nMicro F1:  0.7375\nMacro F1:  0.6487497023100738\n\n\n\n```{python}\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nconf_df = pd.DataFrame(confusion_matrix, index=[f\"{i}\" for i in range(6)], columns=[f\"{i}\" for i in range(6)])\nheatmap = sns.heatmap(conf_df, annot=True, fmt=\"d\", linewidths=0.35, cmap=\"YlGnBu\")\nplt.title(f\"Model Predictions With {(np.sum(confusion_matrix.diagonal()) / y_test.shape[0]) * 100:.2f}% Accuracy\")\n```\n\nText(0.5, 1.0, 'Model Predictions With 73.75% Accuracy')"
  },
  {
    "objectID": "posts/DecisionTreeRFBlog/index.html#randomforestclassifer-ensemble-approach",
    "href": "posts/DecisionTreeRFBlog/index.html#randomforestclassifer-ensemble-approach",
    "title": "Comparing Decision Tree and Random Forest Classifier",
    "section": "RandomForestClassifer (Ensemble approach)",
    "text": "RandomForestClassifer (Ensemble approach)\nAs above with the DecisionTreeClassifer, I first started to implement the RandomForestClassifier by tuning the hyperparameter values. Since a RandomForest is just a collection of DecisionTrees, RandomForestClassifiers, like a DecisionTreeClassifier, have mostly the same hyperparameters, but the RandomForestClassifier has an extra one for the amount of DecisionTrees that should be included in the forest (n_estimators).\nThough this step wasn’t as necessary, since I already did the hyperparameter tuning part for the DecisionTree, but I decided to include it again for the RandomForest with the number of estimators.\nIt is important to note that the n_estimators hyperparameter won’t cause the model to overfit. In fact, it actually does better at generalization when increasing the number of estimators due to the diversity of opinions the model presents for each unique DecisionTree. The only way overfitting can happen in a RandomForest depends on how the underlying DecisionTrees are set up, not the quantity of them.\n\n```{python}\nparam_grid = {\n    'max_depth': [i for i in range(2, 6)],\n    'min_samples_leaf': [2 ** i for i in range(0, 6)],\n    'criterion': [\"entropy\", \"gini\"]\n}\n\nrf = RandomForestClassifier(random_state=42)\n\nprint(y_train.shape)\ngrid_search_rf = GridSearchCV(rf, param_grid, cv=StratifiedKFold(n_splits=5), scoring='accuracy')\ngrid_search_rf.fit(X_train, y_train.values.ravel())\nbest_params_rf = grid_search_rf.best_params_\nprint(\"Best Hyperparameters:\", best_params_rf)\nprint(\"Best Score:\", grid_search_rf.best_score_)\n```\n\n(320, 1)\nBest Hyperparameters: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1}\nBest Score: 0.79375\n\n\n\n```{python}\nrf = RandomForestClassifier(n_estimators=300, \n                            max_depth=best_params_rf[\"max_depth\"], \n                            min_samples_leaf=best_params_rf[\"min_samples_leaf\"],\n                            criterion=best_params_rf[\"criterion\"])\nrf.fit(X_train, y_train.values.ravel())\ny_pred = rf.predict(X_test)\n```\n\nThe above code snippet creates the RandomForestClassifier with the same hyperparameters as the DecisionTree, in addition to the number of estimators (number of decision trees in the forest), trains the classifier, then stores a prediction array.\nHere is a visualizaiton of a subset of DecisionTrees in this RandomForest:\n\n```{python}\n    fig, axes = plt.subplots(nrows = 1,ncols = 5,figsize = (10,3), dpi=250)\n    for index in range(5):\n        tree.plot_tree(rf.estimators_[index],\n                    feature_names = X_train.columns.tolist(), \n                    class_names= [f\"{i}\" for i in range(6)],\n                    filled = True,\n                    ax = axes[index])\n\n        axes[index].set_title('Estimator: ' + str(index + 1), fontsize = 10)\n    plt.show()\n```\n\n\n\n\nAfter running the model, I checked the accuracy output of the prediction array and found that the RandomForestClassifier was able to increase the accuracy of the predictions by a considerable amount on average.\n\n```{python}\nprint(\"Micro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='micro'))\nprint(\"Macro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='macro'))\n```\n\nMicro F1:  0.85\nMacro F1:  0.7075440987205693\n\n\n\n```{python}\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nconf_df = pd.DataFrame(confusion_matrix, index=[f\"{i}\" for i in range(6)], columns=[f\"{i}\" for i in range(6)])\nheatmap = sns.heatmap(conf_df, annot=True, fmt=\"d\", linewidths=0.35, cmap=\"YlGnBu\")\nplt.title(f\"Model Predictions With {(np.sum(confusion_matrix.diagonal()) / y_test.shape[0]) * 100:.2f}% Accuracy\")\n```\n\nText(0.5, 1.0, 'Model Predictions With 85.00% Accuracy')\n\n\n\n\n\nFinally, I decided to calculate the accuracy preformance on multiple samples of RandomForestClassifiers and DecisionTrees at the same time and plot them out in a line chart.\n\n```{python}\n'''\nPlot a graph that compares the two models, randomly generated with tuned hyperparameter models\n'''\ndt_results = []\nrf_results = []\nn_samples = 40\nindexes = [i for i in range(n_samples)]\nfor i in indexes:\n    dt = DecisionTreeClassifier(max_depth=best_params_dt[\"max_depth\"], \n                   min_samples_leaf=best_params_dt[\"min_samples_leaf\"], criterion=best_params_dt[\"criterion\"])\n    dt.fit(X_train, y_train)\n    y_pred_dt = dt.predict(X_test)\n    \n    rf = RandomForestClassifier(n_estimators=300, \n                            max_depth=best_params_rf[\"max_depth\"], \n                            min_samples_leaf=best_params_rf[\"min_samples_leaf\"],\n                            criterion=best_params_rf[\"criterion\"])\n    rf.fit(X_train, y_train.values.ravel())\n    y_pred_rf = rf.predict(X_test)\n    \n    confusion_matrix_dt = sklearn.metrics.confusion_matrix(y_test, y_pred_dt)\n    confusion_matrix_rf = sklearn.metrics.confusion_matrix(y_test, y_pred_rf)\n    \n    dt_results.append((np.sum(confusion_matrix_dt.diagonal()) / y_test.shape[0]) * 100)\n    rf_results.append((np.sum(confusion_matrix_rf.diagonal()) / y_test.shape[0]) * 100)\n\nplt.plot(indexes, dt_results, label=\"DT results\")\nplt.plot(indexes, rf_results, label=\"RF results\")\nplt.xlabel(\"Sample\")\nplt.ylabel(\"Accuracy on Test Data in %\")\nplt.title(\"Accuracy Comparison Between DT and RF on Randomly Generated Models\")\nplt.legend()\nplt.show()\n```"
  },
  {
    "objectID": "posts/DecisionTreeRFBlog/index.html#model-improvements",
    "href": "posts/DecisionTreeRFBlog/index.html#model-improvements",
    "title": "Comparing Decision Tree and Random Forest Classifier",
    "section": "Model Improvements",
    "text": "Model Improvements\nNow that I’ve determined RandomForestClassifier as an overall better approach for this problem, I’ve included more ways to improve the current implementation.\nEarlier, I stated that gender may be a redudndant feature based on the correlation matrix, so I decided to drop that in the dataset when training the model.\n\n```{python}\nprint(X_train.shape)\nprint(y_train.shape)\n\nparam_grid = {\n    'max_depth': [i for i in range(2, 6)],\n    'min_samples_leaf': [2 ** i for i in range(0, 6)],\n    'criterion': [\"entropy\", \"gini\"]\n}\n\nrf = RandomForestClassifier(random_state=42)\ngrid_search_rf = GridSearchCV(rf, param_grid, cv=StratifiedKFold(n_splits=5), scoring='accuracy')\ngrid_search_rf.fit(X_train.drop(['Gender_Encoded'], axis=1), y_train.values.ravel())\nbest_params_rf = grid_search_rf.best_params_\nprint(\"Best Hyperparameters:\", best_params_rf)\nprint(\"Best Score:\", grid_search_rf.best_score_)\n\nrf = RandomForestClassifier(n_estimators=300, \n                            max_depth=best_params_rf[\"max_depth\"], \n                            min_samples_leaf=best_params_rf[\"min_samples_leaf\"],\n                            criterion=best_params_rf[\"criterion\"])\nrf.fit(X_train.drop(['Gender_Encoded'], axis=1), y_train.values.ravel())\ny_pred = rf.predict(X_test.drop(['Gender_Encoded'], axis=1))\n```\n\n(320, 3)\n(320, 1)\nBest Hyperparameters: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1}\nBest Score: 0.85\n\n\n\n```{python}\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nconf_df = pd.DataFrame(confusion_matrix, index=[f\"{i}\" for i in range(6)], columns=[f\"{i}\" for i in range(6)])\nheatmap = sns.heatmap(conf_df, annot=True, fmt=\"d\", linewidths=0.35, cmap=\"YlGnBu\")\nplt.title(f\"Model Predictions With {(np.sum(confusion_matrix.diagonal()) / y_test.shape[0]) * 100:.2f}% Accuracy (NO GENDER)\")\n```\n\nText(0.5, 1.0, 'Model Predictions With 86.25% Accuracy (NO GENDER)')\n\n\n\n\n\n\n```{python}\nprint(\"Micro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='micro'))\nprint(\"Macro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='macro'))\n```\n\nMicro F1:  0.8625\nMacro F1:  0.8370034682234619\n\n\nIt appears that removing that feature, on average, didn’t hurt the preformance of the overall model.\nFinally, below is a learning curve showing accuracy results in respect to the number of samples in the training set. This plot is heavily dependent on the random state of the generated RandomForestClassifier and its underlying DecisionTrees. Sometimes the model is overfitting, so I tried minimizing the hyperparameter values to make sure it mostly doesn’t.\n\n```{python}\ntrain_sizes, train_scores, test_scores = learning_curve(rf, X_train, y_train.values.ravel(), cv=StratifiedKFold(n_splits=5))\ndisplay = LearningCurveDisplay(train_sizes=train_sizes,\n    train_scores=train_scores, test_scores=test_scores, score_name=\"Score\")\ndisplay.plot()\nplt.title(\"Learning Curve on RandomForestClassifer (NO GENDER)\")\nplt.show()\n```"
  },
  {
    "objectID": "posts/DecisionTreeRFBlog/index.html#results-and-conclusions",
    "href": "posts/DecisionTreeRFBlog/index.html#results-and-conclusions",
    "title": "Comparing Decision Tree and Random Forest Classifier",
    "section": "Results and Conclusions",
    "text": "Results and Conclusions\nAfter doing simple experimentation with these models, I have found that, on average, the RandomForestClassifier outpreforms just a singular DecisionTreeClassifier. There are several advantages to having a forest of DecisionTrees rather than a singular tree:\n\nMore generalizability due to the ensemble approach to this problem\nLimits overfitting compared to a DT\nDT has high variance and instability, so having a forest of those trees in a more collective approach would help get more opinions at least.\n\nThough there is more resource complexity with a forest, the benefits of using that over a DT is worth the tradeoff."
  },
  {
    "objectID": "posts/NaiveBayesBlog/NaiveBayesNotebook.html",
    "href": "posts/NaiveBayesBlog/NaiveBayesNotebook.html",
    "title": "Probability Theory with Naive Bayes",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport mnist\n\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn.metrics import confusion_matrix"
  },
  {
    "objectID": "posts/NaiveBayesBlog/NaiveBayesNotebook.html#mnist-data",
    "href": "posts/NaiveBayesBlog/NaiveBayesNotebook.html#mnist-data",
    "title": "Probability Theory with Naive Bayes",
    "section": "MNIST Data",
    "text": "MNIST Data\n\n# mnist.init()\nX_train, y_train, X_test, y_test = mnist.load()\nprint(\"X_train len: \", len(X_train))\nprint(\"X_test len: \", len(X_test))\nprint(\"X_train shape: \", X_train.shape)\nprint(\"X_test shape: \", X_test.shape)\n\nplt.subplot(1, 2, 1)\nplt.title(\"Entry in Train Set\")\nimg = X_train[0,:].reshape(28,28) # First image in the training set.\nplt.imshow(img,cmap='gray')\n\nplt.subplot(1, 2, 2)\nplt.title(\"Entry in Test Set\")\nimg = X_test[0,:].reshape(28,28) # First image in the training set.\nplt.imshow(img,cmap='gray')\nplt.show() # Show the image\n\nX_train len:  60000\nX_test len:  10000\nX_train shape:  (60000, 784)\nX_test shape:  (10000, 784)\n\n\n\n\n\n\nVisualize Every Number\n\nunique_values, indices = np.unique(y_train, return_index=True)\nprint(unique_values, indices)\n\nfor i, label_index in enumerate(indices):\n    plt.subplot(1, len(indices), i + 1)\n    img = X_train[label_index,:].reshape(28,28)\n    plt.imshow(img,cmap='gray')\nplt.show()\n    \n\n[0 1 2 3 4 5 6 7 8 9] [ 1  3  5  7  2  0 13 15 17  4]"
  },
  {
    "objectID": "posts/NaiveBayesBlog/NaiveBayesNotebook.html#naive-bayes-model",
    "href": "posts/NaiveBayesBlog/NaiveBayesNotebook.html#naive-bayes-model",
    "title": "Probability Theory with Naive Bayes",
    "section": "Naive Bayes Model",
    "text": "Naive Bayes Model\n\nunique, counts = np.unique(y_train, return_counts=True)\nsum_counts = np.sum(counts)\npriors = np.divide(counts, sum_counts)\n\nnb = GaussianNB(priors=priors, var_smoothing=0.1)\nnb.fit(X_train, y_train)\ny_pred = nb.predict(X_test)\nprint(y_pred)\nprint(y_test)\n\n# print(\"Priors: \", nb.priors)\n\n[7 2 1 ... 9 8 6]\n[7 2 1 ... 4 5 6]\n\n\n\nAccuracy Results\n\nconfusion = confusion_matrix(y_test, y_pred)\nconf_df = pd.DataFrame(confusion, index=[f\"{i}\" for i in range(10)], columns=[f\"{i}\" for i in range(10)])\nheatmap = sns.heatmap(conf_df, annot=True, fmt=\"d\", linewidths=0.35, cmap=\"YlGnBu\")\nplt.title(f\"Model Predictions With {(np.sum(confusion.diagonal()) / y_test.shape[0]) * 100:.2f}% Accuracy\")\n\nText(0.5, 1.0, 'Model Predictions With 81.40% Accuracy')\n\n\n\n\n\n\nmean_pixel_values = nb.theta_\nplt.figure(figsize=(5,10))\nfor i, digit in enumerate(range(10)):\n    plt.subplot(len(indices) // 2, 2, i + 1)\n    plt.title(f\"Digit {digit}\")\n    plt.axis('off')\n    img = mean_pixel_values[digit].reshape(28,28)\n    plt.imshow(img)\nplt.plot()\n\n[]"
  },
  {
    "objectID": "posts/RegressionBlog/regression.html",
    "href": "posts/RegressionBlog/regression.html",
    "title": "Salary Prediction Using Regression on MLB Data",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.svm import SVR\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error"
  },
  {
    "objectID": "posts/RegressionBlog/regression.html#data-analysis",
    "href": "posts/RegressionBlog/regression.html#data-analysis",
    "title": "Salary Prediction Using Regression on MLB Data",
    "section": "Data Analysis",
    "text": "Data Analysis\nBefore plugging in the data into our SVM, it is important to evaluate redundant information to limit multicollinearity. We can find collinearities through the correlation matrix.\n\ndf = pd.read_csv(\"./Hitters.csv\")\nnum_cols = [col for col in df.columns if df[col].dtypes != \"O\"]\ncorr = df[num_cols].corr()\nsns.set(rc={'figure.figsize': (15, 15)})\nsns.heatmap(corr, cmap=\"RdBu\", annot=True)\nplt.show()\n\n\n\n\n\nRun PCA on highly-correlated features\n\npca_custat = PCA(n_components=1)\npca_indstat = PCA(n_components=1)\npca_fieldstat = PCA(n_components=1)\n\ncustat_df = df[[\"CAtBat\", \"CHits\", \"CHmRun\", \"CRuns\", \"CRBI\", \"CWalks\"]]\nindstat_df = df[[\"AtBat\", \"Hits\", \"HmRun\", \"Runs\", \"RBI\", \"Walks\"]]\nfieldstat_df = df[[\"Assists\", \"Errors\"]]\n\ncustat_df_pca = pca_custat.fit_transform(custat_df)\nindstat_df_pca = pca_indstat.fit_transform(indstat_df)\nfieldstat_df_pca = pca_fieldstat.fit_transform(fieldstat_df)\n\ndf_reduced = df.drop(columns=[\"AtBat\", \"Hits\", \"HmRun\", \"Runs\", \"RBI\", \"Walks\", \"CAtBat\", \"CHits\", \"CHmRun\", \"CRuns\", \"CRBI\", \"CWalks\", \"Assists\", \"Errors\"])\ndf_reduced = df_reduced.drop(columns=[\"League\", \"NewLeague\", \"Division\", \"Years\"])\ndf_reduced[\"I_PC1\"] = indstat_df_pca\ndf_reduced[\"C_PC2\"] = custat_df_pca\ndf_reduced[\"F_PC3\"] = fieldstat_df_pca\n\n# label_mapping_an = {'A': 0, 'N': 1}\n# label_mapping_div = {'E': 0, 'W': 1}\n\n\n# # Apply label encoding to the 'Category' column\n# df_reduced['League'] = df_reduced['League'].map(label_mapping_an)\n# df_reduced['NewLeague'] = df_reduced['NewLeague'].map(label_mapping_an)\n# df_reduced['Division'] = df_reduced['Division'].map(label_mapping_div)\ndf_reduced.dropna(axis=0, inplace=True)\n\nprint(df_reduced)\n\n     PutOuts  Salary       I_PC1        C_PC2       F_PC3\n1        632   475.0   72.590869   801.168761  -63.814023\n2        880   480.0 -108.738125 -1059.201154  -24.703570\n3        200   500.0 -124.894681  3171.420761  -96.026925\n4        805    91.5   63.304301 -2384.553500  -67.009725\n5        282   750.0 -221.371087  1777.108228  314.474814\n..       ...     ...         ...          ...         ...\n317      325   700.0 -117.568310    61.626177  -98.025844\n318      313   875.0 -122.303569  3049.912291  274.332014\n319       37   385.0  -96.674458 -1030.222990    6.049449\n320     1314   960.0 -201.645999   587.940371   24.204149\n321      408  1000.0 -255.582377  2378.079204 -103.023139\n\n[263 rows x 5 columns]\n\n\n\nnum_cols = [col for col in df_reduced.columns if df_reduced[col].dtypes != \"O\"]\ncorr = df_reduced[num_cols].corr()\nsns.set(rc={'figure.figsize': (15, 15)})\nsns.heatmap(corr, cmap=\"RdBu\", annot=True)\nplt.show()\n\n\n\n\nwe’ve reduced the data from 20 columns to 6 columns, which is important, as collinearity can negatively effect the preformance of regression models. Apart from the collinearity effect, I decided to get rid of discretely labeled binary relationships (labels 0 or 1), as this makes the linear regression model more complex and doesn’t effect the model preformance that much.\n\n\nCreate Training and Test Sets\nSince we’re trying to predict salary, I extract “salary” column from the dataframe, storing it into the label, and dropping that column from the feature data.\n\ndf = df_reduced\ny = df[\"Salary\"]\nX = df.drop(columns=\"Salary\")\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n\nprint(f\"Salary STD: ${np.std(y) * 1000:,.2f}\")\nprint(f\"Salary Mean: ${np.mean(y) * 1000:,.2f}\")\nprint(f\"Salary Low: ${np.min(y) * 1000:,.2f}\")\nprint(f\"Salary High: ${np.max(y) * 1000:,.2f}\")\n\nSalary STD: $450,260.22\nSalary Mean: $535,925.88\nSalary Low: $67,500.00\nSalary High: $2,460,000.00"
  },
  {
    "objectID": "posts/RegressionBlog/regression.html#linear-support-vector-regression-kernellinear",
    "href": "posts/RegressionBlog/regression.html#linear-support-vector-regression-kernellinear",
    "title": "Salary Prediction Using Regression on MLB Data",
    "section": "Linear Support Vector Regression (kernel=“linear”)",
    "text": "Linear Support Vector Regression (kernel=“linear”)\n\nsc = StandardScaler()\n\nX_train_scaled = sc.fit_transform(X_train, y_train)\nX_test_scaled = sc.fit_transform(X_test, y_test)\n# pca_all = PCA(n_components=1)\n# X_train_scaled_pca = pca_all.fit_transform(X_train_scaled)\n# X_test_scaled_pca = pca_all.fit_transform(X_test_scaled)\n\nsvr_lin = SVR(kernel=\"linear\", C=1, gamma=\"auto\")\nsvr_lin.fit(X_train_scaled, y_train)\ny_pred = svr_lin.predict(X_test_scaled)\n\n\nVisualize Multiple Linear Regression\n\nplt.figure(figsize=(16, 12))\nindependent_variables = X_train.columns\ndependent_variable = \"Salary\"\nX_test_numpy = X_test.to_numpy()\nfor i, col in enumerate(independent_variables, 1):\n    plt.subplot(3, 3, i)\n    sns.regplot(x=X_train[col],y=y_train,ci=None,color ='red')\n    sns.scatterplot(data=X_train, x=col, y=y_train, color='blue', label='Training Points')\n    sns.scatterplot(data=X_test, x=col, y=y_test, color='green', label='Testing Points')\n    sns.scatterplot(data=X_test, x=col, y=y_pred, color='red', label='Predicted Points')\n    plt.title(f'{dependent_variable} vs. {col}')\n    plt.xlabel(col)\n    plt.ylabel(dependent_variable)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nresult_df = pd.DataFrame(columns=[\"id\", \"actual\", \"predicted\"])\n\nfor i, actual, predicted in zip(y_test.index, y_test, y_pred):\n    entry = [i, actual, predicted]\n    df_entry = pd.DataFrame(entry, index=[\"id\", \"actual\", \"predicted\"]).T\n    result_df = pd.concat((result_df, df_entry))\n#print(result_df)\ndifference = abs(result_df[\"actual\"] - result_df[\"predicted\"])\nprint(f\"Cumulative Difference: ${np.sum(difference) * 1000:,.2f}\")\nprint(f\"Min Difference: ${np.min(difference) * 1000:,.2f}\")\nprint(f\"Max Difference: ${np.max(difference) * 1000:,.2f}\")\nprint(f\"Average Difference: ${np.mean(difference) * 1000:,.2f}\")\nprint(f\"Std Difference: ${np.std(difference) * 1000:,.2f}\")\nprint(f\"Mean Squared Error: ${mean_squared_error(y_test, y_pred):,.2f}\")\n\nCumulative Difference: $10,792,084.13\nMin Difference: $19,061.95\nMax Difference: $1,968,301.30\nAverage Difference: $269,802.10\nStd Difference: $361,894.28\nMean Squared Error: $203,760.65\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nplt.figure(figsize=(8,8))\nresiduals = result_df['actual'] - result_df['predicted']\n\nplt.scatter(result_df['id'], residuals)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('ID')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.show()"
  },
  {
    "objectID": "posts/RegressionBlog/regression.html#non-linear-support-vector-regression-kernelpoly",
    "href": "posts/RegressionBlog/regression.html#non-linear-support-vector-regression-kernelpoly",
    "title": "Salary Prediction Using Regression on MLB Data",
    "section": "Non-Linear Support Vector Regression (kernel=“poly”)",
    "text": "Non-Linear Support Vector Regression (kernel=“poly”)\n\nsvr_poly = SVR(kernel=\"poly\", degree=2, C=75, gamma=\"scale\")\nsvr_poly.fit(X_train_scaled, y_train)\ny_pred = svr_poly.predict(X_test_scaled)\n\nresult_df = pd.DataFrame(columns=[\"id\", \"actual\", \"predicted\"])\n\nfor i, actual, predicted in zip(y_test.index, y_test, y_pred):\n    entry = [i, actual, predicted]\n    df_entry = pd.DataFrame(entry, index=[\"id\", \"actual\", \"predicted\"]).T\n    result_df = pd.concat((result_df, df_entry))\n    \n#print(result_df)\ndifference = abs(result_df[\"actual\"] - result_df[\"predicted\"])\nprint(f\"Cumulative Difference: ${np.sum(difference) * 1000:,.2f}\")\nprint(f\"Min Difference: ${np.min(difference) * 1000:,.2f}\")\nprint(f\"Max Difference: ${np.max(difference) * 1000:,.2f}\")\nprint(f\"Average Difference: ${np.mean(difference) * 1000:,.2f}\")\nprint(f\"Std Difference: ${np.std(difference) * 1000:,.2f}\")\nprint(f\"Mean Squared Error: ${mean_squared_error(y_test, y_pred):,.2f}\")\n\nCumulative Difference: $13,385,640.30\nMin Difference: $798.03\nMax Difference: $1,581,505.51\nAverage Difference: $334,641.01\nStd Difference: $330,986.90\nMean Squared Error: $221,536.94\n\n\n\nplt.figure(figsize=(16, 12))\nindependent_variables = X_train.columns\ndependent_variable = \"Salary\"\nX_test_numpy = X_test.to_numpy()\nfor i, col in enumerate(independent_variables, 1):\n    plt.subplot(3, 3, i)\n    sns.regplot(x=X_train[col],y=y_train,ci=None,color ='red', order=svr_poly.degree)\n    sns.scatterplot(data=X_train, x=col, y=y_train, color='blue', label='Training Points')\n    sns.scatterplot(data=X_test, x=col, y=y_test, color='green', label='Testing Points')\n    sns.scatterplot(data=X_test, x=col, y=y_pred, color='red', label='Predicted Points')\n    plt.title(f'{dependent_variable} vs. {col}')\n    plt.xlabel(col)\n    plt.ylabel(dependent_variable)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nplt.figure(figsize=(8,8))\nresiduals = result_df['actual'] - result_df['predicted']\n\nplt.scatter(result_df['id'], residuals)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('ID')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.show()"
  },
  {
    "objectID": "notebooks/spotify.html",
    "href": "notebooks/spotify.html",
    "title": "Spotify Recommendation System",
    "section": "",
    "text": "Author: Daniel Hassler\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.model_selection import train_test_split, ParameterGrid\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score"
  },
  {
    "objectID": "notebooks/spotify.html#data-analysis",
    "href": "notebooks/spotify.html#data-analysis",
    "title": "Spotify Recommendation System",
    "section": "Data Analysis",
    "text": "Data Analysis\nLook at -&gt; * https://github.com/ageron/handson-ml/blob/master/08_dimensionality_reduction.ipynb * https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset/code\n\noriginal_df = pd.read_csv(\"./dataset-dedup.csv\")\nprint(original_df.columns)\n# original_df = original_df.drop_duplicates(subset=[\"artists\", \"track_name\"], keep=\"first\").reset_index()\nprint(original_df.shape)\n# original_df.to_csv(\"./dataset-dedup.csv\")\nfeatures_x = [\"loudness\", \"popularity\", \"duration_ms\"]\nfeatures_y = [\"popularity\", \"energy\", \"tempo\"]\n\nfor i, (x,y) in enumerate(zip(features_x, features_y)):\n    scatter = sns.scatterplot(x=x, y=y, hue='track_genre', data=original_df, palette=\"viridis\", alpha=0.25)\n    legend_labels = original_df['track_genre'].unique()# [:3]  # Show only the first 3 genres\n    scatter.legend(title='Genre', labels=legend_labels, prop={'size': 1})\n    plt.title(f\"Scatter Plot of {x} vs {y} by genre\")\n    plt.show()\n\nplt.show()\n\nIndex(['Unnamed: 0.1', 'index', 'Unnamed: 0', 'track_id', 'artists',\n       'album_name', 'track_name', 'popularity', 'duration_ms', 'explicit',\n       'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n       'time_signature', 'track_genre'],\n      dtype='object')\n(81344, 23)\n\n\n\n\n\n\n\n\n\n\n\n\nunique_vals = original_df['track_genre'].unique()\nplt.bar(unique_vals, original_df['track_genre'].value_counts())\nplt.title(\"Occurrence of Genre\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Genre\")\n_ = plt.xticks(rotation=\"vertical\", fontsize=4)"
  },
  {
    "objectID": "notebooks/spotify.html#k-means-clustering",
    "href": "notebooks/spotify.html#k-means-clustering",
    "title": "Spotify Recommendation System",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\n\nHyperparameter Tuning for K-Means\n\ninertia = []\n# train_df is the numeric representation of original_df\ntrain_df = original_df.drop(columns=['Unnamed: 0.1', 'index', 'Unnamed: 0', 'track_id',\n       'album_name', 'track_name', 'popularity', 'duration_ms', 'explicit',\n       'danceability', 'energy', 'loudness', 'mode', 'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n       'time_signature'])\n\nfor col in train_df.columns:\n    if not pd.api.types.is_numeric_dtype(train_df[col]):\n        train_df[col] = pd.factorize(original_df[col])[0]\n\nscaler = StandardScaler()\n# df_scaled is the scaled version of train_df\ndf_scaled = scaler.fit_transform(train_df)\npca_num_components = 2\n\n# df_pca to reduce dimensionality\npca = PCA(n_components=pca_num_components).fit_transform(df_scaled)\ndf_pca = pd.DataFrame(pca,columns=['pca1','pca2'])\n\nfor k in range(1, 80, 10):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit_predict(df_pca)\n    inertia.append(kmeans.inertia_)\n\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\nPlotting the Elbow Chart\n\n# Plot the elbow curve\nplt.plot(range(1, 80, 10), inertia, marker='o')\nplt.title('Elbow Method for Optimal K')\nplt.xlabel('Number of Clusters (K)')\nplt.ylabel('Inertia (Within-Cluster Sum of Squares)')\nplt.show()\n\n\n\n\n\n\nChoosing K-Means with “Elbow” Point\n\nkmeans = KMeans(n_clusters=10, random_state=42)\noriginal_df['clusters'] = kmeans.fit_predict(df_pca)\n\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\nsns.scatterplot(x=\"pca1\", y=\"pca2\", hue=original_df['clusters'], data=df_pca)\nplt.title('K-means Clustering PCA of 3 features [track_genre, artists, key]')\nplt.show()"
  },
  {
    "objectID": "notebooks/spotify.html#evaluation",
    "href": "notebooks/spotify.html#evaluation",
    "title": "Spotify Recommendation System",
    "section": "Evaluation",
    "text": "Evaluation\n\noriginal_df['Distance_to_Centroid'] = kmeans.transform(df_pca).min(axis=1)\n\n\ndef get_nearest_entry(idx, k=5):\n    # print(original_df.iloc[idx])\n    # print(train_df.iloc[idx])\n    cluster = kmeans.predict(df_pca.iloc[idx].to_frame().T)[0]\n    cluster_data = original_df[original_df[\"clusters\"] == cluster]\n    cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\n    cluster_data = cluster_data.sort_values(by=\"closest_entries_to_idx\")\n    # print(cluster_data[[\"artists\", \"album_name\", \"track_name\", \"track_genre\"]])\n\n    cluster_data.drop(columns=[\"closest_entries_to_idx\"])\n    print(f\"Top {k} Closest Examples to {cluster_data.loc[idx]['artists']}'s \\\"{cluster_data.loc[idx]['track_name']}\\\"\")\n    print(cluster_data[:k][[\"artists\", \"track_name\", \"track_genre\"]])\n    print(\"\\n\\n\")\n\nget_nearest_entry(35640) # rock song\nget_nearest_entry(16587) # country song\nget_nearest_entry(41220) # rap song\n\nTop 5 Closest Examples to Daughtry's \"September\"\n            artists               track_name     track_genre\n35640      Daughtry                September          grunge\n35381      Daughtry            It's Not Over          grunge\n35839    Stone Sour                 Hesitate          grunge\n55666    Mark Broom                Five/Four  minimal-techno\n40063  TNT;POPR3B3L  I'm Raving - Radio Edit       hardstyle\n\n\n\nTop 5 Closest Examples to Florida Georgia Line's \"Stay\"\n                    artists  \\\n16587  Florida Georgia Line   \n8395    Datsik;Virtual Riot   \n8582            The Prodigy   \n8819            The Prodigy   \n8529            The Prodigy   \n\n                                              track_name track_genre  \n16587                                               Stay     country  \n8395                                               Nasty   breakbeat  \n8582                                               Girls   breakbeat  \n8819                                  We Are The Ruffest   breakbeat  \n8529   Out of Space - Techno Underworld Remix Remastered   breakbeat  \n\n\n\nTop 5 Closest Examples to Future;Lil Uzi Vert's \"Tic Tac\"\n                                          artists              track_name  \\\n41220                         Future;Lil Uzi Vert                 Tic Tac   \n43994  Pritam;Arijit Singh;Shadab;Altamash Faridi  Lambiyaan Si Judaiyaan   \n41226                                    Lil Baby                  All In   \n43981     Pritam;Sukhwinder Singh;Sunidhi Chauhan                Marjaani   \n41207                    Zack Knight;Jasmin Walia         Bom Diggy Diggy   \n\n      track_genre  \n41220     hip-hop  \n43994      indian  \n41226     hip-hop  \n43981      indian  \n41207     hip-hop  \n\n\n\n\n\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_7112\\101083667.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_7112\\101083667.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_7112\\101083667.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\n\n\n\noriginal_df = pd.read_csv(\"./dataset-dedup.csv\")\n# train_df is the numeric representation of original_df\ntrain_df = original_df.drop(columns=['Unnamed: 0.1', 'index', 'Unnamed: 0', 'track_id',\n       'album_name', 'track_name', 'popularity', 'duration_ms', 'explicit',\n       'danceability', 'key', 'energy', 'loudness', 'mode', 'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n       'time_signature'])\n\nfor col in train_df.columns:\n    if not pd.api.types.is_numeric_dtype(train_df[col]):\n        train_df[col] = pd.factorize(original_df[col])[0]\n\nscaler = StandardScaler()\n# df_scaled is the scaled version of train_df\ndf_scaled = scaler.fit_transform(train_df)\npca_num_components = 2\n\n# df_pca to reduce dimensionality\npca = PCA(n_components=pca_num_components).fit_transform(df_scaled)\ndf_pca = pd.DataFrame(pca,columns=['pca1','pca2'])\n\nkmeans = KMeans(n_clusters=10, random_state=42)\noriginal_df['clusters'] = kmeans.fit_predict(df_pca)\n\nsns.scatterplot(x=\"pca1\", y=\"pca2\", hue=original_df['clusters'], data=df_pca)\nplt.title('K-means Clustering PCA of 2 features [track_genre, artists]')\nplt.show()\noriginal_df['Distance_to_Centroid'] = kmeans.transform(df_pca).min(axis=1)\nget_nearest_entry(35640) # rock song\nget_nearest_entry(16587) # country song\nget_nearest_entry(41220) # rap song\n\nd:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_7112\\101083667.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_7112\\101083667.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_7112\\101083667.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\n\n\n\n\n\nTop 5 Closest Examples to Daughtry's \"September\"\n        artists            track_name track_genre\n35439  Daughtry  Waiting for Superman      grunge\n35678  Daughtry         Gone Too Soon      grunge\n35802  Daughtry            I'll Fight      grunge\n35640  Daughtry             September      grunge\n35336  Daughtry                  Home      grunge\n\n\n\nTop 5 Closest Examples to Florida Georgia Line's \"Stay\"\n                    artists         track_name track_genre\n16592  Florida Georgia Line  I Love My Country     country\n16587  Florida Georgia Line               Stay     country\n16938  Florida Georgia Line           H.O.L.Y.     country\n16598  Florida Georgia Line           Sun Daze     country\n16975  Florida Georgia Line               Life     country\n\n\n\nTop 5 Closest Examples to Future;Lil Uzi Vert's \"Tic Tac\"\n                   artists              track_name track_genre\n41220  Future;Lil Uzi Vert                 Tic Tac     hip-hop\n39123            Lionheart                  Cursed    hardcore\n39035            Lionheart                LHHC '17    hardcore\n39040              Bodyjar  A Hazy Shade of Winter    hardcore\n39033         Naked Raygun              Rat Patrol    hardcore"
  },
  {
    "objectID": "notebooks/NaiveBayesNotebook.html",
    "href": "notebooks/NaiveBayesNotebook.html",
    "title": "Probability Theory with Naive Bayes",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport mnist\n\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn.metrics import confusion_matrix"
  },
  {
    "objectID": "notebooks/NaiveBayesNotebook.html#mnist-data",
    "href": "notebooks/NaiveBayesNotebook.html#mnist-data",
    "title": "Probability Theory with Naive Bayes",
    "section": "MNIST Data",
    "text": "MNIST Data\n\n# mnist.init()\nX_train, y_train, X_test, y_test = mnist.load()\nprint(\"X_train len: \", len(X_train))\nprint(\"X_test len: \", len(X_test))\nprint(\"X_train shape: \", X_train.shape)\nprint(\"X_test shape: \", X_test.shape)\n\nplt.subplot(1, 2, 1)\nplt.title(\"Entry in Train Set\")\nimg = X_train[0,:].reshape(28,28) # First image in the training set.\nplt.imshow(img,cmap='gray')\n\nplt.subplot(1, 2, 2)\nplt.title(\"Entry in Test Set\")\nimg = X_test[0,:].reshape(28,28) # First image in the training set.\nplt.imshow(img,cmap='gray')\nplt.show() # Show the image\n\nX_train len:  60000\nX_test len:  10000\nX_train shape:  (60000, 784)\nX_test shape:  (10000, 784)\n\n\n\n\n\n\nVisualize Every Number\n\nunique_values, indices = np.unique(y_train, return_index=True)\nprint(unique_values, indices)\n\nfor i, label_index in enumerate(indices):\n    plt.subplot(1, len(indices), i + 1)\n    img = X_train[label_index,:].reshape(28,28)\n    plt.imshow(img,cmap='gray')\nplt.show()\n    \n\n[0 1 2 3 4 5 6 7 8 9] [ 1  3  5  7  2  0 13 15 17  4]"
  },
  {
    "objectID": "notebooks/NaiveBayesNotebook.html#naive-bayes-model",
    "href": "notebooks/NaiveBayesNotebook.html#naive-bayes-model",
    "title": "Probability Theory with Naive Bayes",
    "section": "Naive Bayes Model",
    "text": "Naive Bayes Model\n\nunique, counts = np.unique(y_train, return_counts=True)\nsum_counts = np.sum(counts)\npriors = np.divide(counts, sum_counts)\n\nnb = GaussianNB(priors=priors, var_smoothing=0.1)\nnb.fit(X_train, y_train)\ny_pred = nb.predict(X_test)\nprint(y_pred)\nprint(y_test)\n\n# print(\"Priors: \", nb.priors)\n\n[7 2 1 ... 9 8 6]\n[7 2 1 ... 4 5 6]\n\n\n\nAccuracy Results\n\nconfusion = confusion_matrix(y_test, y_pred)\nconf_df = pd.DataFrame(confusion, index=[f\"{i}\" for i in range(10)], columns=[f\"{i}\" for i in range(10)])\nheatmap = sns.heatmap(conf_df, annot=True, fmt=\"d\", linewidths=0.35, cmap=\"YlGnBu\")\nplt.title(f\"Model Predictions With {(np.sum(confusion.diagonal()) / y_test.shape[0]) * 100:.2f}% Accuracy\")\n\nText(0.5, 1.0, 'Model Predictions With 81.40% Accuracy')\n\n\n\n\n\n\nmean_pixel_values = nb.theta_\nplt.figure(figsize=(5,10))\nfor i, digit in enumerate(range(10)):\n    plt.subplot(len(indices) // 2, 2, i + 1)\n    plt.title(f\"Digit {digit}\")\n    plt.axis('off')\n    img = mean_pixel_values[digit].reshape(28,28)\n    plt.imshow(img)\nplt.plot()\n\n[]"
  },
  {
    "objectID": "notebooks/anomaly.html",
    "href": "notebooks/anomaly.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Author: Daniel Hassler\nfrom sklearn.datasets import load_iris\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.model_selection import ParameterGrid\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "notebooks/anomaly.html#load-iris-data",
    "href": "notebooks/anomaly.html#load-iris-data",
    "title": "Anomaly Detection",
    "section": "Load Iris Data",
    "text": "Load Iris Data\nFor the purpose of anomaly detection and being able to visualize anomalies/outliers, I ran PCA (principal component analysis) to reduce the dimensionality of the iris dataset (without targets) from shape (150, 4) to (150, 2). This allows me to visualize this dataset in two dimensions and makes clustering more efficient and representative of the true data.\n\niris = load_iris()\npca = PCA(n_components=2)\ndata_pca = pca.fit_transform(iris.data, iris.target)\ndf = pd.DataFrame(data_pca, columns=[\"PC1\", \"PC2\"])\ndf['target'] = iris.target\nprint(df)\n\n          PC1       PC2  target\n0   -2.684126  0.319397       0\n1   -2.714142 -0.177001       0\n2   -2.888991 -0.144949       0\n3   -2.745343 -0.318299       0\n4   -2.728717  0.326755       0\n..        ...       ...     ...\n145  1.944110  0.187532       2\n146  1.527167 -0.375317       2\n147  1.764346  0.078859       2\n148  1.900942  0.116628       2\n149  1.390189 -0.282661       2\n\n[150 rows x 3 columns]"
  },
  {
    "objectID": "notebooks/anomaly.html#plot-the-current-iris-data",
    "href": "notebooks/anomaly.html#plot-the-current-iris-data",
    "title": "Anomaly Detection",
    "section": "Plot the Current Iris Data",
    "text": "Plot the Current Iris Data\n\n_, ax = plt.subplots()\nscatter = ax.scatter(df[\"PC1\"], df[\"PC2\"], c=iris.target, cmap=\"copper\")\nax.set(xlabel=\"PC1\", ylabel=\"PC2\")\n_ = ax.legend(\n    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n)"
  },
  {
    "objectID": "notebooks/anomaly.html#initialize-the-dbscan",
    "href": "notebooks/anomaly.html#initialize-the-dbscan",
    "title": "Anomaly Detection",
    "section": "Initialize the DBSCAN",
    "text": "Initialize the DBSCAN\nIn order to create realistic clusters with DBSCAN and maximize an optimization, we need to preform hyperparameter tuning. Below is a GridSearch implementation for tweaking and finding the best eps and min_samples hyperparameters.\n\nparams = {\n    'eps': [i / 10 for i in range(1, 15)],\n    'min_samples': [i for i in range(1, 10)]\n}\n\nbest_score = -1\nbest_params = {}\n\nfor param_i in ParameterGrid(params):\n    db = DBSCAN(**param_i)\n    labels = db.fit_predict(data_pca)\n    # minimum of 4 clusters (3 classes + 1 outlier)\n    if len(np.unique(labels)) &lt;= 3:\n        continue\n\n    score = silhouette_score(data_pca, labels)\n    if score &gt; best_score:\n        best_score = score\n        best_params = param_i\n\nprint(\"Best Score: \", best_score)\nprint(\"Best Params: \", best_params)\n\n\n\nBest Score:  0.45189188489361554\nBest Params:  {'eps': 0.3, 'min_samples': 6}\n\n\n\ndb = DBSCAN(**best_params).fit(data_pca)\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint(\"Estimated number of clusters: %d\" % n_clusters_)\nprint(\"Estimated number of noise points: %d\" % n_noise_)\n\nEstimated number of clusters: 3\nEstimated number of noise points: 20"
  },
  {
    "objectID": "notebooks/anomaly.html#plot-the-dbscan",
    "href": "notebooks/anomaly.html#plot-the-dbscan",
    "title": "Anomaly Detection",
    "section": "Plot the DBSCAN",
    "text": "Plot the DBSCAN\n\ny_pred = db.fit_predict(data_pca)\nfig, ax = plt.subplots()\nscatter = ax.scatter(df[\"PC1\"], df[\"PC2\"], c=y_pred, cmap='copper')\ntext_labels = [\"outlier\", \"setosa\", \"versicolor\", \"virginica\"]\nlegend1 = ax.legend(scatter.legend_elements()[0], text_labels,\n                    loc=\"lower right\", title=\"Classes\")\nax.add_artist(legend1)\nplt.title(\"DBSCAN of Iris Data\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\n\nText(0, 0.5, 'PC2')"
  },
  {
    "objectID": "notebooks/DecisionTreeNotebook.html",
    "href": "notebooks/DecisionTreeNotebook.html",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "",
    "text": "import numpy as np\nimport sklearn\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, LearningCurveDisplay\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn import tree\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "notebooks/DecisionTreeNotebook.html#imports",
    "href": "notebooks/DecisionTreeNotebook.html#imports",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "",
    "text": "import numpy as np\nimport sklearn\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, LearningCurveDisplay\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn import tree\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "notebooks/DecisionTreeNotebook.html#class-distribution",
    "href": "notebooks/DecisionTreeNotebook.html#class-distribution",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Class Distribution",
    "text": "Class Distribution\n\n# Class imbalance, more obesity.\nunique_values, counts = np.unique(y, return_counts=True)\nplt.bar(unique_values, counts)\nplt.title(\"BMI Classes in the Entire Dataset\")\nplt.xlabel(\"BMI Class\")\nplt.ylabel(\"Occurences in Entire Dataset\")\nplt.show()"
  },
  {
    "objectID": "notebooks/DecisionTreeNotebook.html#feature-correlation",
    "href": "notebooks/DecisionTreeNotebook.html#feature-correlation",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Feature Correlation",
    "text": "Feature Correlation\n\n\ncorrelation_matrix = data.corr()\n\n# Display a heatmap of the correlation matrix\nplt.figure(figsize=(10, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('Correlation Heatmap')\nplt.show()\n\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_33884\\1179218035.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  correlation_matrix = data.corr()"
  },
  {
    "objectID": "notebooks/DecisionTreeNotebook.html#scatter-plot-of-data",
    "href": "notebooks/DecisionTreeNotebook.html#scatter-plot-of-data",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Scatter Plot of Data",
    "text": "Scatter Plot of Data\n\nplt.figure(figsize=(6, 6))\nsns.scatterplot(data=data, x='Height', y='Weight', hue='Index', palette='deep')\nplt.title('Scatter Plot of Height vs Weight')\nplt.show()"
  },
  {
    "objectID": "notebooks/DecisionTreeNotebook.html#distribution-of-features",
    "href": "notebooks/DecisionTreeNotebook.html#distribution-of-features",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Distribution of Features",
    "text": "Distribution of Features\n\n# Compare height and weight between male and female genders using box plots\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=data, x='Gender', y='Height')\nplt.title('Comparison of Height between Genders')\nplt.xlabel('Gender')\nplt.ylabel('Height')\nplt.show()\n\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=data, x='Gender', y='Weight')\nplt.title('Comparison of Weight between Genders')\nplt.xlabel('Gender')\nplt.ylabel('Weight')\nplt.show()"
  },
  {
    "objectID": "notebooks/DecisionTreeNotebook.html#hyperparameter-tuning-dt",
    "href": "notebooks/DecisionTreeNotebook.html#hyperparameter-tuning-dt",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Hyperparameter Tuning DT",
    "text": "Hyperparameter Tuning DT\n\nparam_grid = {\n    'max_depth': [i for i in range(2, 9)],\n    'min_samples_leaf': [2 ** i for i in range(0, 4)],\n    'criterion': [\"entropy\", \"gini\"]\n}\n\ndt = DecisionTreeClassifier(random_state=42)\n\ngrid_search_dt = GridSearchCV(dt, param_grid, cv=StratifiedKFold(n_splits=3), scoring='accuracy')\ngrid_search_dt.fit(X_train, y_train)\nbest_params_dt = grid_search_dt.best_params_\nprint(\"Best Hyperparameters:\", best_params_dt)\nprint(\"Best Score:\", grid_search_dt.best_score_)\n\nBest Hyperparameters: {'criterion': 'entropy', 'max_depth': 8, 'min_samples_leaf': 1}\nBest Score: 0.8281314289073061"
  },
  {
    "objectID": "notebooks/DecisionTreeNotebook.html#decision-tree-classifier",
    "href": "notebooks/DecisionTreeNotebook.html#decision-tree-classifier",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Decision Tree Classifier",
    "text": "Decision Tree Classifier\n\ndt = DecisionTreeClassifier(max_depth=best_params_dt[\"max_depth\"], min_samples_leaf=best_params_dt[\"min_samples_leaf\"], criterion=best_params_dt[\"criterion\"])\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)"
  },
  {
    "objectID": "notebooks/DecisionTreeNotebook.html#decision-tree-visualization",
    "href": "notebooks/DecisionTreeNotebook.html#decision-tree-visualization",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Decision Tree Visualization",
    "text": "Decision Tree Visualization\n\nplt.figure(figsize=(30, 20))\nplot_tree(dt, feature_names=X_train.columns.tolist(), class_names=['0', '1', '2','3','4','5'], filled=True)\n\n[Text(0.5918141592920354, 0.9444444444444444, 'Weight &lt;= 94.5\\nentropy = 2.189\\nsamples = 320\\nvalue = [11, 17, 39, 47, 76, 130]\\nclass = 5'),\n Text(0.3915929203539823, 0.8333333333333334, 'Weight &lt;= 69.5\\nentropy = 2.381\\nsamples = 126\\nvalue = [11, 17, 39, 31, 21, 7]\\nclass = 2'),\n Text(0.23893805309734514, 0.7222222222222222, 'Height &lt;= 167.5\\nentropy = 2.037\\nsamples = 56\\nvalue = [11, 17, 18, 9, 1, 0]\\nclass = 2'),\n Text(0.1415929203539823, 0.6111111111111112, 'Height &lt;= 152.5\\nentropy = 1.371\\nsamples = 25\\nvalue = [1, 0, 14, 9, 1, 0]\\nclass = 2'),\n Text(0.07079646017699115, 0.5, 'Weight &lt;= 57.5\\nentropy = 1.189\\nsamples = 12\\nvalue = [0, 0, 3, 8, 1, 0]\\nclass = 3'),\n Text(0.035398230088495575, 0.3888888888888889, 'Height &lt;= 144.0\\nentropy = 0.811\\nsamples = 4\\nvalue = [0, 0, 3, 1, 0, 0]\\nclass = 2'),\n Text(0.017699115044247787, 0.2777777777777778, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 1, 0, 0]\\nclass = 3'),\n Text(0.05309734513274336, 0.2777777777777778, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 0, 3, 0, 0, 0]\\nclass = 2'),\n Text(0.10619469026548672, 0.3888888888888889, 'Weight &lt;= 68.0\\nentropy = 0.544\\nsamples = 8\\nvalue = [0, 0, 0, 7, 1, 0]\\nclass = 3'),\n Text(0.08849557522123894, 0.2777777777777778, 'entropy = 0.0\\nsamples = 7\\nvalue = [0, 0, 0, 7, 0, 0]\\nclass = 3'),\n Text(0.12389380530973451, 0.2777777777777778, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0]\\nclass = 4'),\n Text(0.21238938053097345, 0.5, 'Weight &lt;= 66.0\\nentropy = 0.773\\nsamples = 13\\nvalue = [1, 0, 11, 1, 0, 0]\\nclass = 2'),\n Text(0.17699115044247787, 0.3888888888888889, 'Height &lt;= 162.5\\nentropy = 0.439\\nsamples = 11\\nvalue = [1, 0, 10, 0, 0, 0]\\nclass = 2'),\n Text(0.1592920353982301, 0.2777777777777778, 'entropy = 0.0\\nsamples = 7\\nvalue = [0, 0, 7, 0, 0, 0]\\nclass = 2'),\n Text(0.19469026548672566, 0.2777777777777778, 'Weight &lt;= 57.5\\nentropy = 0.811\\nsamples = 4\\nvalue = [1, 0, 3, 0, 0, 0]\\nclass = 2'),\n Text(0.17699115044247787, 0.16666666666666666, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0, 0, 0, 0, 0]\\nclass = 0'),\n Text(0.21238938053097345, 0.16666666666666666, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 0, 3, 0, 0, 0]\\nclass = 2'),\n Text(0.24778761061946902, 0.3888888888888889, 'Height &lt;= 163.5\\nentropy = 1.0\\nsamples = 2\\nvalue = [0, 0, 1, 1, 0, 0]\\nclass = 2'),\n Text(0.23008849557522124, 0.2777777777777778, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 1, 0, 0]\\nclass = 3'),\n Text(0.26548672566371684, 0.2777777777777778, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 1, 0, 0, 0]\\nclass = 2'),\n Text(0.336283185840708, 0.6111111111111112, 'Weight &lt;= 55.0\\nentropy = 1.383\\nsamples = 31\\nvalue = [10, 17, 4, 0, 0, 0]\\nclass = 1'),\n Text(0.3008849557522124, 0.5, 'Height &lt;= 180.0\\nentropy = 0.918\\nsamples = 15\\nvalue = [10, 5, 0, 0, 0, 0]\\nclass = 0'),\n Text(0.2831858407079646, 0.3888888888888889, 'entropy = 0.0\\nsamples = 5\\nvalue = [0, 5, 0, 0, 0, 0]\\nclass = 1'),\n Text(0.3185840707964602, 0.3888888888888889, 'entropy = 0.0\\nsamples = 10\\nvalue = [10, 0, 0, 0, 0, 0]\\nclass = 0'),\n Text(0.37168141592920356, 0.5, 'Height &lt;= 182.0\\nentropy = 0.811\\nsamples = 16\\nvalue = [0, 12, 4, 0, 0, 0]\\nclass = 1'),\n Text(0.35398230088495575, 0.3888888888888889, 'Weight &lt;= 59.5\\nentropy = 0.985\\nsamples = 7\\nvalue = [0, 3, 4, 0, 0, 0]\\nclass = 2'),\n Text(0.336283185840708, 0.2777777777777778, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 3, 0, 0, 0, 0]\\nclass = 1'),\n Text(0.37168141592920356, 0.2777777777777778, 'entropy = 0.0\\nsamples = 4\\nvalue = [0, 0, 4, 0, 0, 0]\\nclass = 2'),\n Text(0.3893805309734513, 0.3888888888888889, 'entropy = 0.0\\nsamples = 9\\nvalue = [0, 9, 0, 0, 0, 0]\\nclass = 1'),\n Text(0.5442477876106194, 0.7222222222222222, 'Height &lt;= 168.5\\nentropy = 1.894\\nsamples = 70\\nvalue = [0, 0, 21, 22, 20, 7]\\nclass = 3'),\n Text(0.4778761061946903, 0.6111111111111112, 'Height &lt;= 148.5\\nentropy = 1.408\\nsamples = 33\\nvalue = [0, 0, 0, 7, 19, 7]\\nclass = 4'),\n Text(0.4424778761061947, 0.5, 'Weight &lt;= 84.5\\nentropy = 0.996\\nsamples = 13\\nvalue = [0, 0, 0, 0, 6, 7]\\nclass = 5'),\n Text(0.4247787610619469, 0.3888888888888889, 'Height &lt;= 141.5\\nentropy = 0.592\\nsamples = 7\\nvalue = [0, 0, 0, 0, 6, 1]\\nclass = 4'),\n Text(0.40707964601769914, 0.2777777777777778, 'Height &lt;= 140.5\\nentropy = 1.0\\nsamples = 2\\nvalue = [0, 0, 0, 0, 1, 1]\\nclass = 4'),\n Text(0.3893805309734513, 0.16666666666666666, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0]\\nclass = 4'),\n Text(0.4247787610619469, 0.16666666666666666, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 1]\\nclass = 5'),\n Text(0.4424778761061947, 0.2777777777777778, 'entropy = 0.0\\nsamples = 5\\nvalue = [0, 0, 0, 0, 5, 0]\\nclass = 4'),\n Text(0.46017699115044247, 0.3888888888888889, 'entropy = 0.0\\nsamples = 6\\nvalue = [0, 0, 0, 0, 0, 6]\\nclass = 5'),\n Text(0.5132743362831859, 0.5, 'Weight &lt;= 80.5\\nentropy = 0.934\\nsamples = 20\\nvalue = [0, 0, 0, 7, 13, 0]\\nclass = 4'),\n Text(0.49557522123893805, 0.3888888888888889, 'Height &lt;= 159.0\\nentropy = 0.946\\nsamples = 11\\nvalue = [0, 0, 0, 7, 4, 0]\\nclass = 3'),\n Text(0.4778761061946903, 0.2777777777777778, 'Weight &lt;= 72.0\\nentropy = 0.722\\nsamples = 5\\nvalue = [0, 0, 0, 1, 4, 0]\\nclass = 4'),\n Text(0.46017699115044247, 0.16666666666666666, 'Gender_Encoded &lt;= 0.5\\nentropy = 1.0\\nsamples = 2\\nvalue = [0, 0, 0, 1, 1, 0]\\nclass = 3'),\n Text(0.4424778761061947, 0.05555555555555555, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 1, 0, 0]\\nclass = 3'),\n Text(0.4778761061946903, 0.05555555555555555, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0]\\nclass = 4'),\n Text(0.49557522123893805, 0.16666666666666666, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 0, 0, 0, 3, 0]\\nclass = 4'),\n Text(0.5132743362831859, 0.2777777777777778, 'entropy = 0.0\\nsamples = 6\\nvalue = [0, 0, 0, 6, 0, 0]\\nclass = 3'),\n Text(0.5309734513274337, 0.3888888888888889, 'entropy = 0.0\\nsamples = 9\\nvalue = [0, 0, 0, 0, 9, 0]\\nclass = 4'),\n Text(0.6106194690265486, 0.6111111111111112, 'Weight &lt;= 80.5\\nentropy = 1.133\\nsamples = 37\\nvalue = [0, 0, 21, 15, 1, 0]\\nclass = 2'),\n Text(0.5929203539823009, 0.5, 'entropy = 0.0\\nsamples = 11\\nvalue = [0, 0, 11, 0, 0, 0]\\nclass = 2'),\n Text(0.6283185840707964, 0.5, 'Height &lt;= 188.5\\nentropy = 1.169\\nsamples = 26\\nvalue = [0, 0, 10, 15, 1, 0]\\nclass = 3'),\n Text(0.6106194690265486, 0.3888888888888889, 'Weight &lt;= 89.0\\nentropy = 1.049\\nsamples = 21\\nvalue = [0, 0, 5, 15, 1, 0]\\nclass = 3'),\n Text(0.5663716814159292, 0.2777777777777778, 'Height &lt;= 176.0\\nentropy = 0.98\\nsamples = 12\\nvalue = [0, 0, 5, 7, 0, 0]\\nclass = 3'),\n Text(0.5309734513274337, 0.16666666666666666, 'Gender_Encoded &lt;= 0.5\\nentropy = 0.811\\nsamples = 4\\nvalue = [0, 0, 3, 1, 0, 0]\\nclass = 2'),\n Text(0.5132743362831859, 0.05555555555555555, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 1, 0, 0]\\nclass = 3'),\n Text(0.5486725663716814, 0.05555555555555555, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 0, 3, 0, 0, 0]\\nclass = 2'),\n Text(0.6017699115044248, 0.16666666666666666, 'Height &lt;= 183.0\\nentropy = 0.811\\nsamples = 8\\nvalue = [0, 0, 2, 6, 0, 0]\\nclass = 3'),\n Text(0.584070796460177, 0.05555555555555555, 'entropy = 0.0\\nsamples = 5\\nvalue = [0, 0, 0, 5, 0, 0]\\nclass = 3'),\n Text(0.6194690265486725, 0.05555555555555555, 'entropy = 0.918\\nsamples = 3\\nvalue = [0, 0, 2, 1, 0, 0]\\nclass = 2'),\n Text(0.6548672566371682, 0.2777777777777778, 'Height &lt;= 173.0\\nentropy = 0.503\\nsamples = 9\\nvalue = [0, 0, 0, 8, 1, 0]\\nclass = 3'),\n Text(0.6371681415929203, 0.16666666666666666, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0]\\nclass = 4'),\n Text(0.672566371681416, 0.16666666666666666, 'entropy = 0.0\\nsamples = 8\\nvalue = [0, 0, 0, 8, 0, 0]\\nclass = 3'),\n Text(0.6460176991150443, 0.3888888888888889, 'entropy = 0.0\\nsamples = 5\\nvalue = [0, 0, 5, 0, 0, 0]\\nclass = 2'),\n Text(0.7920353982300885, 0.8333333333333334, 'Height &lt;= 171.5\\nentropy = 1.229\\nsamples = 194\\nvalue = [0, 0, 0, 16, 55, 123]\\nclass = 5'),\n Text(0.7345132743362832, 0.7222222222222222, 'Weight &lt;= 116.0\\nentropy = 0.363\\nsamples = 101\\nvalue = [0, 0, 0, 0, 7, 94]\\nclass = 5'),\n Text(0.7168141592920354, 0.6111111111111112, 'Height &lt;= 164.0\\nentropy = 0.797\\nsamples = 29\\nvalue = [0, 0, 0, 0, 7, 22]\\nclass = 5'),\n Text(0.6991150442477876, 0.5, 'Weight &lt;= 95.5\\nentropy = 0.258\\nsamples = 23\\nvalue = [0, 0, 0, 0, 1, 22]\\nclass = 5'),\n Text(0.6814159292035398, 0.3888888888888889, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0]\\nclass = 4'),\n Text(0.7168141592920354, 0.3888888888888889, 'entropy = 0.0\\nsamples = 22\\nvalue = [0, 0, 0, 0, 0, 22]\\nclass = 5'),\n Text(0.7345132743362832, 0.5, 'entropy = 0.0\\nsamples = 6\\nvalue = [0, 0, 0, 0, 6, 0]\\nclass = 4'),\n Text(0.7522123893805309, 0.6111111111111112, 'entropy = 0.0\\nsamples = 72\\nvalue = [0, 0, 0, 0, 0, 72]\\nclass = 5'),\n Text(0.8495575221238938, 0.7222222222222222, 'Weight &lt;= 126.5\\nentropy = 1.454\\nsamples = 93\\nvalue = [0, 0, 0, 16, 48, 29]\\nclass = 4'),\n Text(0.7876106194690266, 0.6111111111111112, 'Weight &lt;= 114.5\\nentropy = 0.918\\nsamples = 48\\nvalue = [0, 0, 0, 16, 32, 0]\\nclass = 4'),\n Text(0.7699115044247787, 0.5, 'Height &lt;= 181.5\\nentropy = 1.0\\nsamples = 32\\nvalue = [0, 0, 0, 16, 16, 0]\\nclass = 3'),\n Text(0.7522123893805309, 0.3888888888888889, 'entropy = 0.0\\nsamples = 13\\nvalue = [0, 0, 0, 0, 13, 0]\\nclass = 4'),\n Text(0.7876106194690266, 0.3888888888888889, 'Weight &lt;= 105.5\\nentropy = 0.629\\nsamples = 19\\nvalue = [0, 0, 0, 16, 3, 0]\\nclass = 3'),\n Text(0.7699115044247787, 0.2777777777777778, 'entropy = 0.0\\nsamples = 10\\nvalue = [0, 0, 0, 10, 0, 0]\\nclass = 3'),\n Text(0.8053097345132744, 0.2777777777777778, 'Height &lt;= 190.0\\nentropy = 0.918\\nsamples = 9\\nvalue = [0, 0, 0, 6, 3, 0]\\nclass = 3'),\n Text(0.7876106194690266, 0.16666666666666666, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 0, 0, 0, 3, 0]\\nclass = 4'),\n Text(0.8230088495575221, 0.16666666666666666, 'entropy = 0.0\\nsamples = 6\\nvalue = [0, 0, 0, 6, 0, 0]\\nclass = 3'),\n Text(0.8053097345132744, 0.5, 'entropy = 0.0\\nsamples = 16\\nvalue = [0, 0, 0, 0, 16, 0]\\nclass = 4'),\n Text(0.911504424778761, 0.6111111111111112, 'Height &lt;= 185.5\\nentropy = 0.939\\nsamples = 45\\nvalue = [0, 0, 0, 0, 16, 29]\\nclass = 5'),\n Text(0.8761061946902655, 0.5, 'Weight &lt;= 139.5\\nentropy = 0.529\\nsamples = 25\\nvalue = [0, 0, 0, 0, 3, 22]\\nclass = 5'),\n Text(0.8584070796460177, 0.3888888888888889, 'Height &lt;= 178.5\\nentropy = 0.881\\nsamples = 10\\nvalue = [0, 0, 0, 0, 3, 7]\\nclass = 5'),\n Text(0.8407079646017699, 0.2777777777777778, 'entropy = 0.0\\nsamples = 5\\nvalue = [0, 0, 0, 0, 0, 5]\\nclass = 5'),\n Text(0.8761061946902655, 0.2777777777777778, 'Height &lt;= 184.5\\nentropy = 0.971\\nsamples = 5\\nvalue = [0, 0, 0, 0, 3, 2]\\nclass = 4'),\n Text(0.8584070796460177, 0.16666666666666666, 'Height &lt;= 180.0\\nentropy = 0.811\\nsamples = 4\\nvalue = [0, 0, 0, 0, 3, 1]\\nclass = 4'),\n Text(0.8407079646017699, 0.05555555555555555, 'entropy = 1.0\\nsamples = 2\\nvalue = [0, 0, 0, 0, 1, 1]\\nclass = 4'),\n Text(0.8761061946902655, 0.05555555555555555, 'entropy = 0.0\\nsamples = 2\\nvalue = [0, 0, 0, 0, 2, 0]\\nclass = 4'),\n Text(0.8938053097345132, 0.16666666666666666, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 1]\\nclass = 5'),\n Text(0.8938053097345132, 0.3888888888888889, 'entropy = 0.0\\nsamples = 15\\nvalue = [0, 0, 0, 0, 0, 15]\\nclass = 5'),\n Text(0.9469026548672567, 0.5, 'Weight &lt;= 139.0\\nentropy = 0.934\\nsamples = 20\\nvalue = [0, 0, 0, 0, 13, 7]\\nclass = 4'),\n Text(0.9292035398230089, 0.3888888888888889, 'entropy = 0.0\\nsamples = 9\\nvalue = [0, 0, 0, 0, 9, 0]\\nclass = 4'),\n Text(0.9646017699115044, 0.3888888888888889, 'Height &lt;= 196.5\\nentropy = 0.946\\nsamples = 11\\nvalue = [0, 0, 0, 0, 4, 7]\\nclass = 5'),\n Text(0.9469026548672567, 0.2777777777777778, 'Height &lt;= 194.0\\nentropy = 0.544\\nsamples = 8\\nvalue = [0, 0, 0, 0, 1, 7]\\nclass = 5'),\n Text(0.9292035398230089, 0.16666666666666666, 'entropy = 0.0\\nsamples = 5\\nvalue = [0, 0, 0, 0, 0, 5]\\nclass = 5'),\n Text(0.9646017699115044, 0.16666666666666666, 'Weight &lt;= 150.0\\nentropy = 0.918\\nsamples = 3\\nvalue = [0, 0, 0, 0, 1, 2]\\nclass = 5'),\n Text(0.9469026548672567, 0.05555555555555555, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0]\\nclass = 4'),\n Text(0.9823008849557522, 0.05555555555555555, 'entropy = 0.0\\nsamples = 2\\nvalue = [0, 0, 0, 0, 0, 2]\\nclass = 5'),\n Text(0.9823008849557522, 0.2777777777777778, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 0, 0, 0, 3, 0]\\nclass = 4')]"
  },
  {
    "objectID": "notebooks/DecisionTreeNotebook.html#accuracy-results-on-test-data",
    "href": "notebooks/DecisionTreeNotebook.html#accuracy-results-on-test-data",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Accuracy Results on Test Data",
    "text": "Accuracy Results on Test Data\n\nprint(\"Micro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='micro'))\nprint(\"Macro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='macro'))\nprint(\"Precision: \", sklearn.metrics.precision_score(y_test, y_pred, average=\"weighted\"))\nprint(\"Recall: \", sklearn.metrics.recall_score(y_test, y_pred, average='weighted'))\n\n# Calculate AUC-PRC for each class\nauc_prc_scores = []\nfor class_index in range(y_pred.shape[0]):\n    precision, recall, _ = sklearn.metrics.precision_recall_curve(y_test == class_index, y_pred == class_index)\n    auc_prc_scores.append(sklearn.metrics.auc(recall, precision))\n\n# Compute the summary metric (e.g., micro-average)\nmicro_avg_auc_prc = sklearn.metrics.auc(recall, precision)\n\nprint(\"AUC-PRC for Each Class:\", auc_prc_scores)\nprint(\"Micro-Average AUC-PRC:\", micro_avg_auc_prc)\n\nMicro F1:  0.8000000000000002\nMacro F1:  0.7033683460612853\nPrecision:  0.8400535714285715\nRecall:  0.8\nAUC-PRC for Each Class: [0.75, 0.625, 0.7875, 0.8284340659340659, 0.8070833333333334, 0.915301724137931, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\nMicro-Average AUC-PRC: 0.5\n\n\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\nc:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:979: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n\n\n\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nconf_df = pd.DataFrame(confusion_matrix, index=[f\"{i}\" for i in range(6)], columns=[f\"{i}\" for i in range(6)])\nheatmap = sns.heatmap(conf_df, annot=True, fmt=\"d\", linewidths=0.35, cmap=\"YlGnBu\")\nplt.title(f\"Model Predictions With {(np.sum(confusion_matrix.diagonal()) / y_test.shape[0]) * 100:.2f}% Accuracy\")\n\nText(0.5, 1.0, 'Model Predictions With 80.00% Accuracy')"
  },
  {
    "objectID": "notebooks/DecisionTreeNotebook.html#hyperparamter-tuning",
    "href": "notebooks/DecisionTreeNotebook.html#hyperparamter-tuning",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Hyperparamter Tuning",
    "text": "Hyperparamter Tuning\n\nparam_grid = {\n    'max_depth': [i for i in range(2, 9)],\n    'min_samples_leaf': [2 ** i for i in range(0, 4)],\n    'criterion': [\"entropy\", \"gini\"]\n}\n\nrf = RandomForestClassifier(random_state=42)\n\nprint(y_train.shape)\ngrid_search_rf = GridSearchCV(rf, param_grid, cv=StratifiedKFold(n_splits=3), scoring='accuracy')\ngrid_search_rf.fit(X_train, y_train.values.ravel())\nbest_params_rf = grid_search_rf.best_params_\nprint(\"Best Hyperparameters:\", best_params_rf)\nprint(\"Best Score:\", grid_search_rf.best_score_)\n\n(320, 1)\nBest Hyperparameters: {'criterion': 'gini', 'max_depth': 8, 'min_samples_leaf': 1}\nBest Score: 0.8125844942103098"
  },
  {
    "objectID": "notebooks/DecisionTreeNotebook.html#randomforest-classifier",
    "href": "notebooks/DecisionTreeNotebook.html#randomforest-classifier",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "RandomForest Classifier",
    "text": "RandomForest Classifier\n\nrf = RandomForestClassifier(n_estimators=300, \n                            max_depth=best_params_rf[\"max_depth\"], \n                            min_samples_leaf=best_params_rf[\"min_samples_leaf\"],\n                            criterion=best_params_rf[\"criterion\"])\nrf.fit(X_train, y_train.values.ravel())\ny_pred = rf.predict(X_test)"
  },
  {
    "objectID": "notebooks/DecisionTreeNotebook.html#plotting-the-randomforest-trees",
    "href": "notebooks/DecisionTreeNotebook.html#plotting-the-randomforest-trees",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Plotting the RandomForest Trees",
    "text": "Plotting the RandomForest Trees\n\n    fig, axes = plt.subplots(nrows = 1,ncols = 5,figsize = (10,3), dpi=250)\n    for index in range(5):\n        tree.plot_tree(rf.estimators_[index],\n                    feature_names = X_train.columns.tolist(), \n                    class_names= [f\"{i}\" for i in range(6)],\n                    filled = True,\n                    ax = axes[index])\n\n        axes[index].set_title('Estimator: ' + str(index + 1), fontsize = 10)\n#     fig.savefig(f'rf_{dt.n_estimators}trees.png')\n    plt.show()\n\n\n\n\n\nprint(\"Micro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='micro'))\nprint(\"Macro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='macro'))\n\nMicro F1:  0.8875\nMacro F1:  0.8523938236704195\n\n\n\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nconf_df = pd.DataFrame(confusion_matrix, index=[f\"{i}\" for i in range(6)], columns=[f\"{i}\" for i in range(6)])\nheatmap = sns.heatmap(conf_df, annot=True, fmt=\"d\", linewidths=0.35, cmap=\"YlGnBu\")\nplt.title(f\"Model Predictions With {(np.sum(confusion_matrix.diagonal()) / y_test.shape[0]) * 100:.2f}% Accuracy\")\n\nText(0.5, 1.0, 'Model Predictions With 88.75% Accuracy')\n\n\n\n\n\n\n'''\nPlot a graph that compares the two models, randomly generated with tuned hyperparameter models\n'''\ndt_results = []\nrf_results = []\nn_samples = 40\nindexes = [i for i in range(n_samples)]\nfor i in indexes:\n    dt = DecisionTreeClassifier(max_depth=8, min_samples_leaf=2, min_samples_split=4)\n    dt.fit(X_train, y_train)\n    y_pred_dt = dt.predict(X_test)\n    \n    rf = RandomForestClassifier(n_estimators=325, max_depth=8)\n    rf.fit(X_train, y_train.values.ravel())\n    y_pred_rf = rf.predict(X_test)\n    \n    confusion_matrix_dt = sklearn.metrics.confusion_matrix(y_test, y_pred_dt)\n    confusion_matrix_rf = sklearn.metrics.confusion_matrix(y_test, y_pred_rf)\n    \n    dt_results.append((np.sum(confusion_matrix_dt.diagonal()) / y_test.shape[0]) * 100)\n    rf_results.append((np.sum(confusion_matrix_rf.diagonal()) / y_test.shape[0]) * 100)\n\nplt.plot(indexes, dt_results, label=\"DT results\")\nplt.plot(indexes, rf_results, label=\"RF results\")\nplt.xlabel(\"Sample\")\nplt.ylabel(\"Accuracy on Test Data in %\")\nplt.title(\"Accuracy Comparison Between DT and RF on Randomly Generated Models\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "notebooks/DecisionTreeNotebook.html#improvements",
    "href": "notebooks/DecisionTreeNotebook.html#improvements",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Improvements",
    "text": "Improvements\n\nprint(X_train.shape)\nprint(y_train.shape)\n\nparam_grid = {\n    'max_depth': [i for i in range(2, 10)],\n    'min_samples_leaf': [2 ** i for i in range(0, 4)],\n    'criterion': [\"entropy\", \"gini\"]\n}\n\nrf = RandomForestClassifier(random_state=42)\ngrid_search_rf = GridSearchCV(rf, param_grid, cv=StratifiedKFold(n_splits=5), scoring='accuracy')\ngrid_search_rf.fit(X_train.drop(['Gender_Encoded'], axis=1), y_train.values.ravel())\nbest_params_rf = grid_search_rf.best_params_\nprint(\"Best Hyperparameters:\", best_params_rf)\nprint(\"Best Score:\", grid_search_rf.best_score_)\n\nrf = RandomForestClassifier(n_estimators=300, \n                            max_depth=best_params_rf[\"max_depth\"], \n                            min_samples_leaf=best_params_rf[\"min_samples_leaf\"],\n                            criterion=best_params_rf[\"criterion\"])\nrf.fit(X_train.drop(['Gender_Encoded'], axis=1), y_train.values.ravel())\ny_pred = rf.predict(X_test.drop(['Gender_Encoded'], axis=1))\n\n(320, 3)\n(320, 1)\nBest Hyperparameters: {'criterion': 'entropy', 'max_depth': 8, 'min_samples_leaf': 1}\nBest Score: 0.85625\n\n\n\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nconf_df = pd.DataFrame(confusion_matrix, index=[f\"{i}\" for i in range(6)], columns=[f\"{i}\" for i in range(6)])\nheatmap = sns.heatmap(conf_df, annot=True, fmt=\"d\", linewidths=0.35, cmap=\"YlGnBu\")\nplt.title(f\"Model Predictions With {(np.sum(confusion_matrix.diagonal()) / y_test.shape[0]) * 100:.2f}% Accuracy (NO GENDER)\")\n\nText(0.5, 1.0, 'Model Predictions With 90.00% Accuracy (NO GENDER)')\n\n\n\n\n\n\nprint(\"Micro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='micro'))\nprint(\"Macro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='macro'))\n\nMicro F1:  0.9\nMacro F1:  0.8450314415205457"
  },
  {
    "objectID": "notebooks/DecisionTreeNotebook.html#learning-curve",
    "href": "notebooks/DecisionTreeNotebook.html#learning-curve",
    "title": "Comparing DT and RandomForest on BMI Data",
    "section": "Learning Curve",
    "text": "Learning Curve\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.tree import DecisionTreeClassifier\ntrain_sizes, train_scores, test_scores = learning_curve(rf, X_train, y_train.values.ravel(), cv=StratifiedKFold(n_splits=5))\ndisplay = LearningCurveDisplay(train_sizes=train_sizes,\n    train_scores=train_scores, test_scores=test_scores, score_name=\"Score\")\ndisplay.plot()\nplt.show()"
  },
  {
    "objectID": "notebooks/regression.html",
    "href": "notebooks/regression.html",
    "title": "Salary Prediction Using Regression on MLB Data",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.svm import SVR\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error"
  },
  {
    "objectID": "notebooks/regression.html#data-analysis",
    "href": "notebooks/regression.html#data-analysis",
    "title": "Salary Prediction Using Regression on MLB Data",
    "section": "Data Analysis",
    "text": "Data Analysis\nBefore plugging in the data into our SVM, it is important to evaluate redundant information to limit multicollinearity. We can find collinearities through the correlation matrix.\n\ndf = pd.read_csv(\"./Hitters.csv\")\nnum_cols = [col for col in df.columns if df[col].dtypes != \"O\"]\ncorr = df[num_cols].corr()\nsns.set(rc={'figure.figsize': (15, 15)})\nsns.heatmap(corr, cmap=\"RdBu\", annot=True)\nplt.show()\n\n\n\n\n\nRun PCA on highly-correlated features\n\npca_custat = PCA(n_components=1)\npca_indstat = PCA(n_components=1)\npca_fieldstat = PCA(n_components=1)\n\ncustat_df = df[[\"CAtBat\", \"CHits\", \"CHmRun\", \"CRuns\", \"CRBI\", \"CWalks\"]]\nindstat_df = df[[\"AtBat\", \"Hits\", \"HmRun\", \"Runs\", \"RBI\", \"Walks\"]]\nfieldstat_df = df[[\"Assists\", \"Errors\"]]\n\ncustat_df_pca = pca_custat.fit_transform(custat_df)\nindstat_df_pca = pca_indstat.fit_transform(indstat_df)\nfieldstat_df_pca = pca_fieldstat.fit_transform(fieldstat_df)\n\ndf_reduced = df.drop(columns=[\"AtBat\", \"Hits\", \"HmRun\", \"Runs\", \"RBI\", \"Walks\", \"CAtBat\", \"CHits\", \"CHmRun\", \"CRuns\", \"CRBI\", \"CWalks\", \"Assists\", \"Errors\"])\ndf_reduced = df_reduced.drop(columns=[\"League\", \"NewLeague\", \"Division\", \"Years\"])\ndf_reduced[\"I_PC1\"] = indstat_df_pca\ndf_reduced[\"C_PC2\"] = custat_df_pca\ndf_reduced[\"F_PC3\"] = fieldstat_df_pca\n\n# label_mapping_an = {'A': 0, 'N': 1}\n# label_mapping_div = {'E': 0, 'W': 1}\n\n\n# # Apply label encoding to the 'Category' column\n# df_reduced['League'] = df_reduced['League'].map(label_mapping_an)\n# df_reduced['NewLeague'] = df_reduced['NewLeague'].map(label_mapping_an)\n# df_reduced['Division'] = df_reduced['Division'].map(label_mapping_div)\ndf_reduced.dropna(axis=0, inplace=True)\n\nprint(df_reduced)\n\n     PutOuts  Salary       I_PC1        C_PC2       F_PC3\n1        632   475.0   72.590869   801.168761  -63.814023\n2        880   480.0 -108.738125 -1059.201154  -24.703570\n3        200   500.0 -124.894681  3171.420761  -96.026925\n4        805    91.5   63.304301 -2384.553500  -67.009725\n5        282   750.0 -221.371087  1777.108228  314.474814\n..       ...     ...         ...          ...         ...\n317      325   700.0 -117.568310    61.626177  -98.025844\n318      313   875.0 -122.303569  3049.912291  274.332014\n319       37   385.0  -96.674458 -1030.222990    6.049449\n320     1314   960.0 -201.645999   587.940371   24.204149\n321      408  1000.0 -255.582377  2378.079204 -103.023139\n\n[263 rows x 5 columns]\n\n\n\nnum_cols = [col for col in df_reduced.columns if df_reduced[col].dtypes != \"O\"]\ncorr = df_reduced[num_cols].corr()\nsns.set(rc={'figure.figsize': (15, 15)})\nsns.heatmap(corr, cmap=\"RdBu\", annot=True)\nplt.show()\n\n\n\n\nwe’ve reduced the data from 20 columns to 6 columns, which is important, as collinearity can negatively effect the preformance of regression models. Apart from the collinearity effect, I decided to get rid of discretely labeled binary relationships (labels 0 or 1), as this makes the linear regression model more complex and doesn’t effect the model preformance that much.\n\n\nCreate Training and Test Sets\nSince we’re trying to predict salary, I extract “salary” column from the dataframe, storing it into the label, and dropping that column from the feature data.\n\ndf = df_reduced\ny = df[\"Salary\"]\nX = df.drop(columns=\"Salary\")\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n\nprint(f\"Salary STD: ${np.std(y) * 1000:,.2f}\")\nprint(f\"Salary Mean: ${np.mean(y) * 1000:,.2f}\")\nprint(f\"Salary Low: ${np.min(y) * 1000:,.2f}\")\nprint(f\"Salary High: ${np.max(y) * 1000:,.2f}\")\n\nSalary STD: $450,260.22\nSalary Mean: $535,925.88\nSalary Low: $67,500.00\nSalary High: $2,460,000.00"
  },
  {
    "objectID": "notebooks/regression.html#linear-support-vector-regression-kernellinear",
    "href": "notebooks/regression.html#linear-support-vector-regression-kernellinear",
    "title": "Salary Prediction Using Regression on MLB Data",
    "section": "Linear Support Vector Regression (kernel=“linear”)",
    "text": "Linear Support Vector Regression (kernel=“linear”)\n\nsc = StandardScaler()\n\nX_train_scaled = sc.fit_transform(X_train, y_train)\nX_test_scaled = sc.fit_transform(X_test, y_test)\n# pca_all = PCA(n_components=1)\n# X_train_scaled_pca = pca_all.fit_transform(X_train_scaled)\n# X_test_scaled_pca = pca_all.fit_transform(X_test_scaled)\n\nsvr_lin = SVR(kernel=\"linear\", C=1, gamma=\"auto\")\nsvr_lin.fit(X_train_scaled, y_train)\ny_pred = svr_lin.predict(X_test_scaled)\n\n\nVisualize Multiple Linear Regression\n\nplt.figure(figsize=(16, 12))\nindependent_variables = X_train.columns\ndependent_variable = \"Salary\"\nX_test_numpy = X_test.to_numpy()\nfor i, col in enumerate(independent_variables, 1):\n    plt.subplot(3, 3, i)\n    sns.regplot(x=X_train[col],y=y_train,ci=None,color ='red')\n    sns.scatterplot(data=X_train, x=col, y=y_train, color='blue', label='Training Points')\n    sns.scatterplot(data=X_test, x=col, y=y_test, color='green', label='Testing Points')\n    sns.scatterplot(data=X_test, x=col, y=y_pred, color='red', label='Predicted Points')\n    plt.title(f'{dependent_variable} vs. {col}')\n    plt.xlabel(col)\n    plt.ylabel(dependent_variable)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nresult_df = pd.DataFrame(columns=[\"id\", \"actual\", \"predicted\"])\n\nfor i, actual, predicted in zip(y_test.index, y_test, y_pred):\n    entry = [i, actual, predicted]\n    df_entry = pd.DataFrame(entry, index=[\"id\", \"actual\", \"predicted\"]).T\n    result_df = pd.concat((result_df, df_entry))\n#print(result_df)\ndifference = abs(result_df[\"actual\"] - result_df[\"predicted\"])\nprint(f\"Cumulative Difference: ${np.sum(difference) * 1000:,.2f}\")\nprint(f\"Min Difference: ${np.min(difference) * 1000:,.2f}\")\nprint(f\"Max Difference: ${np.max(difference) * 1000:,.2f}\")\nprint(f\"Average Difference: ${np.mean(difference) * 1000:,.2f}\")\nprint(f\"Std Difference: ${np.std(difference) * 1000:,.2f}\")\nprint(f\"Mean Squared Error: ${mean_squared_error(y_test, y_pred):,.2f}\")\n\nCumulative Difference: $10,792,084.13\nMin Difference: $19,061.95\nMax Difference: $1,968,301.30\nAverage Difference: $269,802.10\nStd Difference: $361,894.28\nMean Squared Error: $203,760.65\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nplt.figure(figsize=(8,8))\nresiduals = result_df['actual'] - result_df['predicted']\n\nplt.scatter(result_df['id'], residuals)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('ID')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.show()"
  },
  {
    "objectID": "notebooks/regression.html#non-linear-support-vector-regression-kernelpoly",
    "href": "notebooks/regression.html#non-linear-support-vector-regression-kernelpoly",
    "title": "Salary Prediction Using Regression on MLB Data",
    "section": "Non-Linear Support Vector Regression (kernel=“poly”)",
    "text": "Non-Linear Support Vector Regression (kernel=“poly”)\n\nsvr_poly = SVR(kernel=\"poly\", degree=2, C=75, gamma=\"scale\")\nsvr_poly.fit(X_train_scaled, y_train)\ny_pred = svr_poly.predict(X_test_scaled)\n\nresult_df = pd.DataFrame(columns=[\"id\", \"actual\", \"predicted\"])\n\nfor i, actual, predicted in zip(y_test.index, y_test, y_pred):\n    entry = [i, actual, predicted]\n    df_entry = pd.DataFrame(entry, index=[\"id\", \"actual\", \"predicted\"]).T\n    result_df = pd.concat((result_df, df_entry))\n    \n#print(result_df)\ndifference = abs(result_df[\"actual\"] - result_df[\"predicted\"])\nprint(f\"Cumulative Difference: ${np.sum(difference) * 1000:,.2f}\")\nprint(f\"Min Difference: ${np.min(difference) * 1000:,.2f}\")\nprint(f\"Max Difference: ${np.max(difference) * 1000:,.2f}\")\nprint(f\"Average Difference: ${np.mean(difference) * 1000:,.2f}\")\nprint(f\"Std Difference: ${np.std(difference) * 1000:,.2f}\")\nprint(f\"Mean Squared Error: ${mean_squared_error(y_test, y_pred):,.2f}\")\n\nCumulative Difference: $13,385,640.30\nMin Difference: $798.03\nMax Difference: $1,581,505.51\nAverage Difference: $334,641.01\nStd Difference: $330,986.90\nMean Squared Error: $221,536.94\n\n\n\nplt.figure(figsize=(16, 12))\nindependent_variables = X_train.columns\ndependent_variable = \"Salary\"\nX_test_numpy = X_test.to_numpy()\nfor i, col in enumerate(independent_variables, 1):\n    plt.subplot(3, 3, i)\n    sns.regplot(x=X_train[col],y=y_train,ci=None,color ='red', order=svr_poly.degree)\n    sns.scatterplot(data=X_train, x=col, y=y_train, color='blue', label='Training Points')\n    sns.scatterplot(data=X_test, x=col, y=y_test, color='green', label='Testing Points')\n    sns.scatterplot(data=X_test, x=col, y=y_pred, color='red', label='Predicted Points')\n    plt.title(f'{dependent_variable} vs. {col}')\n    plt.xlabel(col)\n    plt.ylabel(dependent_variable)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nplt.figure(figsize=(8,8))\nresiduals = result_df['actual'] - result_df['predicted']\n\nplt.scatter(result_df['id'], residuals)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('ID')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.show()"
  }
]