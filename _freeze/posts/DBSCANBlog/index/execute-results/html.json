{
  "hash": "220f2fa750f052b77e9247b293009c3e",
  "result": {
    "markdown": "---\nexecute:\n  echo: fenced\ntitle: DBSCAN Outlier Detection on Iris Data\nauthor: Daniel Hassler\ndate: '2023-10-28'\ncategories:\n  - code\n  - anomaly\nimage: image.jpg\ntoc: true\ntoc-title: Table of contents\nformat:\n  html:\n    embed-resources: true\n    code-copy: true\n    code-link: true\n    code-tools: true\n    theme:\n      dark: darkly\n      light: flatly\n    pdf:\n      title: Anomaly Detection\n      author: Daniel Hassler\n      pdf-engine: 'C:/Program Files (x86)/wkhtmltopdf'\n  docx: default\n  ipynb: default\n  gfm: default\nfilters:\n  - social-share\nshare:\n  permalink: 'https://hassledw.github.io/ML-blog-posts/posts/DBSCANBlog/'\n  description: Anomaly detection blog using DBSCAN\n  twitter: true\n  facebook: true\n  reddit: true\n  stumble: false\n  tumblr: false\n  linkedin: true\n  email: true\n---\n\n<!-- title: \"Comparing Decision Tree and Random Forest Classifier Performance\"\nformat:\n  html:\n    code-fold: true\njupyter: python3 -->\n**Author: Daniel Hassler**\n\n<link rel=\"stylesheet\" type=\"text/css\" href=\"./index.css\">\n<div class=\"social-icons\">\n  <a href=\"https://github.com/hassledw\"><i class=\"fab fa-github\"></i></a>\n  <a href=\"https://www.linkedin.com/in/daniel-hassler-85027a21a/\"><i class=\"fab fa-linkedin\"></i></a>\n  <!-- Add more social media links/icons as needed -->\n</div>\n\n## Introduction\nAnomaly detection is a common task for a lot of unsupervised learning settings. In the case of anomaly detection here, I will be performing DBSCAN, a clustering algorithm, on the infamous Iris dataset to detect outliers.\n\nFirst, I import all the necessary libraries for the project:\n\n::: {.cell execution_count=1}\n```` { .cell-code}\n```{{python}}\nfrom sklearn.datasets import load_iris\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.model_selection import ParameterGrid\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n```\n\n````\n:::\n\n\n## Data\nFor the data, I will be using the Iris dataset taken from `sklearn.datasets` library. This dataset is useful for educational purposes and is an accurate representation of some real world data; it contains `150` samples of iris data and has `4` columns: Sepal Length, Sepal Width, Petal Length and Petal Width.\n\nFor the purpose of anomaly detection and being able to visualize anomalies/outliers, I ran PCA (principal component analysis) to reduce the dimensionality of the iris dataset (without targets) from shape (150, 4) to (150, 2). PCA finds the eigenvalues and eigenvectors of the covariance matrix of the entire dataset, and the algorithm takes the top `n_components`, in this case two, to represent the data in two dimensions. This allows me to visualize this dataset in two dimensions and makes clustering more efficient and representative.\n\n::: {.cell execution_count=2}\n```` { .cell-code}\n```{{python}}\niris = load_iris()\npca = PCA(n_components=2)\ndata_pca = pca.fit_transform(iris.data, iris.target)\ndf = pd.DataFrame(data_pca, columns=[\"PC1\", \"PC2\"])\ndf['target'] = iris.target\nprint(df)\n```\n\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n          PC1       PC2  target\n0   -2.684126  0.319397       0\n1   -2.714142 -0.177001       0\n2   -2.888991 -0.144949       0\n3   -2.745343 -0.318299       0\n4   -2.728717  0.326755       0\n..        ...       ...     ...\n145  1.944110  0.187532       2\n146  1.527167 -0.375317       2\n147  1.764346  0.078859       2\n148  1.900942  0.116628       2\n149  1.390189 -0.282661       2\n\n[150 rows x 3 columns]\n```\n:::\n:::\n\n\nNext, because I have the labels, I plot the reduced dimensionality representation of the Iris dataset just to visualize the dataset:\n\n::: {.cell execution_count=3}\n```` { .cell-code}\n```{{python}}\n_, ax = plt.subplots()\nscatter = ax.scatter(df[\"PC1\"], df[\"PC2\"], c=iris.target, cmap=\"copper\")\nax.set(xlabel=\"PC1\", ylabel=\"PC2\")\n_ = ax.legend(\n    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n)\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=600 height=434}\n:::\n:::\n\n\n## DBSCAN Clustering\nDensity-Based Spatial Clustering Of Applications With Noise (DBSCAN) is a clustering algorithm that groups each point into a neighborhood, given a radius (`eps`) and a minimum number of points (`min_samples`). This is more representative for applications with real-life data, especially since they can contain noise.\n\n### Hyperparameter Tuning\nIn order to create realistic clusters with DBSCAN and maximize an optimization, we need to preform hyperparameter tuning. Below is a GridSearch implementation for tweaking and finding the best `eps` and `min_samples` hyperparameters. \n\n::: {.cell execution_count=4}\n```` { .cell-code}\n```{{python}}\nparams = {\n    'eps': [i / 10 for i in range(1, 15)],\n    'min_samples': [i for i in range(1, 10)]\n}\n\nbest_score = -1\nbest_params = {}\n\nfor param_i in ParameterGrid(params):\n    db = DBSCAN(**param_i)\n    labels = db.fit_predict(data_pca)\n    # minimum of 4 clusters (3 classes + 1 outlier)\n    if len(np.unique(labels)) <= 3:\n        continue\n    curr_score = silhouette_score(data_pca, labels)\n    if curr_score > best_score:\n        best_score = curr_score\n        best_params = param_i\n\nprint(\"Best Score: \", best_score)\nprint(\"Best Params: \", best_params)\n```\n\n````\n\n::: {.cell-output .cell-output-stdout}\n```\nBest Score:  0.45189188489361554\nBest Params:  {'eps': 0.3, 'min_samples': 6}\n```\n:::\n:::\n\n\n### DBSCAN Initialization and Visualization\nNext, I plugged in the \"best\" hyperparameters to the DBSCAN object generated from the GridSearch, and visualized the DBSCAN clusters with the outliers in the next codeblock.\n\n::: {.cell execution_count=5}\n```` { .cell-code}\n```{{python}}\ndb = DBSCAN(**best_params).fit(data_pca)\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint(\"Estimated number of clusters: %d\" % n_clusters_)\nprint(\"Estimated number of noise points: %d\" % n_noise_)\n```\n\n````\n\n::: {.cell-output .cell-output-stdout}\n```\nEstimated number of clusters: 3\nEstimated number of noise points: 20\n```\n:::\n:::\n\n\n::: {.cell execution_count=6}\n```` { .cell-code}\n```{{python}}\ny_pred = db.fit_predict(data_pca)\nfig, ax = plt.subplots()\nscatter = ax.scatter(df[\"PC1\"], df[\"PC2\"], c=y_pred, cmap='copper')\ntext_labels = [\"outlier\", \"setosa\", \"versicolor\", \"virginica\"]\nlegend1 = ax.legend(scatter.legend_elements()[0], text_labels,\n                    loc=\"lower right\", title=\"Classes\")\nax.add_artist(legend1)\nplt.title(\"DBSCAN of Iris Data\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\n```\n\n````\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nText(0, 0.5, 'PC2')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-2.png){width=600 height=449}\n:::\n:::\n\n\nGiven the best hyperparameters, with realistic limitations of at least 4 clusters (3 classes, 1 outlier), the clusters look roughly similar to the expected classifications. The great advantage to DBSCAN is the ability to come up with these clusters without knowledge of the original labels (unsupervised), and based on the visualization of all this, it's roughly representative of the actual data.\n\n### Outliers\nThe outliers from the above visualization are represented in black. Based on the configuration of the DBSCAN object, it produced three main clusters, one for each label, and a predicted outlier one. It is important to note that the outliers in the graph are past the farthest ends of the main clusters, which is truly representative of outliers/anomalies. To emphasize further, the strength and quantity of such outliers in a DBSCAN cluster is heavily dependent on its hyperparameter setup.\n\n## Improvements\nThere are some improvements we can make to DBSCAN. One improvement would include more data samples, as more data CAN further improve the clusters and limit some outliers. Another improvement we can make is exploring the hyperparameters further with finer `eps` values.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}