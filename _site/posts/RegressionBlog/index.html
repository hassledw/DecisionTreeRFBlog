<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>

<meta charset="utf-8" />
<meta name="generator" content="quarto-1.3.450" />

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />

<meta name="author" content="Daniel Hassler" />
<meta name="dcterms.date" content="2023-11-14" />

<title>ML-Blog-Posts – Comparing Multiple Linear and Non-Linear Regression on MLB Data</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<!-- htmldependencies:E3FAD763 -->
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css" />
</head>

<body>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="/index.html">
    <span class="navbar-title">ML-Blog-Posts</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse"
  aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation"
  onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="/about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/hassledw" rel="" target=""><i 
  class="bi bi-github" 
  role="img" 
>
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/daniel-hassler-85027a21a/" rel="" target=""><i 
  class="bi bi-linkedin" 
  role="img" 
>
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <div id="quarto-toc-target"></div>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
  <div class="quarto-title-banner">
    <div class="quarto-title column-body">
      <h1 class="title">Comparing Multiple Linear and Non-Linear Regression on MLB Data</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">code</div>
                <div class="quarto-category">regression</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Daniel Hassler </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 14, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header>
<nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#data" id="toc-data">Data</a>
  <ul>
  <li><a href="#creating-the-training-and-test-sets" id="toc-creating-the-training-and-test-sets">Creating the Training and Test Sets</a></li>
  </ul></li>
  <li><a href="#multiple-linear-support-vector-regression-kernellinear" id="toc-multiple-linear-support-vector-regression-kernellinear">Multiple Linear Support Vector Regression (kernel=“linear”)</a>
  <ul>
  <li><a href="#multiple-linear-regression-visualization" id="toc-multiple-linear-regression-visualization">Multiple Linear Regression Visualization</a></li>
  </ul></li>
  <li><a href="#multiple-non-linear-support-vector-regression-kernelpoly" id="toc-multiple-non-linear-support-vector-regression-kernelpoly">Multiple Non-Linear Support Vector Regression (kernel=“poly”)</a>
  <ul>
  <li><a href="#multiple-non-linear-regression-visualization" id="toc-multiple-non-linear-regression-visualization">Multiple Non-Linear Regression Visualization</a></li>
  </ul></li>
  <li><a href="#discussion-and-improvements" id="toc-discussion-and-improvements">Discussion and Improvements</a></li>
  </ul>
</nav>
<!-- title: "Comparing Decision Tree and Random Forest Classifier Performance"
format:
  html:
    code-fold: true
jupyter: python3 -->
<p><strong>Author: Daniel Hassler</strong></p>
<link rel="stylesheet" type="text/css" href="./index.css">
<div class="social-icons">
<p><a href="https://github.com/hassledw"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/daniel-hassler-85027a21a/"><i class="fab fa-linkedin"></i></a> <!-- Add more social media links/icons as needed --></p>
</div>
<div class="cell" data-execution_count="1">
<div class="sourceCode" id="cb1"><pre class="sourceCode markdown cell-code"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVR</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code></pre></div>
</div>
<section id="data" class="level2">
<h2>Data</h2>
<p>With Kaggle, I was able to find an MLB (Major League Baseball) dataset consisting of player data from the 1986-1987 seasons (https://www.kaggle.com/datasets/mathchi/hitters-baseball-data). The data itself has many features consisting of individual stats for the season, cumulative career stats, fielding stats, and salary. In total, there are <code>20</code> features with <code>322</code> entries before preprocessing.</p>
<p>The goal of this notebook is to showcase linear and non-linear regression as a way of predicting <code>salary</code> (in thousands), a continuous variable, for my dataset. But before I run through the regression process, I have to clean the data first and figure out correlations.</p>
<p>One negative influence on regression models is collinearity in the feature space. Collinearity is problematic for several reasons, including overfitting, interpretability, and inefficiency. When there are many features that are highly correlated, this will create a strong negative impact on the performance. So before plugging into the model, I analyzed the correlation matrix consisting of correlations between all the features.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode" id="cb2"><pre class="sourceCode markdown cell-code"><code class="sourceCode markdown"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&quot;./Hitters.csv&quot;</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>num_cols <span class="op">=</span> [col <span class="cf">for</span> col <span class="kw">in</span> df.columns <span class="cf">if</span> df[col].dtypes <span class="op">!=</span> <span class="st">&quot;O&quot;</span>]</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>corr <span class="op">=</span> df[num_cols].corr()</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>(rc<span class="op">=</span>{<span class="st">&#39;figure.figsize&#39;</span>: (<span class="dv">15</span>, <span class="dv">15</span>)})</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>sns.heatmap(corr, cmap<span class="op">=</span><span class="st">&quot;RdBu&quot;</span>, annot<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-3-output-1.png" width="1083" height="1160" /></p>
</div>
</div>
<p>Based on the above correlation matrix, I can see there’s a high correlation between all the individual season player stats (<code>AtBat</code>, <code>Hits</code>, <code>HmRun</code>, <code>Runs</code>, <code>RBI</code>, and <code>Walks</code>), cumulative player stats (<code>CAtBat</code>, <code>CHits</code>, <code>CHmRun</code>, <code>CRuns</code>, <code>CRBI</code>, <code>Years</code>, and <code>CWalks</code>), and finally some fielding stats (<code>Assists</code>, <code>Errors</code>).</p>
<p>One way to remove these strong correlations is to run a dimensionality reduction technique. In this case, I will be using PCA (principal component analysis) seperately on those three highly correlated areas: individual stats, cumulative stats, and fielding.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode" id="cb3"><pre class="sourceCode markdown cell-code"><code class="sourceCode markdown"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>pca_custat <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>pca_indstat <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>pca_fieldstat <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>custat_df <span class="op">=</span> df[[<span class="st">&quot;CAtBat&quot;</span>, <span class="st">&quot;CHits&quot;</span>, <span class="st">&quot;CHmRun&quot;</span>, <span class="st">&quot;CRuns&quot;</span>, <span class="st">&quot;CRBI&quot;</span>, <span class="st">&quot;CWalks&quot;</span>]]</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>indstat_df <span class="op">=</span> df[[<span class="st">&quot;AtBat&quot;</span>, <span class="st">&quot;Hits&quot;</span>, <span class="st">&quot;HmRun&quot;</span>, <span class="st">&quot;Runs&quot;</span>, <span class="st">&quot;RBI&quot;</span>, <span class="st">&quot;Walks&quot;</span>]]</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>fieldstat_df <span class="op">=</span> df[[<span class="st">&quot;Assists&quot;</span>, <span class="st">&quot;Errors&quot;</span>]]</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>custat_df_pca <span class="op">=</span> pca_custat.fit_transform(custat_df)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>indstat_df_pca <span class="op">=</span> pca_indstat.fit_transform(indstat_df)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>fieldstat_df_pca <span class="op">=</span> pca_fieldstat.fit_transform(fieldstat_df)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>df_reduced <span class="op">=</span> df.drop(columns<span class="op">=</span>[<span class="st">&quot;AtBat&quot;</span>, <span class="st">&quot;Hits&quot;</span>, <span class="st">&quot;HmRun&quot;</span>, <span class="st">&quot;Runs&quot;</span>, <span class="st">&quot;RBI&quot;</span>, <span class="st">&quot;Walks&quot;</span>, <span class="st">&quot;CAtBat&quot;</span>, <span class="st">&quot;CHits&quot;</span>, <span class="st">&quot;CHmRun&quot;</span>, <span class="st">&quot;CRuns&quot;</span>, <span class="st">&quot;CRBI&quot;</span>, <span class="st">&quot;CWalks&quot;</span>, <span class="st">&quot;Assists&quot;</span>, <span class="st">&quot;Errors&quot;</span>])</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>df_reduced <span class="op">=</span> df_reduced.drop(columns<span class="op">=</span>[<span class="st">&quot;League&quot;</span>, <span class="st">&quot;NewLeague&quot;</span>, <span class="st">&quot;Division&quot;</span>, <span class="st">&quot;Years&quot;</span>])</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>df_reduced[<span class="st">&quot;I_PC1&quot;</span>] <span class="op">=</span> indstat_df_pca</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>df_reduced[<span class="st">&quot;C_PC2&quot;</span>] <span class="op">=</span> custat_df_pca</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>df_reduced[<span class="st">&quot;F_PC3&quot;</span>] <span class="op">=</span> fieldstat_df_pca</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>df_reduced.dropna(axis<span class="op">=</span><span class="dv">0</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode" id="cb4"><pre class="sourceCode markdown cell-code"><code class="sourceCode markdown"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>num_cols <span class="op">=</span> [col <span class="cf">for</span> col <span class="kw">in</span> df_reduced.columns <span class="cf">if</span> df_reduced[col].dtypes <span class="op">!=</span> <span class="st">&quot;O&quot;</span>]</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>corr <span class="op">=</span> df_reduced[num_cols].corr()</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>(rc<span class="op">=</span>{<span class="st">&#39;figure.figsize&#39;</span>: (<span class="dv">15</span>, <span class="dv">15</span>)})</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>sns.heatmap(corr, cmap<span class="op">=</span><span class="st">&quot;RdBu&quot;</span>, annot<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-5-output-1.png" width="1092" height="1160" /></p>
</div>
</div>
<p>After applying PCA and removing other features, I’ve reduced the data from <code>20</code> columns to just <code>5</code> columns, which is important, as collinearity can negatively effect the performance of regression models. Apart from the collinearity effect, I decided to get rid of discretely labeled binary relationships (labels 0 or 1), as this makes the linear regression model more complex and can only negatively impact the performance. I also discarded <code>years</code> as a feature because it was highly correlated with the PCA of the cumulative stats (r=<code>0.91</code>).</p>
<section id="creating-the-training-and-test-sets" class="level3">
<h3>Creating the Training and Test Sets</h3>
<p>Since we’re trying to predict salary, I extract <code>salary</code> column from the <code>df_reduced</code>, storing it into labels array <code>y</code>, and dropping that column from the feature data. The <code>X</code> data feature space has dropped from <code>5</code> columns to <code>4</code> columns, so there are <code>4</code> total features. For splitting the data into test and train, I am using sklearn’s <code>train_test_split()</code>.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode" id="cb5"><pre class="sourceCode markdown cell-code"><code class="sourceCode markdown"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df_reduced</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">&quot;Salary&quot;</span>]</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop(columns<span class="op">=</span><span class="st">&quot;Salary&quot;</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.15</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Salary STD: $</span><span class="sc">{</span>np<span class="sc">.</span>std(y) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Salary Mean: $</span><span class="sc">{</span>np<span class="sc">.</span>mean(y) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Salary Low: $</span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">min</span>(y) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Salary High: $</span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">max</span>(y) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Salary STD: $450,260.22
Salary Mean: $535,925.88
Salary Low: $67,500.00
Salary High: $2,460,000.00</code></pre>
</div>
</div>
</section>
</section>
<section id="multiple-linear-support-vector-regression-kernellinear" class="level2">
<h2>Multiple Linear Support Vector Regression (kernel=“linear”)</h2>
<p>Now, I will test out multiple linear regression using Sklearn’s <code>SVR</code> (support vector regression) class from the <code>SVM</code> library. Before passing the data into the regression model, I scaled the data using <code>StandardScaler()</code> as this is important for faster computation with regression. For the hyperparameters, I toyed with different <code>C</code> values, which influences the degree of regularization applied to the SVR model. A smaller <code>C</code> value leads to a simpler model, but a larger <code>C</code> value would fit to the training data more closely, which can potentially overfit if the number is too high. For the <code>C</code> value on the <code>linear</code> kernel, my model showed good preformance at <code>C=1</code>.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode" id="cb7"><pre class="sourceCode markdown cell-code"><code class="sourceCode markdown"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>sc <span class="op">=</span> StandardScaler()</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>X_train_scaled <span class="op">=</span> sc.fit_transform(X_train, y_train)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>X_test_scaled <span class="op">=</span> sc.fit_transform(X_test, y_test)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># pca_all = PCA(n_components=1)</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co"># X_train_scaled_pca = pca_all.fit_transform(X_train_scaled)</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># X_test_scaled_pca = pca_all.fit_transform(X_test_scaled)</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>svr_lin <span class="op">=</span> SVR(kernel<span class="op">=</span><span class="st">&quot;linear&quot;</span>, C<span class="op">=</span><span class="dv">1</span>, gamma<span class="op">=</span><span class="st">&quot;auto&quot;</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>svr_lin.fit(X_train_scaled, y_train)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> svr_lin.predict(X_test_scaled)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code></pre></div>
</div>
<section id="multiple-linear-regression-visualization" class="level3">
<h3>Multiple Linear Regression Visualization</h3>
<div class="cell" data-execution_count="7">
<div class="sourceCode" id="cb8"><pre class="sourceCode markdown cell-code"><code class="sourceCode markdown"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">12</span>))</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>independent_variables <span class="op">=</span> X_train.columns</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>dependent_variable <span class="op">=</span> <span class="st">&quot;Salary&quot;</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>X_test_numpy <span class="op">=</span> X_test.to_numpy()</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, col <span class="kw">in</span> <span class="bu">enumerate</span>(independent_variables, <span class="dv">1</span>):</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">3</span>, <span class="dv">3</span>, i)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    sns.regplot(x<span class="op">=</span>X_train[col],y<span class="op">=</span>y_train,ci<span class="op">=</span><span class="va">None</span>,color <span class="op">=</span><span class="st">&#39;red&#39;</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    sns.scatterplot(data<span class="op">=</span>X_train, x<span class="op">=</span>col, y<span class="op">=</span>y_train, color<span class="op">=</span><span class="st">&#39;blue&#39;</span>, label<span class="op">=</span><span class="st">&#39;Training Points&#39;</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    sns.scatterplot(data<span class="op">=</span>X_test, x<span class="op">=</span>col, y<span class="op">=</span>y_test, color<span class="op">=</span><span class="st">&#39;green&#39;</span>, label<span class="op">=</span><span class="st">&#39;Testing Points&#39;</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    sns.scatterplot(data<span class="op">=</span>X_test, x<span class="op">=</span>col, y<span class="op">=</span>y_pred, color<span class="op">=</span><span class="st">&#39;red&#39;</span>, label<span class="op">=</span><span class="st">&#39;Predicted Points&#39;</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f&#39;</span><span class="sc">{</span>dependent_variable<span class="sc">}</span><span class="ss"> vs. </span><span class="sc">{</span>col<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(col)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(dependent_variable)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-8-output-1.png" width="1509" height="789" /></p>
</div>
</div>
<p>Here is a visualization showing linear regression applied on all of the features in the feature space, which make up the overall prediction. For each feature, the red line represents the function applied, which in this case is linear because I’m using a linear kernel, and the points represent all the different datapoints in the dataset. <code>X_train</code> points are in blue, <code>X_test</code> points are in green with their actual label <code>y_test</code>, and predicted points are in red (the <code>X_test</code> dataset on the <code>y_pred</code>).</p>
<p>Based on the above visualization, it appears that linear regression works very well for all the features, although in any case, outliers are a problem. It is also worth noting that changing the <code>C</code> value does change the results, so modifying that <strong>may</strong> improve preformance with more fine-tuning. Below, I have outputed some metrics on the model:</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode" id="cb9"><pre class="sourceCode markdown cell-code"><code class="sourceCode markdown"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>result_df <span class="op">=</span> pd.DataFrame(columns<span class="op">=</span>[<span class="st">&quot;id&quot;</span>, <span class="st">&quot;actual&quot;</span>, <span class="st">&quot;predicted&quot;</span>])</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, actual, predicted <span class="kw">in</span> <span class="bu">zip</span>(y_test.index, y_test, y_pred):</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    entry <span class="op">=</span> [i, actual, predicted]</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    df_entry <span class="op">=</span> pd.DataFrame(entry, index<span class="op">=</span>[<span class="st">&quot;id&quot;</span>, <span class="st">&quot;actual&quot;</span>, <span class="st">&quot;predicted&quot;</span>]).T</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    result_df <span class="op">=</span> pd.concat((result_df, df_entry))</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co">#print(result_df)</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>difference <span class="op">=</span> <span class="bu">abs</span>(result_df[<span class="st">&quot;actual&quot;</span>] <span class="op">-</span> result_df[<span class="st">&quot;predicted&quot;</span>])</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Cumulative Difference: $</span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">sum</span>(difference) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Min Difference: $</span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">min</span>(difference) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Max Difference: $</span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">max</span>(difference) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Average Difference: $</span><span class="sc">{</span>np<span class="sc">.</span>mean(difference) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Std Difference: $</span><span class="sc">{</span>np<span class="sc">.</span>std(difference) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Mean Squared Error: $</span><span class="sc">{</span>mean_squared_error(y_test, y_pred)<span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Cumulative Difference: $10,792,084.13
Min Difference: $19,061.95
Max Difference: $1,968,301.30
Average Difference: $269,802.10
Std Difference: $361,894.28
Mean Squared Error: $203,760.65</code></pre>
</div>
</div>
<p>Based on this data, the total difference (sum of all the differences) between the actual and predicted outputs could be better, as the average difference between the labels is around <code>270</code> = <code>$270,000</code>. The <code>MSE</code> is a common metric used in these types of problems, and my <code>MSE</code>(mean squared error) score is around 205 (<code>$205,000</code>), which is a respectable <code>MSE</code> value for this dataset due to its high standard deviation at (<code>$450,260.22</code>).</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode" id="cb11"><pre class="sourceCode markdown cell-code"><code class="sourceCode markdown"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">8</span>))</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>residuals <span class="op">=</span> result_df[<span class="st">&#39;actual&#39;</span>] <span class="op">-</span> result_df[<span class="st">&#39;predicted&#39;</span>]</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>plt.scatter(result_df[<span class="st">&#39;id&#39;</span>], residuals)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">&#39;r&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;ID&#39;</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;Residuals&#39;</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Residual Plot&#39;</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-10-output-1.png" width="680" height="677" /></p>
</div>
</div>
<p>Above is a visualization showing how far the differences are for each value in the test set (the residuals). <code>0</code> means no difference between the actual and the predicted, any number below <code>0</code> means the predicted value was higher than the actual value, and any number above <code>0</code> means the predicted value was lower than the actual value.</p>
</section>
</section>
<section id="multiple-non-linear-support-vector-regression-kernelpoly" class="level2">
<h2>Multiple Non-Linear Support Vector Regression (kernel=“poly”)</h2>
<p>Now I run SVR on a non-linear kernel and assess its comparison to a linear kernel. Since real world data has a lot of non-linearity, this comparison is worth attempting.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode" id="cb12"><pre class="sourceCode markdown cell-code"><code class="sourceCode markdown"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>svr_poly <span class="op">=</span> SVR(kernel<span class="op">=</span><span class="st">&quot;poly&quot;</span>, degree<span class="op">=</span><span class="dv">2</span>, C<span class="op">=</span><span class="dv">75</span>, gamma<span class="op">=</span><span class="st">&quot;scale&quot;</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>svr_poly.fit(X_train_scaled, y_train)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> svr_poly.predict(X_test_scaled)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>result_df <span class="op">=</span> pd.DataFrame(columns<span class="op">=</span>[<span class="st">&quot;id&quot;</span>, <span class="st">&quot;actual&quot;</span>, <span class="st">&quot;predicted&quot;</span>])</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, actual, predicted <span class="kw">in</span> <span class="bu">zip</span>(y_test.index, y_test, y_pred):</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    entry <span class="op">=</span> [i, actual, predicted]</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    df_entry <span class="op">=</span> pd.DataFrame(entry, index<span class="op">=</span>[<span class="st">&quot;id&quot;</span>, <span class="st">&quot;actual&quot;</span>, <span class="st">&quot;predicted&quot;</span>]).T</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    result_df <span class="op">=</span> pd.concat((result_df, df_entry))</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="co">#print(result_df)</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>difference <span class="op">=</span> <span class="bu">abs</span>(result_df[<span class="st">&quot;actual&quot;</span>] <span class="op">-</span> result_df[<span class="st">&quot;predicted&quot;</span>])</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Cumulative Difference: $</span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">sum</span>(difference) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Min Difference: $</span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">min</span>(difference) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Max Difference: $</span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">max</span>(difference) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Average Difference: $</span><span class="sc">{</span>np<span class="sc">.</span>mean(difference) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Std Difference: $</span><span class="sc">{</span>np<span class="sc">.</span>std(difference) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Mean Squared Error: $</span><span class="sc">{</span>mean_squared_error(y_test, y_pred)<span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Cumulative Difference: $13,385,640.30
Min Difference: $798.03
Max Difference: $1,581,505.51
Average Difference: $334,641.01
Std Difference: $330,986.90
Mean Squared Error: $221,536.94</code></pre>
</div>
</div>
<p>Based on the model run through with a polynomial kernel, the results are overall noticeably worse than the linear kernel, but not by a whole lot. Although, it is worth noting that the <code>C</code> value is crucial in this result. I was trying to balance the trade-off between conforming to the function and simplicity. For the polynomial degree, I decide to go with <code>2</code>, as <code>1</code> is linear and <code>3</code> didn’t preform as anticipated, as the function plotted didn’t represent some of the features as well as <code>2</code>.</p>
<section id="multiple-non-linear-regression-visualization" class="level3">
<h3>Multiple Non-Linear Regression Visualization</h3>
<div class="cell" data-execution_count="11">
<div class="sourceCode" id="cb14"><pre class="sourceCode markdown cell-code"><code class="sourceCode markdown"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">12</span>))</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>independent_variables <span class="op">=</span> X_train.columns</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>dependent_variable <span class="op">=</span> <span class="st">&quot;Salary&quot;</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>X_test_numpy <span class="op">=</span> X_test.to_numpy()</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, col <span class="kw">in</span> <span class="bu">enumerate</span>(independent_variables, <span class="dv">1</span>):</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">3</span>, <span class="dv">3</span>, i)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    sns.regplot(x<span class="op">=</span>X_train[col],y<span class="op">=</span>y_train,ci<span class="op">=</span><span class="va">None</span>,color <span class="op">=</span><span class="st">&#39;red&#39;</span>, order<span class="op">=</span>svr_poly.degree)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    sns.scatterplot(data<span class="op">=</span>X_train, x<span class="op">=</span>col, y<span class="op">=</span>y_train, color<span class="op">=</span><span class="st">&#39;blue&#39;</span>, label<span class="op">=</span><span class="st">&#39;Training Points&#39;</span>)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    sns.scatterplot(data<span class="op">=</span>X_test, x<span class="op">=</span>col, y<span class="op">=</span>y_test, color<span class="op">=</span><span class="st">&#39;green&#39;</span>, label<span class="op">=</span><span class="st">&#39;Testing Points&#39;</span>)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    sns.scatterplot(data<span class="op">=</span>X_test, x<span class="op">=</span>col, y<span class="op">=</span>y_pred, color<span class="op">=</span><span class="st">&#39;red&#39;</span>, label<span class="op">=</span><span class="st">&#39;Predicted Points&#39;</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f&#39;</span><span class="sc">{</span>dependent_variable<span class="sc">}</span><span class="ss"> vs. </span><span class="sc">{</span>col<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(col)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(dependent_variable)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-12-output-1.png" width="1509" height="789" /></p>
</div>
</div>
<p>Based on the above visualzation, using a polynomial function with degree <code>2</code>, shows interesting results. For the <code>Salary vs PutOuts</code> plot, the data plotted resembles a linear kernel, but in actuality it’s a very zoomed in polynomial kernel. The <code>Salary vs I_PC1</code> showed a curve which I expected. It starts off at a peak and then lowers like a parabolic function (degree 2). The <code>Salary vs C_PC2</code> plot is interesting in the sense that it’s a negative parabola; I would say this is not a fully representative curve as it seems to be fitting to the outlier at the end of the plot. Finally, the <code>Salary vs F_PC3</code> plot seems to be similar to the first plot as it resembles more of a linear kernel.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode" id="cb15"><pre class="sourceCode markdown cell-code"><code class="sourceCode markdown"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">8</span>))</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>residuals <span class="op">=</span> result_df[<span class="st">&#39;actual&#39;</span>] <span class="op">-</span> result_df[<span class="st">&#39;predicted&#39;</span>]</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>plt.scatter(result_df[<span class="st">&#39;id&#39;</span>], residuals)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">&#39;r&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;ID&#39;</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;Residuals&#39;</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Residual Plot&#39;</span>)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-13-output-1.png" width="680" height="677" /></p>
</div>
</div>
<p>Above is the residual plot for this model with the same setup as the linear kernel one.</p>
</section>
</section>
<section id="discussion-and-improvements" class="level2">
<h2>Discussion and Improvements</h2>
<p>In actuality, it appears to me that the <code>poly</code> non-linear kernel represents the data better for certain features even though it doesn’t perform as well against the <code>linear</code> kernel. I believe the tradeoff to this approach highly depends on the <code>C</code> value for both approaches and requires more hyperparameter awareness and optimization.</p>
<p>In terms of features, I believe all of these features have strong presuasion in determing an MLB player’s salary, but there is one flaw. Some players, regardless of preformance, are more “famous” than other players. It is likely that more famous players have higher salaries simply because they generate more revenue for the teams they play for, but that doesn’t necessarily mean the player’s popularity is correlated with skill. This is the reason why I believe there are outliers in this dataset. If there were a way to accurately determine popularity, that would be a key feature in predicting salary as well for this particular domain. Speaking of outliers, I would also like to point out that outliers can have a significant impact on these regression models. Depending on the case, deleting outliers may be a valid option, but that should come with caution as deleting data can result in loss of valuable information.</p>
<div id="quarto-navigation-envelope" class="hidden">
<p><span class="hidden" data-render-id="quarto-int-sidebar-title">ML-Blog-Posts</span> <span class="hidden" data-render-id="quarto-int-navbar-title">ML-Blog-Posts</span> <span class="hidden" data-render-id="quarto-int-navbar:About">About</span> <span class="hidden" data-render-id="quarto-int-navbar:/about.html">/about.html</span> <span class="hidden" data-render-id="quarto-int-navbar:https://github.com/hassledw">https://github.com/hassledw</span> <span class="hidden" data-render-id="quarto-int-navbar:https://www.linkedin.com/in/daniel-hassler-85027a21a/">https://www.linkedin.com/in/daniel-hassler-85027a21a/</span></p>
</div>
<div id="quarto-meta-markdown" class="hidden">
<p><span class="hidden" data-render-id="quarto-metatitle">ML-Blog-Posts - Comparing Multiple Linear and Non-Linear Regression on MLB Data</span> <span class="hidden" data-render-id="quarto-twittercardtitle">ML-Blog-Posts - Comparing Multiple Linear and Non-Linear Regression on MLB Data</span> <span class="hidden" data-render-id="quarto-ogcardtitle">ML-Blog-Posts - Comparing Multiple Linear and Non-Linear Regression on MLB Data</span> <span class="hidden" data-render-id="quarto-metasitename">ML-Blog-Posts</span> <span class="hidden" data-render-id="quarto-twittercarddesc"></span> <span class="hidden" data-render-id="quarto-ogcardddesc"></span></p>
</div>
<!-- -->
<div class="quarto-embedded-source-code">
<div class="sourceCode" id="cb16" data-shortcodes="false"><pre class="sourceCode markdown"><code class="sourceCode markdown"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co">  echo: fenced</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> &quot;Comparing Multiple Linear and Non-Linear Regression on MLB Data&quot;</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> &quot;Daniel Hassler&quot;</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> &quot;2023-11-14&quot;</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [code, regression]</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> &quot;MLB.png&quot;</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc-title:</span><span class="co"> &quot;Table of contents&quot;</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="co">    embed-resources: true</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a><span class="co">    code-copy: true</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="co">    code-link: true</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="co">    theme:</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a><span class="co">      dark: darkly</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a><span class="co">      light: flatly</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="co">    pdf:</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a><span class="co">      title: &quot;LinearAndNonLinearReg&quot;</span></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a><span class="co">      author: &quot;Daniel Hassler&quot;</span></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a><span class="co">      pdf-engine: &quot;C:/Program Files (x86)/wkhtmltopdf&quot;</span></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a><span class="co">  docx: default</span></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a><span class="co">  ipynb: default</span></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a><span class="co">  gfm: default</span></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a><span class="an">filters:</span></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a><span class="co">  - social-share</span></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a><span class="an">share:</span></span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a><span class="co">  permalink: &quot;https://hassledw.github.io/ML-blog-posts/posts/RegressionBlog/&quot;</span></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a><span class="co">  description: &quot;Comparing Multiple Linear and Non-Linear Regression on MLB Data&quot;</span></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a><span class="co">  twitter: true</span></span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a><span class="co">  facebook: true</span></span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a><span class="co">  reddit: true</span></span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a><span class="co">  stumble: false</span></span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a><span class="co">  tumblr: false</span></span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a><span class="co">  linkedin: true</span></span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a><span class="co">  email: true</span></span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- title: &quot;Comparing Decision Tree and Random Forest Classifier Performance&quot;</span></span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a><span class="co">format:</span></span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a><span class="co">jupyter: python3 --&gt;</span></span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a>**Author: Daniel Hassler**</span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;link</span> <span class="er">rel</span><span class="ot">=</span><span class="st">&quot;stylesheet&quot;</span> <span class="er">type</span><span class="ot">=</span><span class="st">&quot;text/css&quot;</span> <span class="er">href</span><span class="ot">=</span><span class="st">&quot;./index.css&quot;</span><span class="kw">&gt;</span></span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;div</span> <span class="er">class</span><span class="ot">=</span><span class="st">&quot;social-icons&quot;</span><span class="kw">&gt;</span></span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a>  <span class="kw">&lt;a</span> <span class="er">href</span><span class="ot">=</span><span class="st">&quot;https://github.com/hassledw&quot;</span><span class="kw">&gt;&lt;i</span> <span class="er">class</span><span class="ot">=</span><span class="st">&quot;fab fa-github&quot;</span><span class="kw">&gt;&lt;/i&gt;&lt;/a&gt;</span></span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a>  <span class="kw">&lt;a</span> <span class="er">href</span><span class="ot">=</span><span class="st">&quot;https://www.linkedin.com/in/daniel-hassler-85027a21a/&quot;</span><span class="kw">&gt;&lt;i</span> <span class="er">class</span><span class="ot">=</span><span class="st">&quot;fab fa-linkedin&quot;</span><span class="kw">&gt;&lt;/i&gt;&lt;/a&gt;</span></span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a>  <span class="co">&lt;!-- Add more social media links/icons as needed --&gt;</span></span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;/div&gt;</span></span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVR</span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data</span></span>
<span id="cb16-71"><a href="#cb16-71" aria-hidden="true" tabindex="-1"></a>With Kaggle, I was able to find an MLB (Major League Baseball) dataset consisting of player data from the 1986-1987 seasons (https://www.kaggle.com/datasets/mathchi/hitters-baseball-data). The data itself has many features consisting of individual stats for the season, cumulative career stats, fielding stats, and salary. In total, there are <span class="in">`20`</span> features with <span class="in">`322`</span> entries before preprocessing.</span>
<span id="cb16-72"><a href="#cb16-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-73"><a href="#cb16-73" aria-hidden="true" tabindex="-1"></a>The goal of this notebook is to showcase linear and non-linear regression as a way of predicting <span class="in">`salary`</span> (in thousands), a continuous variable, for my dataset. But before I run through the regression process, I have to clean the data first and figure out correlations.</span>
<span id="cb16-74"><a href="#cb16-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-75"><a href="#cb16-75" aria-hidden="true" tabindex="-1"></a>One negative influence on regression models is collinearity in the feature space. Collinearity is problematic for several reasons, including overfitting, interpretability, and inefficiency. When there are many features that are highly correlated, this will create a strong negative impact on the performance. So before plugging into the model, I analyzed the correlation matrix consisting of correlations between all the features.</span>
<span id="cb16-76"><a href="#cb16-76" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb16-77"><a href="#cb16-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-78"><a href="#cb16-78" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb16-79"><a href="#cb16-79" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&quot;./Hitters.csv&quot;</span>)</span>
<span id="cb16-80"><a href="#cb16-80" aria-hidden="true" tabindex="-1"></a>num_cols <span class="op">=</span> [col <span class="cf">for</span> col <span class="kw">in</span> df.columns <span class="cf">if</span> df[col].dtypes <span class="op">!=</span> <span class="st">&quot;O&quot;</span>]</span>
<span id="cb16-81"><a href="#cb16-81" aria-hidden="true" tabindex="-1"></a>corr <span class="op">=</span> df[num_cols].corr()</span>
<span id="cb16-82"><a href="#cb16-82" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>(rc<span class="op">=</span>{<span class="st">&#39;figure.figsize&#39;</span>: (<span class="dv">15</span>, <span class="dv">15</span>)})</span>
<span id="cb16-83"><a href="#cb16-83" aria-hidden="true" tabindex="-1"></a>sns.heatmap(corr, cmap<span class="op">=</span><span class="st">&quot;RdBu&quot;</span>, annot<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-84"><a href="#cb16-84" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb16-85"><a href="#cb16-85" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-86"><a href="#cb16-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-87"><a href="#cb16-87" aria-hidden="true" tabindex="-1"></a>Based on the above correlation matrix, I can see there&#39;s a high correlation between all the individual season player stats (<span class="in">`AtBat`</span>, <span class="in">`Hits`</span>, <span class="in">`HmRun`</span>, <span class="in">`Runs`</span>, <span class="in">`RBI`</span>, and <span class="in">`Walks`</span>), cumulative player stats (<span class="in">`CAtBat`</span>, <span class="in">`CHits`</span>, <span class="in">`CHmRun`</span>, <span class="in">`CRuns`</span>, <span class="in">`CRBI`</span>, <span class="in">`Years`</span>, and <span class="in">`CWalks`</span>), and finally some fielding stats (<span class="in">`Assists`</span>, <span class="in">`Errors`</span>).</span>
<span id="cb16-88"><a href="#cb16-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-89"><a href="#cb16-89" aria-hidden="true" tabindex="-1"></a>One way to remove these strong correlations is to run a dimensionality reduction technique. In this case, I will be using PCA (principal component analysis) seperately on those three highly correlated areas: individual stats, cumulative stats, and fielding.</span>
<span id="cb16-90"><a href="#cb16-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-91"><a href="#cb16-91" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb16-92"><a href="#cb16-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-93"><a href="#cb16-93" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb16-94"><a href="#cb16-94" aria-hidden="true" tabindex="-1"></a>pca_custat <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-95"><a href="#cb16-95" aria-hidden="true" tabindex="-1"></a>pca_indstat <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-96"><a href="#cb16-96" aria-hidden="true" tabindex="-1"></a>pca_fieldstat <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-97"><a href="#cb16-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-98"><a href="#cb16-98" aria-hidden="true" tabindex="-1"></a>custat_df <span class="op">=</span> df[[<span class="st">&quot;CAtBat&quot;</span>, <span class="st">&quot;CHits&quot;</span>, <span class="st">&quot;CHmRun&quot;</span>, <span class="st">&quot;CRuns&quot;</span>, <span class="st">&quot;CRBI&quot;</span>, <span class="st">&quot;CWalks&quot;</span>]]</span>
<span id="cb16-99"><a href="#cb16-99" aria-hidden="true" tabindex="-1"></a>indstat_df <span class="op">=</span> df[[<span class="st">&quot;AtBat&quot;</span>, <span class="st">&quot;Hits&quot;</span>, <span class="st">&quot;HmRun&quot;</span>, <span class="st">&quot;Runs&quot;</span>, <span class="st">&quot;RBI&quot;</span>, <span class="st">&quot;Walks&quot;</span>]]</span>
<span id="cb16-100"><a href="#cb16-100" aria-hidden="true" tabindex="-1"></a>fieldstat_df <span class="op">=</span> df[[<span class="st">&quot;Assists&quot;</span>, <span class="st">&quot;Errors&quot;</span>]]</span>
<span id="cb16-101"><a href="#cb16-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-102"><a href="#cb16-102" aria-hidden="true" tabindex="-1"></a>custat_df_pca <span class="op">=</span> pca_custat.fit_transform(custat_df)</span>
<span id="cb16-103"><a href="#cb16-103" aria-hidden="true" tabindex="-1"></a>indstat_df_pca <span class="op">=</span> pca_indstat.fit_transform(indstat_df)</span>
<span id="cb16-104"><a href="#cb16-104" aria-hidden="true" tabindex="-1"></a>fieldstat_df_pca <span class="op">=</span> pca_fieldstat.fit_transform(fieldstat_df)</span>
<span id="cb16-105"><a href="#cb16-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-106"><a href="#cb16-106" aria-hidden="true" tabindex="-1"></a>df_reduced <span class="op">=</span> df.drop(columns<span class="op">=</span>[<span class="st">&quot;AtBat&quot;</span>, <span class="st">&quot;Hits&quot;</span>, <span class="st">&quot;HmRun&quot;</span>, <span class="st">&quot;Runs&quot;</span>, <span class="st">&quot;RBI&quot;</span>, <span class="st">&quot;Walks&quot;</span>, <span class="st">&quot;CAtBat&quot;</span>, <span class="st">&quot;CHits&quot;</span>, <span class="st">&quot;CHmRun&quot;</span>, <span class="st">&quot;CRuns&quot;</span>, <span class="st">&quot;CRBI&quot;</span>, <span class="st">&quot;CWalks&quot;</span>, <span class="st">&quot;Assists&quot;</span>, <span class="st">&quot;Errors&quot;</span>])</span>
<span id="cb16-107"><a href="#cb16-107" aria-hidden="true" tabindex="-1"></a>df_reduced <span class="op">=</span> df_reduced.drop(columns<span class="op">=</span>[<span class="st">&quot;League&quot;</span>, <span class="st">&quot;NewLeague&quot;</span>, <span class="st">&quot;Division&quot;</span>, <span class="st">&quot;Years&quot;</span>])</span>
<span id="cb16-108"><a href="#cb16-108" aria-hidden="true" tabindex="-1"></a>df_reduced[<span class="st">&quot;I_PC1&quot;</span>] <span class="op">=</span> indstat_df_pca</span>
<span id="cb16-109"><a href="#cb16-109" aria-hidden="true" tabindex="-1"></a>df_reduced[<span class="st">&quot;C_PC2&quot;</span>] <span class="op">=</span> custat_df_pca</span>
<span id="cb16-110"><a href="#cb16-110" aria-hidden="true" tabindex="-1"></a>df_reduced[<span class="st">&quot;F_PC3&quot;</span>] <span class="op">=</span> fieldstat_df_pca</span>
<span id="cb16-111"><a href="#cb16-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-112"><a href="#cb16-112" aria-hidden="true" tabindex="-1"></a>df_reduced.dropna(axis<span class="op">=</span><span class="dv">0</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-113"><a href="#cb16-113" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-114"><a href="#cb16-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-115"><a href="#cb16-115" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb16-116"><a href="#cb16-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-117"><a href="#cb16-117" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb16-118"><a href="#cb16-118" aria-hidden="true" tabindex="-1"></a>num_cols <span class="op">=</span> [col <span class="cf">for</span> col <span class="kw">in</span> df_reduced.columns <span class="cf">if</span> df_reduced[col].dtypes <span class="op">!=</span> <span class="st">&quot;O&quot;</span>]</span>
<span id="cb16-119"><a href="#cb16-119" aria-hidden="true" tabindex="-1"></a>corr <span class="op">=</span> df_reduced[num_cols].corr()</span>
<span id="cb16-120"><a href="#cb16-120" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>(rc<span class="op">=</span>{<span class="st">&#39;figure.figsize&#39;</span>: (<span class="dv">15</span>, <span class="dv">15</span>)})</span>
<span id="cb16-121"><a href="#cb16-121" aria-hidden="true" tabindex="-1"></a>sns.heatmap(corr, cmap<span class="op">=</span><span class="st">&quot;RdBu&quot;</span>, annot<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-122"><a href="#cb16-122" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb16-123"><a href="#cb16-123" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-124"><a href="#cb16-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-125"><a href="#cb16-125" aria-hidden="true" tabindex="-1"></a>After applying PCA and removing other features, I&#39;ve reduced the data from <span class="in">`20`</span> columns to just <span class="in">`5`</span> columns, which is important, as collinearity can negatively effect the performance of regression models. Apart from the collinearity effect, I decided to get rid of discretely labeled binary relationships (labels 0 or 1), as this makes the linear regression model more complex and can only negatively impact the performance. I also discarded <span class="in">`years`</span> as a feature because it was highly correlated with the PCA of the cumulative stats (r=<span class="in">`0.91`</span>).</span>
<span id="cb16-126"><a href="#cb16-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-127"><a href="#cb16-127" aria-hidden="true" tabindex="-1"></a><span class="fu">### Creating the Training and Test Sets</span></span>
<span id="cb16-128"><a href="#cb16-128" aria-hidden="true" tabindex="-1"></a>Since we&#39;re trying to predict salary, I extract <span class="in">`salary`</span> column from the <span class="in">`df_reduced`</span>, storing it into labels array <span class="in">`y`</span>, and dropping that column from the feature data. The <span class="in">`X`</span> data feature space has dropped from <span class="in">`5`</span> columns to <span class="in">`4`</span> columns, so there are <span class="in">`4`</span> total features. For splitting the data into test and train, I am using sklearn&#39;s <span class="in">`train_test_split()`</span>.</span>
<span id="cb16-129"><a href="#cb16-129" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb16-130"><a href="#cb16-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-131"><a href="#cb16-131" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb16-132"><a href="#cb16-132" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df_reduced</span>
<span id="cb16-133"><a href="#cb16-133" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">&quot;Salary&quot;</span>]</span>
<span id="cb16-134"><a href="#cb16-134" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop(columns<span class="op">=</span><span class="st">&quot;Salary&quot;</span>)</span>
<span id="cb16-135"><a href="#cb16-135" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.15</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb16-136"><a href="#cb16-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-137"><a href="#cb16-137" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Salary STD: $</span><span class="sc">{</span>np<span class="sc">.</span>std(y) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-138"><a href="#cb16-138" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Salary Mean: $</span><span class="sc">{</span>np<span class="sc">.</span>mean(y) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-139"><a href="#cb16-139" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Salary Low: $</span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">min</span>(y) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-140"><a href="#cb16-140" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Salary High: $</span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">max</span>(y) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-141"><a href="#cb16-141" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-142"><a href="#cb16-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-143"><a href="#cb16-143" aria-hidden="true" tabindex="-1"></a><span class="fu">## Multiple Linear Support Vector Regression (kernel=&quot;linear&quot;)</span></span>
<span id="cb16-144"><a href="#cb16-144" aria-hidden="true" tabindex="-1"></a>Now, I will test out multiple linear regression using Sklearn&#39;s <span class="in">`SVR`</span> (support vector regression) class from the <span class="in">`SVM`</span> library. Before passing the data into the regression model, I scaled the data using <span class="in">`StandardScaler()`</span> as this is important for faster computation with regression. For the hyperparameters, I toyed with different <span class="in">`C`</span> values, which influences the degree of regularization applied to the SVR model. A smaller <span class="in">`C`</span> value leads to a simpler model, but a larger <span class="in">`C`</span> value would fit to the training data more closely, which can potentially overfit if the number is too high. For the <span class="in">`C`</span> value on the <span class="in">`linear`</span> kernel, my model showed good preformance at <span class="in">`C=1`</span>. </span>
<span id="cb16-145"><a href="#cb16-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-146"><a href="#cb16-146" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb16-147"><a href="#cb16-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-148"><a href="#cb16-148" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb16-149"><a href="#cb16-149" aria-hidden="true" tabindex="-1"></a>sc <span class="op">=</span> StandardScaler()</span>
<span id="cb16-150"><a href="#cb16-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-151"><a href="#cb16-151" aria-hidden="true" tabindex="-1"></a>X_train_scaled <span class="op">=</span> sc.fit_transform(X_train, y_train)</span>
<span id="cb16-152"><a href="#cb16-152" aria-hidden="true" tabindex="-1"></a>X_test_scaled <span class="op">=</span> sc.fit_transform(X_test, y_test)</span>
<span id="cb16-153"><a href="#cb16-153" aria-hidden="true" tabindex="-1"></a><span class="co"># pca_all = PCA(n_components=1)</span></span>
<span id="cb16-154"><a href="#cb16-154" aria-hidden="true" tabindex="-1"></a><span class="co"># X_train_scaled_pca = pca_all.fit_transform(X_train_scaled)</span></span>
<span id="cb16-155"><a href="#cb16-155" aria-hidden="true" tabindex="-1"></a><span class="co"># X_test_scaled_pca = pca_all.fit_transform(X_test_scaled)</span></span>
<span id="cb16-156"><a href="#cb16-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-157"><a href="#cb16-157" aria-hidden="true" tabindex="-1"></a>svr_lin <span class="op">=</span> SVR(kernel<span class="op">=</span><span class="st">&quot;linear&quot;</span>, C<span class="op">=</span><span class="dv">1</span>, gamma<span class="op">=</span><span class="st">&quot;auto&quot;</span>)</span>
<span id="cb16-158"><a href="#cb16-158" aria-hidden="true" tabindex="-1"></a>svr_lin.fit(X_train_scaled, y_train)</span>
<span id="cb16-159"><a href="#cb16-159" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> svr_lin.predict(X_test_scaled)</span>
<span id="cb16-160"><a href="#cb16-160" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-161"><a href="#cb16-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-162"><a href="#cb16-162" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multiple Linear Regression Visualization</span></span>
<span id="cb16-163"><a href="#cb16-163" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb16-164"><a href="#cb16-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-165"><a href="#cb16-165" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb16-166"><a href="#cb16-166" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">12</span>))</span>
<span id="cb16-167"><a href="#cb16-167" aria-hidden="true" tabindex="-1"></a>independent_variables <span class="op">=</span> X_train.columns</span>
<span id="cb16-168"><a href="#cb16-168" aria-hidden="true" tabindex="-1"></a>dependent_variable <span class="op">=</span> <span class="st">&quot;Salary&quot;</span></span>
<span id="cb16-169"><a href="#cb16-169" aria-hidden="true" tabindex="-1"></a>X_test_numpy <span class="op">=</span> X_test.to_numpy()</span>
<span id="cb16-170"><a href="#cb16-170" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, col <span class="kw">in</span> <span class="bu">enumerate</span>(independent_variables, <span class="dv">1</span>):</span>
<span id="cb16-171"><a href="#cb16-171" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">3</span>, <span class="dv">3</span>, i)</span>
<span id="cb16-172"><a href="#cb16-172" aria-hidden="true" tabindex="-1"></a>    sns.regplot(x<span class="op">=</span>X_train[col],y<span class="op">=</span>y_train,ci<span class="op">=</span><span class="va">None</span>,color <span class="op">=</span><span class="st">&#39;red&#39;</span>)</span>
<span id="cb16-173"><a href="#cb16-173" aria-hidden="true" tabindex="-1"></a>    sns.scatterplot(data<span class="op">=</span>X_train, x<span class="op">=</span>col, y<span class="op">=</span>y_train, color<span class="op">=</span><span class="st">&#39;blue&#39;</span>, label<span class="op">=</span><span class="st">&#39;Training Points&#39;</span>)</span>
<span id="cb16-174"><a href="#cb16-174" aria-hidden="true" tabindex="-1"></a>    sns.scatterplot(data<span class="op">=</span>X_test, x<span class="op">=</span>col, y<span class="op">=</span>y_test, color<span class="op">=</span><span class="st">&#39;green&#39;</span>, label<span class="op">=</span><span class="st">&#39;Testing Points&#39;</span>)</span>
<span id="cb16-175"><a href="#cb16-175" aria-hidden="true" tabindex="-1"></a>    sns.scatterplot(data<span class="op">=</span>X_test, x<span class="op">=</span>col, y<span class="op">=</span>y_pred, color<span class="op">=</span><span class="st">&#39;red&#39;</span>, label<span class="op">=</span><span class="st">&#39;Predicted Points&#39;</span>)</span>
<span id="cb16-176"><a href="#cb16-176" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f&#39;</span><span class="sc">{</span>dependent_variable<span class="sc">}</span><span class="ss"> vs. </span><span class="sc">{</span>col<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb16-177"><a href="#cb16-177" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(col)</span>
<span id="cb16-178"><a href="#cb16-178" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(dependent_variable)</span>
<span id="cb16-179"><a href="#cb16-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-180"><a href="#cb16-180" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb16-181"><a href="#cb16-181" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb16-182"><a href="#cb16-182" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-183"><a href="#cb16-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-184"><a href="#cb16-184" aria-hidden="true" tabindex="-1"></a>Here is a visualization showing linear regression applied on all of the features in the feature space, which make up the overall prediction. For each feature, the red line represents the function applied, which in this case is linear because I&#39;m using a linear kernel, and the points represent all the different datapoints in the dataset. <span class="in">`X_train`</span> points are in blue, <span class="in">`X_test`</span> points are in green with their actual label <span class="in">`y_test`</span>, and predicted points are in red (the <span class="in">`X_test`</span> dataset on the <span class="in">`y_pred`</span>).</span>
<span id="cb16-185"><a href="#cb16-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-186"><a href="#cb16-186" aria-hidden="true" tabindex="-1"></a>Based on the above visualization, it appears that linear regression works very well for all the features, although in any case, outliers are a problem. It is also worth noting that changing the <span class="in">`C`</span> value does change the results, so modifying that **may** improve preformance with more fine-tuning. Below, I have outputed some metrics on the model:</span>
<span id="cb16-187"><a href="#cb16-187" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb16-188"><a href="#cb16-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-189"><a href="#cb16-189" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb16-190"><a href="#cb16-190" aria-hidden="true" tabindex="-1"></a>result_df <span class="op">=</span> pd.DataFrame(columns<span class="op">=</span>[<span class="st">&quot;id&quot;</span>, <span class="st">&quot;actual&quot;</span>, <span class="st">&quot;predicted&quot;</span>])</span>
<span id="cb16-191"><a href="#cb16-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-192"><a href="#cb16-192" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, actual, predicted <span class="kw">in</span> <span class="bu">zip</span>(y_test.index, y_test, y_pred):</span>
<span id="cb16-193"><a href="#cb16-193" aria-hidden="true" tabindex="-1"></a>    entry <span class="op">=</span> [i, actual, predicted]</span>
<span id="cb16-194"><a href="#cb16-194" aria-hidden="true" tabindex="-1"></a>    df_entry <span class="op">=</span> pd.DataFrame(entry, index<span class="op">=</span>[<span class="st">&quot;id&quot;</span>, <span class="st">&quot;actual&quot;</span>, <span class="st">&quot;predicted&quot;</span>]).T</span>
<span id="cb16-195"><a href="#cb16-195" aria-hidden="true" tabindex="-1"></a>    result_df <span class="op">=</span> pd.concat((result_df, df_entry))</span>
<span id="cb16-196"><a href="#cb16-196" aria-hidden="true" tabindex="-1"></a><span class="co">#print(result_df)</span></span>
<span id="cb16-197"><a href="#cb16-197" aria-hidden="true" tabindex="-1"></a>difference <span class="op">=</span> <span class="bu">abs</span>(result_df[<span class="st">&quot;actual&quot;</span>] <span class="op">-</span> result_df[<span class="st">&quot;predicted&quot;</span>])</span>
<span id="cb16-198"><a href="#cb16-198" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Cumulative Difference: $</span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">sum</span>(difference) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-199"><a href="#cb16-199" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Min Difference: $</span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">min</span>(difference) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-200"><a href="#cb16-200" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Max Difference: $</span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">max</span>(difference) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-201"><a href="#cb16-201" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Average Difference: $</span><span class="sc">{</span>np<span class="sc">.</span>mean(difference) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-202"><a href="#cb16-202" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Std Difference: $</span><span class="sc">{</span>np<span class="sc">.</span>std(difference) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-203"><a href="#cb16-203" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Mean Squared Error: $</span><span class="sc">{</span>mean_squared_error(y_test, y_pred)<span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-204"><a href="#cb16-204" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-205"><a href="#cb16-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-206"><a href="#cb16-206" aria-hidden="true" tabindex="-1"></a>Based on this data, the total difference (sum of all the differences) between the actual and predicted outputs could be better, as the average difference between the labels is around <span class="in">`270`</span> = <span class="in">`$270,000`</span>. The <span class="in">`MSE`</span> is a common metric used in these types of problems, and my <span class="in">`MSE`</span>(mean squared error) score is around 205 (<span class="in">`$205,000`</span>), which is a respectable <span class="in">`MSE`</span> value for this dataset due to its high standard deviation at (<span class="in">`$450,260.22`</span>).</span>
<span id="cb16-207"><a href="#cb16-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-208"><a href="#cb16-208" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb16-209"><a href="#cb16-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-210"><a href="#cb16-210" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb16-211"><a href="#cb16-211" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">8</span>))</span>
<span id="cb16-212"><a href="#cb16-212" aria-hidden="true" tabindex="-1"></a>residuals <span class="op">=</span> result_df[<span class="st">&#39;actual&#39;</span>] <span class="op">-</span> result_df[<span class="st">&#39;predicted&#39;</span>]</span>
<span id="cb16-213"><a href="#cb16-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-214"><a href="#cb16-214" aria-hidden="true" tabindex="-1"></a>plt.scatter(result_df[<span class="st">&#39;id&#39;</span>], residuals)</span>
<span id="cb16-215"><a href="#cb16-215" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">&#39;r&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>)</span>
<span id="cb16-216"><a href="#cb16-216" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;ID&#39;</span>)</span>
<span id="cb16-217"><a href="#cb16-217" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;Residuals&#39;</span>)</span>
<span id="cb16-218"><a href="#cb16-218" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Residual Plot&#39;</span>)</span>
<span id="cb16-219"><a href="#cb16-219" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb16-220"><a href="#cb16-220" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-221"><a href="#cb16-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-222"><a href="#cb16-222" aria-hidden="true" tabindex="-1"></a>Above is a visualization showing how far the differences are for each value in the test set (the residuals). <span class="in">`0`</span> means no difference between the actual and the predicted, any number below <span class="in">`0`</span> means the predicted value was higher than the actual value, and any number above <span class="in">`0`</span> means the predicted value was lower than the actual value.</span>
<span id="cb16-223"><a href="#cb16-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-224"><a href="#cb16-224" aria-hidden="true" tabindex="-1"></a><span class="fu">## Multiple Non-Linear Support Vector Regression (kernel=&quot;poly&quot;)</span></span>
<span id="cb16-225"><a href="#cb16-225" aria-hidden="true" tabindex="-1"></a>Now I run SVR on a non-linear kernel and assess its comparison to a linear kernel. Since real world data has a lot of non-linearity, this comparison is worth attempting.</span>
<span id="cb16-226"><a href="#cb16-226" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb16-227"><a href="#cb16-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-228"><a href="#cb16-228" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb16-229"><a href="#cb16-229" aria-hidden="true" tabindex="-1"></a>svr_poly <span class="op">=</span> SVR(kernel<span class="op">=</span><span class="st">&quot;poly&quot;</span>, degree<span class="op">=</span><span class="dv">2</span>, C<span class="op">=</span><span class="dv">75</span>, gamma<span class="op">=</span><span class="st">&quot;scale&quot;</span>)</span>
<span id="cb16-230"><a href="#cb16-230" aria-hidden="true" tabindex="-1"></a>svr_poly.fit(X_train_scaled, y_train)</span>
<span id="cb16-231"><a href="#cb16-231" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> svr_poly.predict(X_test_scaled)</span>
<span id="cb16-232"><a href="#cb16-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-233"><a href="#cb16-233" aria-hidden="true" tabindex="-1"></a>result_df <span class="op">=</span> pd.DataFrame(columns<span class="op">=</span>[<span class="st">&quot;id&quot;</span>, <span class="st">&quot;actual&quot;</span>, <span class="st">&quot;predicted&quot;</span>])</span>
<span id="cb16-234"><a href="#cb16-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-235"><a href="#cb16-235" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, actual, predicted <span class="kw">in</span> <span class="bu">zip</span>(y_test.index, y_test, y_pred):</span>
<span id="cb16-236"><a href="#cb16-236" aria-hidden="true" tabindex="-1"></a>    entry <span class="op">=</span> [i, actual, predicted]</span>
<span id="cb16-237"><a href="#cb16-237" aria-hidden="true" tabindex="-1"></a>    df_entry <span class="op">=</span> pd.DataFrame(entry, index<span class="op">=</span>[<span class="st">&quot;id&quot;</span>, <span class="st">&quot;actual&quot;</span>, <span class="st">&quot;predicted&quot;</span>]).T</span>
<span id="cb16-238"><a href="#cb16-238" aria-hidden="true" tabindex="-1"></a>    result_df <span class="op">=</span> pd.concat((result_df, df_entry))</span>
<span id="cb16-239"><a href="#cb16-239" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-240"><a href="#cb16-240" aria-hidden="true" tabindex="-1"></a><span class="co">#print(result_df)</span></span>
<span id="cb16-241"><a href="#cb16-241" aria-hidden="true" tabindex="-1"></a>difference <span class="op">=</span> <span class="bu">abs</span>(result_df[<span class="st">&quot;actual&quot;</span>] <span class="op">-</span> result_df[<span class="st">&quot;predicted&quot;</span>])</span>
<span id="cb16-242"><a href="#cb16-242" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Cumulative Difference: $</span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">sum</span>(difference) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-243"><a href="#cb16-243" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Min Difference: $</span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">min</span>(difference) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-244"><a href="#cb16-244" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Max Difference: $</span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">max</span>(difference) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-245"><a href="#cb16-245" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Average Difference: $</span><span class="sc">{</span>np<span class="sc">.</span>mean(difference) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-246"><a href="#cb16-246" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Std Difference: $</span><span class="sc">{</span>np<span class="sc">.</span>std(difference) <span class="op">*</span> <span class="dv">1000</span><span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-247"><a href="#cb16-247" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Mean Squared Error: $</span><span class="sc">{</span>mean_squared_error(y_test, y_pred)<span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-248"><a href="#cb16-248" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-249"><a href="#cb16-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-250"><a href="#cb16-250" aria-hidden="true" tabindex="-1"></a>Based on the model run through with a polynomial kernel, the results are overall noticeably worse than the linear kernel, but not by a whole lot. Although, it is worth noting that the <span class="in">`C`</span> value is crucial in this result. I was trying to balance the trade-off between conforming to the function and simplicity. For the polynomial degree, I decide to go with <span class="in">`2`</span>, as <span class="in">`1`</span> is linear and <span class="in">`3`</span> didn&#39;t preform as anticipated, as the function plotted didn&#39;t represent some of the features as well as <span class="in">`2`</span>.</span>
<span id="cb16-251"><a href="#cb16-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-252"><a href="#cb16-252" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multiple Non-Linear Regression Visualization</span></span>
<span id="cb16-253"><a href="#cb16-253" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb16-254"><a href="#cb16-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-255"><a href="#cb16-255" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb16-256"><a href="#cb16-256" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">12</span>))</span>
<span id="cb16-257"><a href="#cb16-257" aria-hidden="true" tabindex="-1"></a>independent_variables <span class="op">=</span> X_train.columns</span>
<span id="cb16-258"><a href="#cb16-258" aria-hidden="true" tabindex="-1"></a>dependent_variable <span class="op">=</span> <span class="st">&quot;Salary&quot;</span></span>
<span id="cb16-259"><a href="#cb16-259" aria-hidden="true" tabindex="-1"></a>X_test_numpy <span class="op">=</span> X_test.to_numpy()</span>
<span id="cb16-260"><a href="#cb16-260" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, col <span class="kw">in</span> <span class="bu">enumerate</span>(independent_variables, <span class="dv">1</span>):</span>
<span id="cb16-261"><a href="#cb16-261" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">3</span>, <span class="dv">3</span>, i)</span>
<span id="cb16-262"><a href="#cb16-262" aria-hidden="true" tabindex="-1"></a>    sns.regplot(x<span class="op">=</span>X_train[col],y<span class="op">=</span>y_train,ci<span class="op">=</span><span class="va">None</span>,color <span class="op">=</span><span class="st">&#39;red&#39;</span>, order<span class="op">=</span>svr_poly.degree)</span>
<span id="cb16-263"><a href="#cb16-263" aria-hidden="true" tabindex="-1"></a>    sns.scatterplot(data<span class="op">=</span>X_train, x<span class="op">=</span>col, y<span class="op">=</span>y_train, color<span class="op">=</span><span class="st">&#39;blue&#39;</span>, label<span class="op">=</span><span class="st">&#39;Training Points&#39;</span>)</span>
<span id="cb16-264"><a href="#cb16-264" aria-hidden="true" tabindex="-1"></a>    sns.scatterplot(data<span class="op">=</span>X_test, x<span class="op">=</span>col, y<span class="op">=</span>y_test, color<span class="op">=</span><span class="st">&#39;green&#39;</span>, label<span class="op">=</span><span class="st">&#39;Testing Points&#39;</span>)</span>
<span id="cb16-265"><a href="#cb16-265" aria-hidden="true" tabindex="-1"></a>    sns.scatterplot(data<span class="op">=</span>X_test, x<span class="op">=</span>col, y<span class="op">=</span>y_pred, color<span class="op">=</span><span class="st">&#39;red&#39;</span>, label<span class="op">=</span><span class="st">&#39;Predicted Points&#39;</span>)</span>
<span id="cb16-266"><a href="#cb16-266" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f&#39;</span><span class="sc">{</span>dependent_variable<span class="sc">}</span><span class="ss"> vs. </span><span class="sc">{</span>col<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb16-267"><a href="#cb16-267" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(col)</span>
<span id="cb16-268"><a href="#cb16-268" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(dependent_variable)</span>
<span id="cb16-269"><a href="#cb16-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-270"><a href="#cb16-270" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb16-271"><a href="#cb16-271" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb16-272"><a href="#cb16-272" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-273"><a href="#cb16-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-274"><a href="#cb16-274" aria-hidden="true" tabindex="-1"></a>Based on the above visualzation, using a polynomial function with degree <span class="in">`2`</span>, shows interesting results. For the <span class="in">`Salary vs PutOuts`</span> plot, the data plotted resembles a linear kernel, but in actuality it&#39;s a very zoomed in polynomial kernel. The <span class="in">`Salary vs I_PC1`</span> showed a curve which I expected. It starts off at a peak and then lowers like a parabolic function (degree 2). The <span class="in">`Salary vs C_PC2`</span> plot is interesting in the sense that it&#39;s a negative parabola; I would say this is not a fully representative curve as it seems to be fitting to the outlier at the end of the plot. Finally, the <span class="in">`Salary vs F_PC3`</span> plot seems to be similar to the first plot as it resembles more of a linear kernel.</span>
<span id="cb16-275"><a href="#cb16-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-276"><a href="#cb16-276" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb16-277"><a href="#cb16-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-278"><a href="#cb16-278" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb16-279"><a href="#cb16-279" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">8</span>))</span>
<span id="cb16-280"><a href="#cb16-280" aria-hidden="true" tabindex="-1"></a>residuals <span class="op">=</span> result_df[<span class="st">&#39;actual&#39;</span>] <span class="op">-</span> result_df[<span class="st">&#39;predicted&#39;</span>]</span>
<span id="cb16-281"><a href="#cb16-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-282"><a href="#cb16-282" aria-hidden="true" tabindex="-1"></a>plt.scatter(result_df[<span class="st">&#39;id&#39;</span>], residuals)</span>
<span id="cb16-283"><a href="#cb16-283" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">&#39;r&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>)</span>
<span id="cb16-284"><a href="#cb16-284" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;ID&#39;</span>)</span>
<span id="cb16-285"><a href="#cb16-285" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;Residuals&#39;</span>)</span>
<span id="cb16-286"><a href="#cb16-286" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Residual Plot&#39;</span>)</span>
<span id="cb16-287"><a href="#cb16-287" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb16-288"><a href="#cb16-288" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-289"><a href="#cb16-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-290"><a href="#cb16-290" aria-hidden="true" tabindex="-1"></a>Above is the residual plot for this model with the same setup as the linear kernel one.</span>
<span id="cb16-291"><a href="#cb16-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-292"><a href="#cb16-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-293"><a href="#cb16-293" aria-hidden="true" tabindex="-1"></a><span class="fu">## Discussion and Improvements</span></span>
<span id="cb16-294"><a href="#cb16-294" aria-hidden="true" tabindex="-1"></a>In actuality, it appears to me that the <span class="in">`poly`</span> non-linear kernel represents the data better for certain features even though it doesn&#39;t perform as well against the <span class="in">`linear`</span> kernel. I believe the tradeoff to this approach highly depends on the <span class="in">`C`</span> value for both approaches and requires more hyperparameter awareness and optimization.</span>
<span id="cb16-295"><a href="#cb16-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-296"><a href="#cb16-296" aria-hidden="true" tabindex="-1"></a>In terms of features, I believe all of these features have strong presuasion in determing an MLB player&#39;s salary, but there is one flaw. Some players, regardless of preformance, are more &quot;famous&quot; than other players. It is likely that more famous players have higher salaries simply because they generate more revenue for the teams they play for, but that doesn&#39;t necessarily mean the player&#39;s popularity is correlated with skill. This is the reason why I believe there are outliers in this dataset. If there were a way to accurately determine popularity, that would be a key feature in predicting salary as well for this particular domain. Speaking of outliers, I would also like to point out that outliers can have a significant impact on these regression models. Depending on the case, deleting outliers may be a valid option, but that should come with caution as deleting data can result in loss of valuable information. </span></code></pre></div>
</div>
</section>

</main> <!-- /main -->
<script id = "quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<div class= "page-columns page-rows-contents page-layout-article"><div class="social-share"><a href="https://twitter.com/share?url=https://hassledw.github.io/ML-blog-posts/posts/RegressionBlog/&text=Comparing Multiple Linear and Non-Linear Regression on MLB Data" target="_blank" class="twitter"><i class="fab fa-twitter fa-fw fa-lg"></i></a><a href="https://www.linkedin.com/shareArticle?url=https://hassledw.github.io/ML-blog-posts/posts/RegressionBlog/&title=Comparing Multiple Linear and Non-Linear Regression on MLB Data" target="_blank" class="linkedin"><i class="fa-brands fa-linkedin-in fa-fw fa-lg"></i></a>  <a href="mailto:?subject=Comparing Multiple Linear and Non-Linear Regression on MLB Data&body=Check out this link:https://hassledw.github.io/ML-blog-posts/posts/RegressionBlog/" target="_blank" class="email"><i class="fa-solid fa-envelope fa-fw fa-lg"></i></a><a href="https://www.facebook.com/sharer.php?u=https://hassledw.github.io/ML-blog-posts/posts/RegressionBlog/" target="_blank" class="facebook"><i class="fab fa-facebook-f fa-fw fa-lg"></i></a><a href="https://reddit.com/submit?url=https://hassledw.github.io/ML-blog-posts/posts/RegressionBlog/&title=Comparing Multiple Linear and Non-Linear Regression on MLB Data" target="_blank" class="reddit">   <i class="fa-brands fa-reddit-alien fa-fw fa-lg"></i></a></div></div>

</body>

</html>