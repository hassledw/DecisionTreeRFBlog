{
  "hash": "e1b7f75a19bf84aa6a313435d3979b0f",
  "result": {
    "markdown": "---\nexecute:\n  echo: fenced\ntitle: Comparing Decision Tree and Random Forest Classifier\ntoc: true\ntoc-title: Table of contents\nformat:\n  html:\n    embed-resources: true\n    code-copy: true\n    code-link: true\n    code-tools: true\n    theme:\n      dark: darkly\n      light: flatly\n    pdf:\n      title: DecisionTreeAndRF\n      author: Daniel Hassler\n      pdf-engine: 'C:/Program Files (x86)/wkhtmltopdf'\n  docx: default\n  ipynb: default\n  gfm: default\nfilters:\n  - social-share\nshare:\n  permalink: 'https://hassledw.github.io/ML-blog-posts/posts/DecisionTreeRFBlog/'\n  description: Comparing RandomForestClassifier and DecisionTreeClassifier on BMI data\n  twitter: true\n  facebook: true\n  reddit: true\n  stumble: false\n  tumblr: false\n  linkedin: true\n  email: true\n---\n\n<!-- title: \"Comparing Decision Tree and Random Forest Classifier Performance\"\nformat:\n  html:\n    code-fold: true\njupyter: python3 -->\n**Author: Daniel Hassler**\n\n<link rel=\"stylesheet\" type=\"text/css\" href=\"./index.css\">\n<div class=\"social-icons\">\n  <a href=\"https://github.com/hassledw\"><i class=\"fab fa-github\"></i></a>\n  <a href=\"https://www.linkedin.com/in/daniel-hassler-85027a21a/\"><i class=\"fab fa-linkedin\"></i></a>\n  <!-- Add more social media links/icons as needed -->\n</div>\n\n## Sample Data Used in Classification\nTo compare a DecisionTree and a RandomForestClassifier, the first step I took was to gather some data and run some visualizations and analysis. Through Kaggle, I was able to obtain a small dataset on person features and their BMI (Body Mass Index) data. The data consists of just around 400 samples with features: gender, height, and weight, and the goal is to predict BMI. \n\n::: {.cell execution_count=1}\n```` { .cell-code}\n```{{python}}\nimport numpy as np\nimport sklearn\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, learning_curve, LearningCurveDisplay\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n```\n\n````\n:::\n\n\n::: {.cell execution_count=2}\n```` { .cell-code}\n```{{python}}\ndata = pd.read_csv(\"./datasets/bmi_train.csv\")\ncategory_mapping = {'Male': 0, 'Female': 1}\ndata['Gender_Encoded'] = data['Gender'].map(category_mapping) # converts categorical data to numeric data.\nX = data.drop(['Gender','Index'], axis=1)\ny = data.drop(['Gender', 'Gender_Encoded', 'Height', 'Weight'], axis=1)\nprint(\"All X shape: \", X.shape)\nprint(\"All y shape: \", y.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nprint(\"X_train shape: \", X_train.shape)\nprint(\"y_train shape: \", y_train.shape)\nprint(\"X_test shape: \", X_test.shape)\nprint(\"y_test shape: \", y_test.shape)\n```\n\n````\n\n::: {.cell-output .cell-output-stdout}\n```\nAll X shape:  (400, 3)\nAll y shape:  (400, 1)\nX_train shape:  (320, 3)\ny_train shape:  (320, 1)\nX_test shape:  (80, 3)\ny_test shape:  (80, 1)\n```\n:::\n:::\n\n\nIn the above code snippet, I first populated my data into a Pandas dataframe and then split up the data into a \"training\" and \"testing\" datasets. I decided to go with an 80/20% split between train and test (with its corresponding labels), as that seems to be the most standard approach in the industry. The significant benefit here is that I possess labeled data on both sets, a challenge in practice. This enables me to make comparisons between predictions and outcomes on my data, eliminating the need to procure any additional \"test\" data.\n\n::: {.cell execution_count=3}\n```` { .cell-code}\n```{{python}}\nplt.figure(figsize=(6, 6))\nsns.scatterplot(data=data, x='Height', y='Weight', hue='Index', palette='deep')\nplt.title('Scatter Plot of Height vs Weight')\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=519 height=523}\n:::\n:::\n\n\nNext, I created a scatterplot showing the distribution of the entire dataset (n=400) to find linear associations. Based on the scatterplot above, I was roughly able to see that there was a class imbalance. \n\n::: {.cell execution_count=4}\n```` { .cell-code}\n```{{python}}\n# Class imbalance, more obesity.\nunique_values, counts = np.unique(y, return_counts=True)\nplt.bar(unique_values, counts)\nplt.title(\"BMI Classes in the Entire Dataset\")\nplt.xlabel(\"BMI Class\")\nplt.ylabel(\"Occurences in Entire Dataset\")\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=593 height=449}\n:::\n:::\n\n\nThe labels are all discrete and sequential, consisting of whole numbers between 0 and 5, further enforcing my intuition for using a classifier approach. A \"0\" in my case represents someone with an **exeptionally low** BMI, whereas a \"5\" depicts an **exceptionally high** BMI. Based on the distribution of the data, there appears to be a huge class imbalance, heavily favoring the amount of **exceptionally high** instances in the dataset; this was something I needed to keep in mind when building the classifiers for this dataset.\n\n::: {.cell execution_count=5}\n```` { .cell-code}\n```{{python}}\ncorrelation_matrix = data.corr()\n\n# Display a heatmap of the correlation matrix\nplt.figure(figsize=(10, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('Correlation Heatmap')\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_16796\\1253239144.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  correlation_matrix = data.corr()\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-2.png){width=738 height=505}\n:::\n:::\n\n\nThe correlation matrix depicts the correlation between features (height, weight, gender, BMI) in the dataset. It uses the pearson's correlation coefficient to compute this:\n$$\n r =\n  \\frac{ \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y}) }{\n        \\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^{n}(y_i-\\bar{y})^2}}\n$$\n\nBased on the features presented, most are not correlated strongly, but there is a glaring strong correlation between weight and BMI. It is also important to note that `gender` doesn't influence classification results, as the key factors to determining BMI is height and weight. \n\n## DecisionTreeClassifier\n\nIn order to start the model building process, I decided to tune the hyperparamters first by running a `GridSearch`\n\n::: {.cell execution_count=6}\n```` { .cell-code}\n```{{python}}\nparam_grid = {\n    'max_depth': [i for i in range(2, 6)],\n    'min_samples_leaf': [2 ** i for i in range(0, 6)],\n    'criterion': [\"entropy\", \"gini\"]\n}\n\ndt = DecisionTreeClassifier(random_state=42)\n\ngrid_search_dt = GridSearchCV(dt, param_grid, cv=StratifiedKFold(n_splits=5), scoring='accuracy')\ngrid_search_dt.fit(X_train, y_train)\nbest_params_dt = grid_search_dt.best_params_\nprint(\"Best Hyperparameters:\", best_params_dt)\nprint(\"Best Score:\", grid_search_dt.best_score_)\n```\n\n````\n\n::: {.cell-output .cell-output-stdout}\n```\nBest Hyperparameters: {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 8}\nBest Score: 0.78125\n```\n:::\n:::\n\n\nI recognized that `max_depth` was an important hyperparameter for the DecisionTree (DT), as the depth of the tree heavily influences overfitting, but other hyperparameters are important as well, such as:\n\n* `min_samples_leaf`: the minimum amount of samples needed in a leaf node of the DT. For example, when min_samples_leaf is set to 10, that means a node won't split if it has fewer than 10 samples. When this number is higher, the model can create a more generalized tree, although, when the number is smaller, it'll create more specific splits, resulting in a more complex tree (more potential for overfitting).\n\n* `criterion`: this hyperparameter chooses whether to use entropy or Gini index as a way to calculate dissimilarity in a node. I found that in most cases, entropy outpreformed the Gini index.\n$$\nEntropy(C) = -\\sum_{c=1}^Cp(c)\\log(p(c))\n$$\n\n$$\nGini(C) = 1 - \\sum_{c=1}^Cp(c)^2\n$$\n\nNow that I've determined the necessary hyperparameters for this classifier, I initialize the `GridSearchCV` object to analyze every combination of the above hyperparameters. Within its search, it goes through an important cross-validation step (cv) that splits the training data into multiple folds and iterates through each fold for each hyperparameter combination.\n\nThere were a few options I could've chose from for the cv parameter in `GridSearchCV`, but in order to account for class imbalance like I stated earlier, I decided to go with a `StratifiedKFold` cross-validator. StratifiedKFold accounts for class label imbalance by keeping an equal precentage of classes for training and testing represented in the dataset. Below is a picture representing this:\n\n<img style=\"display: block;\n    margin-left: auto;\n    margin-right: auto;\"\n    height=\"300\" width=\"300\" src=\"https://amueller.github.io/aml/_images/stratified_cv.png\"></img>\n\n::: {.cell execution_count=7}\n```` { .cell-code}\n```{{python}}\ndt = DecisionTreeClassifier(max_depth=best_params_dt[\"max_depth\"], min_samples_leaf=best_params_dt[\"min_samples_leaf\"], criterion=best_params_dt[\"criterion\"])\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\n```\n\n````\n:::\n\n\nI then created a `DecisionTreeClassifier` with the 'best' tuned hyperparameters from the above grid search and populated the `y_pred` array with the predictions from the test dataset. After that, I plotted the tree out using Sklearn's plot_tree method.\n\n::: {.cell execution_count=8}\n```` { .cell-code}\n```{{python}}\nplt.figure(figsize=(10, 10))\nplot_tree(dt, feature_names=X_train.columns.tolist(), class_names=['0', '1', '2','3','4','5'], filled=True)\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=763 height=758}\n:::\n:::\n\n\nAfter plotting the tree, I created a confusion matrix, showing where my predictions fell. Currently, the model sits around 75-86% accurate due to the above hyperparameter values and the randomly generated tree with those hyperparameter values. Not bad for a small dataset with class imbalance.\n\n::: {.cell execution_count=9}\n```` { .cell-code}\n```{{python}}\nprint(\"Micro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='micro'))\nprint(\"Macro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='macro'))\n```\n\n````\n\n::: {.cell-output .cell-output-stdout}\n```\nMicro F1:  0.7375\nMacro F1:  0.6487497023100738\n```\n:::\n:::\n\n\n::: {.cell execution_count=10}\n```` { .cell-code}\n```{{python}}\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nconf_df = pd.DataFrame(confusion_matrix, index=[f\"{i}\" for i in range(6)], columns=[f\"{i}\" for i in range(6)])\nheatmap = sns.heatmap(conf_df, annot=True, fmt=\"d\", linewidths=0.35, cmap=\"YlGnBu\")\nplt.title(f\"Model Predictions With {(np.sum(confusion_matrix.diagonal()) / y_test.shape[0]) * 100:.2f}% Accuracy\")\n```\n\n````\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\nText(0.5, 1.0, 'Model Predictions With 73.75% Accuracy')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-2.png){width=529 height=431}\n:::\n:::\n\n\n## RandomForestClassifer (Ensemble approach)\n\nAs above with the `DecisionTreeClassifer`, I first started to implement the `RandomForestClassifier` by tuning the hyperparameter values. Since a RandomForest is just a collection of DecisionTrees, RandomForestClassifiers, like a `DecisionTreeClassifier`, have mostly the same hyperparameters, but the `RandomForestClassifier` has an extra one for the amount of DecisionTrees that should be included in the forest (`n_estimators`).\n\nThough this step wasn't as necessary, since I already did the hyperparameter tuning part for the DecisionTree, but I decided to include it again for the RandomForest with the number of estimators.\n\nIt is important to note that the `n_estimators` hyperparameter won't cause the model to overfit. In fact, it actually does better at generalization when increasing the number of estimators due to the diversity of opinions the model presents for each unique DecisionTree. The only way overfitting can happen in a RandomForest depends on how the underlying DecisionTrees are set up, not the quantity of them.\n\n::: {.cell execution_count=11}\n```` { .cell-code}\n```{{python}}\nparam_grid = {\n    'max_depth': [i for i in range(2, 6)],\n    'min_samples_leaf': [2 ** i for i in range(0, 6)],\n    'criterion': [\"entropy\", \"gini\"]\n}\n\nrf = RandomForestClassifier(random_state=42)\n\nprint(y_train.shape)\ngrid_search_rf = GridSearchCV(rf, param_grid, cv=StratifiedKFold(n_splits=5), scoring='accuracy')\ngrid_search_rf.fit(X_train, y_train.values.ravel())\nbest_params_rf = grid_search_rf.best_params_\nprint(\"Best Hyperparameters:\", best_params_rf)\nprint(\"Best Score:\", grid_search_rf.best_score_)\n```\n\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n(320, 1)\nBest Hyperparameters: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1}\nBest Score: 0.79375\n```\n:::\n:::\n\n\n::: {.cell execution_count=12}\n```` { .cell-code}\n```{{python}}\nrf = RandomForestClassifier(n_estimators=300, \n                            max_depth=best_params_rf[\"max_depth\"], \n                            min_samples_leaf=best_params_rf[\"min_samples_leaf\"],\n                            criterion=best_params_rf[\"criterion\"])\nrf.fit(X_train, y_train.values.ravel())\ny_pred = rf.predict(X_test)\n```\n\n````\n:::\n\n\nThe above code snippet creates the `RandomForestClassifier` with the same hyperparameters as the DecisionTree, in addition to the number of estimators (number of decision trees in the forest), trains the classifier, then stores a prediction array.\n\nHere is a visualizaiton of a subset of DecisionTrees in this RandomForest:\n\n::: {.cell execution_count=13}\n```` { .cell-code}\n```{{python}}\n    fig, axes = plt.subplots(nrows = 1,ncols = 5,figsize = (10,3), dpi=250)\n    for index in range(5):\n        tree.plot_tree(rf.estimators_[index],\n                    feature_names = X_train.columns.tolist(), \n                    class_names= [f\"{i}\" for i in range(6)],\n                    filled = True,\n                    ax = axes[index])\n\n        axes[index].set_title('Estimator: ' + str(index + 1), fontsize = 10)\n    plt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-1.png){}\n:::\n:::\n\n\nAfter running the model, I checked the accuracy output of the prediction array and found that the RandomForestClassifier was able to increase the accuracy of the predictions by a considerable amount on average.\n\n::: {.cell execution_count=14}\n```` { .cell-code}\n```{{python}}\nprint(\"Micro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='micro'))\nprint(\"Macro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='macro'))\n```\n\n````\n\n::: {.cell-output .cell-output-stdout}\n```\nMicro F1:  0.85\nMacro F1:  0.7075440987205693\n```\n:::\n:::\n\n\n::: {.cell execution_count=15}\n```` { .cell-code}\n```{{python}}\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nconf_df = pd.DataFrame(confusion_matrix, index=[f\"{i}\" for i in range(6)], columns=[f\"{i}\" for i in range(6)])\nheatmap = sns.heatmap(conf_df, annot=True, fmt=\"d\", linewidths=0.35, cmap=\"YlGnBu\")\nplt.title(f\"Model Predictions With {(np.sum(confusion_matrix.diagonal()) / y_test.shape[0]) * 100:.2f}% Accuracy\")\n```\n\n````\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\nText(0.5, 1.0, 'Model Predictions With 85.00% Accuracy')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-16-output-2.png){width=529 height=431}\n:::\n:::\n\n\nFinally, I decided to calculate the accuracy preformance on multiple samples of RandomForestClassifiers and DecisionTrees at the same time and plot them out in a line chart.\n\n::: {.cell execution_count=16}\n```` { .cell-code}\n```{{python}}\n'''\nPlot a graph that compares the two models, randomly generated with tuned hyperparameter models\n'''\ndt_results = []\nrf_results = []\nn_samples = 40\nindexes = [i for i in range(n_samples)]\nfor i in indexes:\n    dt = DecisionTreeClassifier(max_depth=best_params_dt[\"max_depth\"], \n                   min_samples_leaf=best_params_dt[\"min_samples_leaf\"], criterion=best_params_dt[\"criterion\"])\n    dt.fit(X_train, y_train)\n    y_pred_dt = dt.predict(X_test)\n    \n    rf = RandomForestClassifier(n_estimators=300, \n                            max_depth=best_params_rf[\"max_depth\"], \n                            min_samples_leaf=best_params_rf[\"min_samples_leaf\"],\n                            criterion=best_params_rf[\"criterion\"])\n    rf.fit(X_train, y_train.values.ravel())\n    y_pred_rf = rf.predict(X_test)\n    \n    confusion_matrix_dt = sklearn.metrics.confusion_matrix(y_test, y_pred_dt)\n    confusion_matrix_rf = sklearn.metrics.confusion_matrix(y_test, y_pred_rf)\n    \n    dt_results.append((np.sum(confusion_matrix_dt.diagonal()) / y_test.shape[0]) * 100)\n    rf_results.append((np.sum(confusion_matrix_rf.diagonal()) / y_test.shape[0]) * 100)\n\nplt.plot(indexes, dt_results, label=\"DT results\")\nplt.plot(indexes, rf_results, label=\"RF results\")\nplt.xlabel(\"Sample\")\nplt.ylabel(\"Accuracy on Test Data in %\")\nplt.title(\"Accuracy Comparison Between DT and RF on Randomly Generated Models\")\nplt.legend()\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-1.png){width=623 height=449}\n:::\n:::\n\n\n## Model Improvements\n\nNow that I've determined RandomForestClassifier as an overall better approach for this problem, I've included more ways to improve the current implementation.\n\nEarlier, I stated that `gender` may be a redudndant feature based on the correlation matrix, so I decided to drop that in the dataset when training the model.\n\n::: {.cell execution_count=17}\n```` { .cell-code}\n```{{python}}\nprint(X_train.shape)\nprint(y_train.shape)\n\nparam_grid = {\n    'max_depth': [i for i in range(2, 6)],\n    'min_samples_leaf': [2 ** i for i in range(0, 6)],\n    'criterion': [\"entropy\", \"gini\"]\n}\n\nrf = RandomForestClassifier(random_state=42)\ngrid_search_rf = GridSearchCV(rf, param_grid, cv=StratifiedKFold(n_splits=5), scoring='accuracy')\ngrid_search_rf.fit(X_train.drop(['Gender_Encoded'], axis=1), y_train.values.ravel())\nbest_params_rf = grid_search_rf.best_params_\nprint(\"Best Hyperparameters:\", best_params_rf)\nprint(\"Best Score:\", grid_search_rf.best_score_)\n\nrf = RandomForestClassifier(n_estimators=300, \n                            max_depth=best_params_rf[\"max_depth\"], \n                            min_samples_leaf=best_params_rf[\"min_samples_leaf\"],\n                            criterion=best_params_rf[\"criterion\"])\nrf.fit(X_train.drop(['Gender_Encoded'], axis=1), y_train.values.ravel())\ny_pred = rf.predict(X_test.drop(['Gender_Encoded'], axis=1))\n```\n\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n(320, 3)\n(320, 1)\nBest Hyperparameters: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1}\nBest Score: 0.85\n```\n:::\n:::\n\n\n::: {.cell execution_count=18}\n```` { .cell-code}\n```{{python}}\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nconf_df = pd.DataFrame(confusion_matrix, index=[f\"{i}\" for i in range(6)], columns=[f\"{i}\" for i in range(6)])\nheatmap = sns.heatmap(conf_df, annot=True, fmt=\"d\", linewidths=0.35, cmap=\"YlGnBu\")\nplt.title(f\"Model Predictions With {(np.sum(confusion_matrix.diagonal()) / y_test.shape[0]) * 100:.2f}% Accuracy (NO GENDER)\")\n```\n\n````\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\nText(0.5, 1.0, 'Model Predictions With 86.25% Accuracy (NO GENDER)')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-19-output-2.png){width=529 height=431}\n:::\n:::\n\n\n::: {.cell execution_count=19}\n```` { .cell-code}\n```{{python}}\nprint(\"Micro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='micro'))\nprint(\"Macro F1: \", sklearn.metrics.f1_score(y_test, y_pred, average='macro'))\n```\n\n````\n\n::: {.cell-output .cell-output-stdout}\n```\nMicro F1:  0.8625\nMacro F1:  0.8370034682234619\n```\n:::\n:::\n\n\nIt appears that removing that feature, on average, didn't hurt the preformance of the overall model.\n\nFinally, below is a learning curve showing accuracy results in respect to the number of samples in the training set. This plot is heavily dependent on the random state of the generated RandomForestClassifier and its underlying DecisionTrees. Sometimes the model is overfitting, so I tried minimizing the hyperparameter values to make sure it mostly doesn't.\n\n::: {.cell execution_count=20}\n```` { .cell-code}\n```{{python}}\ntrain_sizes, train_scores, test_scores = learning_curve(rf, X_train, y_train.values.ravel(), cv=StratifiedKFold(n_splits=5))\ndisplay = LearningCurveDisplay(train_sizes=train_sizes,\n    train_scores=train_scores, test_scores=test_scores, score_name=\"Score\")\ndisplay.plot()\nplt.title(\"Learning Curve on RandomForestClassifer (NO GENDER)\")\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-21-output-1.png){width=589 height=449}\n:::\n:::\n\n\n## Results and Conclusions\n\nAfter doing simple experimentation with these models, I have found that, on average, the RandomForestClassifier outpreforms just a singular DecisionTreeClassifier. There are several advantages to having a forest of DecisionTrees rather than a singular tree:\n\n* More generalizability due to the ensemble approach to this problem\n\n* Limits overfitting compared to a DT\n\n* DT has high variance and instability, so having a forest of those trees in a more collective approach would help get more opinions at least.\n\nThough there is more resource complexity with a forest, the benefits of using that over a DT is worth the tradeoff. \n\n",
    "supporting": [
      "index_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}