{
  "hash": "63285e93c77cc56fc9bb8eb4be2009cf",
  "result": {
    "markdown": "---\nexecute:\n  echo: fenced\ntitle: Probability Theory with Naive Bayes Application\nauthor: Daniel Hassler\ndate: '2023-11-05'\ncategories:\n  - code\n  - probability\nimage: bayes.png\ntoc: true\ntoc-title: Table of contents\nformat:\n  html:\n    embed-resources: true\n    code-copy: true\n    code-link: true\n    code-tools: true\n    theme:\n      dark: darkly\n      light: flatly\n    pdf:\n      title: NaiveBayesProbTheory\n      author: Daniel Hassler\n      pdf-engine: 'C:/Program Files (x86)/wkhtmltopdf'\n  docx: default\n  ipynb: default\n  gfm: default\nfilters:\n  - social-share\nshare:\n  permalink: 'https://hassledw.github.io/ML-blog-posts/posts/NaiveBayesBlog/'\n  description: Probability Theory with Naive Bayes Application\n  twitter: true\n  facebook: true\n  reddit: true\n  stumble: false\n  tumblr: false\n  linkedin: true\n  email: true\n---\n\n<!-- title: \"Comparing Decision Tree and Random Forest Classifier Performance\"\nformat:\n  html:\n    code-fold: true\njupyter: python3 -->\n**Author: Daniel Hassler**\n\n<link rel=\"stylesheet\" type=\"text/css\" href=\"./index.css\">\n<div class=\"social-icons\">\n  <a href=\"https://github.com/hassledw\"><i class=\"fab fa-github\"></i></a>\n  <a href=\"https://www.linkedin.com/in/daniel-hassler-85027a21a/\"><i class=\"fab fa-linkedin\"></i></a>\n  <!-- Add more social media links/icons as needed -->\n</div>\n\n::: {.cell execution_count=1}\n```` { .cell-code}\n```{{python}}\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport mnist\n\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn.metrics import confusion_matrix\n```\n\n````\n:::\n\n\n## Data\nBefore diving into the probability theory and using Naive Bayes, I will first introduce the dataset I am using for this application.\n\nThe dataset I am using to explain probability theory with Naive Bayes is the MNIST dataset, a large dataset containing pictures/drawings of digits 0-9. There are `60,000` training images and `10,000` testing images in my particular dataset.\n\nBelow, is a visualization of one entry in the training set and one entry in the test set to show what these digits look like.\n\n::: {.cell execution_count=2}\n```` { .cell-code}\n```{{python}}\n# mnist.init()\nX_train, y_train, X_test, y_test = mnist.load()\nprint(\"X_train len: \", len(X_train))\nprint(\"X_test len: \", len(X_test))\nprint(\"X_train shape: \", X_train.shape)\nprint(\"X_test shape: \", X_test.shape)\n\nplt.subplot(1, 2, 1)\nplt.title(\"Entry in Train Set\")\nimg = X_train[0,:].reshape(28,28) # First image in the training set.\nplt.imshow(img,cmap='gray')\n\nplt.subplot(1, 2, 2)\nplt.title(\"Entry in Test Set\")\nimg = X_test[0,:].reshape(28,28) # First image in the test set.\nplt.imshow(img,cmap='gray')\nplt.show() # Show the image\n```\n\n````\n\n::: {.cell-output .cell-output-stdout}\n```\nX_train len:  60000\nX_test len:  10000\nX_train shape:  (60000, 784)\nX_test shape:  (10000, 784)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-2.png){width=566 height=298}\n:::\n:::\n\n\nHere is a visualization showing a unique entry in the `X_train` data for each digit.\n\n::: {.cell execution_count=3}\n```` { .cell-code}\n```{{python}}\nunique_values, indices = np.unique(y_train, return_index=True)\n\nfor i, label_index in enumerate(indices):\n    plt.subplot(1, len(indices), i + 1)\n    img = X_train[label_index,:].reshape(28,28)\n    plt.imshow(img,cmap='gray')\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=571 height=90}\n:::\n:::\n\n\n## Naive Bayes Background\nA fundamental concept in probability theory is **Bayes' Theorem**. The theorem is used to update the belief of an event occuring given new evidence:\n$$\n P(A|B) =\n  \\frac{ P(B|A)P(A)}{\n       P(B)}\n$$\n\nIn the theorem above, `P(A|B)` is represented as the probability of event A occurring given event B occurred. `P(B|A)` is the probability of B occurring given that event A occured. `P(A)` and `P(B)` are independent events.\n\n**Naive Bayes** is a machine learning algorithm that relies on the concept of Bayes Theorem and idea of conditional independence. In an application using Naive Bayes, we are assuming that the features are independent of each other (naive assumption), which is rarely ever true in real world scenarios, but it is a valid benchmark and has some benefits. This is the overall idea of the application of Bayes' Theorem with Naive Bayes:\n$$\n P(C|X) =\n  \\frac{ P(X|C)P(C)}{\n       P(X)}\n$$\n\nThe goal is to find the probability of class `C` given observation `X`. In our case with the MNIST dataset, `X` is the feature set that represents every pixel in the 28x28 images (784 total features) and `C` is a representation of all the classes, digits 0-9. The naive assumption with the MNIST dataset is treating the pixels as independent observations.\n\nFirst, we can get the `prior` probabilities, represented as `P(C)` from the training set itself. This is the probability occurrence of each class in the dataset. I calculated this by this equation, where `N_c` is the number of occurrences of class c and `N` is the sum of all classes occurrences.\n$$\nP(C=c) = \\frac{N_c}{N}\n$$\n\nWe can then get the likelihood probability `P(X|C)`, the probability of the feature (pixel) given class `C`. We can get this directly from the training data itself. We can get this by observing the data or by calculating the probability density function if we're assuming the data flows like a Gaussian distribution.\n\n\n`P(C|X)`, the posterior probability, represents the probability of class `C` given feature `X`. Based on this, in our prediction stage with Naive Bayes, we are taking the class with the **max** probability to get the classification.\n\n\n## Naive Bayes Classification\nFor the purposes of explaining probability theory with NB, visualizing data, speed consideration, and understanding of naive bayes on the MNIST dataset, I decided to go with sklearn's `GaussianNB` model, which is a commonly used baseline model for a lot of distributions that follow a Gaussian distribution. By chosing this model, I am assuming that my MNIST data follows a Gaussian distribution, but the data itself doesn't directly follow this assumption. So, compared to other methods like CNNs (see improvements), we will see a performance degredation with this model, as MNIST pixels are not normally distributed. Although, the performance of this model we will see is \"reasonable\" and better than expected, especially since we can still assume conditional independence between the features, which is a large assumption for Naive Bayes. \n\nFor the hyperparameter values, I gathered the dataset's `prior` distribution by simply taking the frequency of the dataset and dividing it by the sum of all the frequencies, and then passed it into the `GaussianNB` model. Another parameter I had to tune is the `var_smoothing` value. This value is used to prevent any division by zero during probability estimation. By default, sklearn sets this value to `1e-09`, but the effectiveness of this value depends on the dataset, so in the end, I found `0.1` to have the best accuracy performance.\n\n::: {.cell execution_count=4}\n```` { .cell-code}\n```{{python}}\nunique, counts = np.unique(y_train, return_counts=True)\nsum_counts = np.sum(counts)\npriors = np.divide(counts, sum_counts)\n\nnb = GaussianNB(priors=priors, var_smoothing=0.1)\nnb.fit(X_train, y_train)\ny_pred = nb.predict(X_test)\nprint(\"Priors: \", nb.priors)\n```\n\n````\n\n::: {.cell-output .cell-output-stdout}\n```\nPriors:  [0.09871667 0.11236667 0.0993     0.10218333 0.09736667 0.09035\n 0.09863333 0.10441667 0.09751667 0.09915   ]\n```\n:::\n:::\n\n\n## Results\nIn order to evaluate the preformance of the `GaussianNB` classifier, I ran a confusion matrix to visualize where the predictions fall. As you can see, we have around 81% accuracy on our given classifier, which is about what we expected for this dataset.\n\n::: {.cell execution_count=5}\n```` { .cell-code}\n```{{python}}\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nconf_df = pd.DataFrame(confusion_matrix, index=[f\"{i}\" for i in range(10)], columns=[f\"{i}\" for i in range(10)])\nheatmap = sns.heatmap(conf_df, annot=True, fmt=\"d\", linewidths=0.35, cmap=\"YlGnBu\")\nplt.title(f\"Model Predictions With {(np.sum(confusion_matrix.diagonal()) / y_test.shape[0]) * 100:.2f}% Accuracy\")\n```\n\n````\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nText(0.5, 1.0, 'Model Predictions With 81.40% Accuracy')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-2.png){width=546 height=431}\n:::\n:::\n\n\nNext, I used the `GaussianNB` theta value to extract the `mean_pixel_values`, which stores the estimated mean of each pixel for every class. This visualization plots a heatmap of each pixel, where the pixels that are highlighted in yellow have the highest mean for that digit and the dark blue being the lowest.\n\n::: {.cell execution_count=6}\n```` { .cell-code}\n```{{python}}\nmean_pixel_values = nb.theta_\nplt.figure(figsize=(5,10))\nfor i, digit in enumerate(range(10)):\n    plt.subplot(len(indices) // 2, 2, i + 1)\n    plt.title(f\"Digit {digit}\")\n    plt.axis('off')\n    img = mean_pixel_values[digit].reshape(28,28)\n    plt.imshow(img)\nplt.plot()\n```\n\n````\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n[]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-2.png){width=350 height=778}\n:::\n:::\n\n\n## Improvements\nThough we achieve decent accuracy with the MNIST dataset using `GaussianNB` classifier, this model has one big flaw for image classification; it only looks at the discretized values for specific points in the image. What would happen if I shifted the \"0\" or \"9\" to a different section of the image (not centered)? it would not be able to classify this case effectively.\n\nA way to fix the above limitation is using convolutional neural networks (CNNs), which is a deep learning classifier used in a lot of computer vision and even NLP related applications. Its main feature is using the idea of a \"sliding window\" to find more meaningful representations, which means the location of the object we're classifying is less important.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}