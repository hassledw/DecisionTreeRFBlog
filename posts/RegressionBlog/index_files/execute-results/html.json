{
  "hash": "00854a0fd5c0445713e5bf308a1cf3b9",
  "result": {
    "markdown": "---\nexecute:\n  echo: fenced\ntitle: Comparing Multiple Linear and Non-Linear Regression on MLB Data\ntoc: true\ntoc-title: Table of contents\nformat:\n  html:\n    embed-resources: true\n    code-copy: true\n    code-link: true\n    code-tools: true\n    theme:\n      dark: darkly\n      light: flatly\n    pdf:\n      title: LinearAndNonLinearReg\n      author: Daniel Hassler\n      pdf-engine: 'C:/Program Files (x86)/wkhtmltopdf'\n  docx: default\n  ipynb: default\n  gfm: default\nfilters:\n  - social-share\nshare:\n  permalink: 'https://hassledw.github.io/ML-blog-posts/posts/RegressionBlog/'\n  description: Comparing Multiple Linear and Non-Linear Regression on MLB Data\n  twitter: true\n  facebook: true\n  reddit: true\n  stumble: false\n  tumblr: false\n  linkedin: true\n  email: true\n---\n\n<!-- title: \"Comparing Decision Tree and Random Forest Classifier Performance\"\nformat:\n  html:\n    code-fold: true\njupyter: python3 -->\n**Author: Daniel Hassler**\n\n<link rel=\"stylesheet\" type=\"text/css\" href=\"./index.css\">\n<div class=\"social-icons\">\n  <a href=\"https://github.com/hassledw\"><i class=\"fab fa-github\"></i></a>\n  <a href=\"https://www.linkedin.com/in/daniel-hassler-85027a21a/\"><i class=\"fab fa-linkedin\"></i></a>\n  <!-- Add more social media links/icons as needed -->\n</div>\n\n::: {.cell execution_count=1}\n```` { .cell-code}\n```{{python}}\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.svm import SVR\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n```\n\n````\n:::\n\n\n## Data\nWith Kaggle, I was able to find an MLB (Major League Baseball) dataset consisting of player data from the 1986-1987 seasons (https://www.kaggle.com/datasets/mathchi/hitters-baseball-data). The data itself has many features consisting of individual stats for the season, cumulative career stats, fielding stats, and salary. In total, there are `20` features with `322` entries before preprocessing.\n\nThe goal of this notebook is to showcase linear and non-linear regression as a way of predicting `salary` (in thousands), a continuous variable, for my dataset. But before I run through the regression process, I have to clean the data first and figure out correlations.\n\nOne negative influence on regression models is collinearity in the feature space. Collinearity is problematic for several reasons, including overfitting, interpretability, and inefficiency. When there are many features that are highly correlated, this will create a strong negative impact on the performance. So before plugging into the model, I analyzed the correlation matrix consisting of correlations between all the features.\n\n::: {.cell execution_count=2}\n```` { .cell-code}\n```{{python}}\ndf = pd.read_csv(\"./Hitters.csv\")\nnum_cols = [col for col in df.columns if df[col].dtypes != \"O\"]\ncorr = df[num_cols].corr()\nsns.set(rc={'figure.figsize': (15, 15)})\nsns.heatmap(corr, cmap=\"RdBu\", annot=True)\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=1083 height=1160}\n:::\n:::\n\n\nBased on the above correlation matrix, I can see there's a high correlation between all the individual season player stats (`AtBat`, `Hits`, `HmRun`, `Runs`, `RBI`, and `Walks`), cumulative player stats (`CAtBat`, `CHits`, `CHmRun`, `CRuns`, `CRBI`, `Years`, and `CWalks`), and finally some fielding stats (`Assists`, `Errors`).\n\nOne way to remove these strong correlations is to run a dimensionality reduction technique. In this case, I will be using PCA (principal component analysis) seperately on those three highly correlated areas: individual stats, cumulative stats, and fielding.\n\n::: {.cell execution_count=3}\n```` { .cell-code}\n```{{python}}\npca_custat = PCA(n_components=1)\npca_indstat = PCA(n_components=1)\npca_fieldstat = PCA(n_components=1)\n\ncustat_df = df[[\"CAtBat\", \"CHits\", \"CHmRun\", \"CRuns\", \"CRBI\", \"CWalks\"]]\nindstat_df = df[[\"AtBat\", \"Hits\", \"HmRun\", \"Runs\", \"RBI\", \"Walks\"]]\nfieldstat_df = df[[\"Assists\", \"Errors\"]]\n\ncustat_df_pca = pca_custat.fit_transform(custat_df)\nindstat_df_pca = pca_indstat.fit_transform(indstat_df)\nfieldstat_df_pca = pca_fieldstat.fit_transform(fieldstat_df)\n\ndf_reduced = df.drop(columns=[\"AtBat\", \"Hits\", \"HmRun\", \"Runs\", \"RBI\", \"Walks\", \"CAtBat\", \"CHits\", \"CHmRun\", \"CRuns\", \"CRBI\", \"CWalks\", \"Assists\", \"Errors\"])\ndf_reduced = df_reduced.drop(columns=[\"League\", \"NewLeague\", \"Division\", \"Years\"])\ndf_reduced[\"I_PC1\"] = indstat_df_pca\ndf_reduced[\"C_PC2\"] = custat_df_pca\ndf_reduced[\"F_PC3\"] = fieldstat_df_pca\n\ndf_reduced.dropna(axis=0, inplace=True)\n```\n\n````\n:::\n\n\n::: {.cell execution_count=4}\n```` { .cell-code}\n```{{python}}\nnum_cols = [col for col in df_reduced.columns if df_reduced[col].dtypes != \"O\"]\ncorr = df_reduced[num_cols].corr()\nsns.set(rc={'figure.figsize': (15, 15)})\nsns.heatmap(corr, cmap=\"RdBu\", annot=True)\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=1092 height=1160}\n:::\n:::\n\n\nAfter applying PCA and removing other features, I've reduced the data from `20` columns to just `5` columns, which is important, as collinearity can negatively effect the performance of regression models. Apart from the collinearity effect, I decided to get rid of discretely labeled binary relationships (labels 0 or 1), as this makes the linear regression model more complex and can only negatively impact the performance. I also discarded `years` as a feature because it was highly correlated with the PCA of the cumulative stats (r=`0.91`).\n\n### Creating the Training and Test Sets\nSince we're trying to predict salary, I extract `salary` column from the `df_reduced`, storing it into labels array `y`, and dropping that column from the feature data. The `X` data feature space has dropped from `5` columns to `4` columns, so there are `4` total features. For splitting the data into test and train, I am using sklearn's `train_test_split()`.\n\n::: {.cell execution_count=5}\n```` { .cell-code}\n```{{python}}\ndf = df_reduced\ny = df[\"Salary\"]\nX = df.drop(columns=\"Salary\")\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n\nprint(f\"Salary STD: ${np.std(y) * 1000:,.2f}\")\nprint(f\"Salary Mean: ${np.mean(y) * 1000:,.2f}\")\nprint(f\"Salary Low: ${np.min(y) * 1000:,.2f}\")\nprint(f\"Salary High: ${np.max(y) * 1000:,.2f}\")\n```\n\n````\n\n::: {.cell-output .cell-output-stdout}\n```\nSalary STD: $450,260.22\nSalary Mean: $535,925.88\nSalary Low: $67,500.00\nSalary High: $2,460,000.00\n```\n:::\n:::\n\n\n## Multiple Linear Support Vector Regression (kernel=\"linear\")\nNow, I will test out multiple linear regression using Sklearn's `SVR` (support vector regression) class from the `SVM` library. Before passing the data into the regression model, I scaled the data using `StandardScaler()` as this is important for faster computation with regression. For the hyperparameters, I toyed with different `C` values, which influences the degree of regularization applied to the SVR model. A smaller `C` value leads to a simpler model, but a larger `C` value would fit to the training data more closely, which can potentially overfit if the number is too high. For the `C` value on the `linear` kernel, my model showed good preformance at `C=1`. \n\n::: {.cell execution_count=6}\n```` { .cell-code}\n```{{python}}\nsc = StandardScaler()\n\nX_train_scaled = sc.fit_transform(X_train, y_train)\nX_test_scaled = sc.fit_transform(X_test, y_test)\n# pca_all = PCA(n_components=1)\n# X_train_scaled_pca = pca_all.fit_transform(X_train_scaled)\n# X_test_scaled_pca = pca_all.fit_transform(X_test_scaled)\n\nsvr_lin = SVR(kernel=\"linear\", C=1, gamma=\"auto\")\nsvr_lin.fit(X_train_scaled, y_train)\ny_pred = svr_lin.predict(X_test_scaled)\n```\n\n````\n:::\n\n\n### Multiple Linear Regression Visualization\n\n::: {.cell execution_count=7}\n```` { .cell-code}\n```{{python}}\nplt.figure(figsize=(16, 12))\nindependent_variables = X_train.columns\ndependent_variable = \"Salary\"\nX_test_numpy = X_test.to_numpy()\nfor i, col in enumerate(independent_variables, 1):\n    plt.subplot(3, 3, i)\n    sns.regplot(x=X_train[col],y=y_train,ci=None,color ='red')\n    sns.scatterplot(data=X_train, x=col, y=y_train, color='blue', label='Training Points')\n    sns.scatterplot(data=X_test, x=col, y=y_test, color='green', label='Testing Points')\n    sns.scatterplot(data=X_test, x=col, y=y_pred, color='red', label='Predicted Points')\n    plt.title(f'{dependent_variable} vs. {col}')\n    plt.xlabel(col)\n    plt.ylabel(dependent_variable)\n\nplt.tight_layout()\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=1509 height=789}\n:::\n:::\n\n\nHere is a visualization showing linear regression applied on all of the features in the feature space, which make up the overall prediction. For each feature, the red line represents the function applied, which in this case is linear because I'm using a linear kernel, and the points represent all the different datapoints in the dataset. `X_train` points are in blue, `X_test` points are in green with their actual label `y_test`, and predicted points are in red (the `X_test` dataset on the `y_pred`).\n\nBased on the above visualization, it appears that linear regression works very well for all the features, although in any case, outliers are a problem. It is also worth noting that changing the `C` value does change the results, so modifying that **may** improve preformance with more fine-tuning. Below, I have outputed some metrics on the model:\n\n::: {.cell execution_count=8}\n```` { .cell-code}\n```{{python}}\nresult_df = pd.DataFrame(columns=[\"id\", \"actual\", \"predicted\"])\n\nfor i, actual, predicted in zip(y_test.index, y_test, y_pred):\n    entry = [i, actual, predicted]\n    df_entry = pd.DataFrame(entry, index=[\"id\", \"actual\", \"predicted\"]).T\n    result_df = pd.concat((result_df, df_entry))\n#print(result_df)\ndifference = abs(result_df[\"actual\"] - result_df[\"predicted\"])\nprint(f\"Cumulative Difference: ${np.sum(difference) * 1000:,.2f}\")\nprint(f\"Min Difference: ${np.min(difference) * 1000:,.2f}\")\nprint(f\"Max Difference: ${np.max(difference) * 1000:,.2f}\")\nprint(f\"Average Difference: ${np.mean(difference) * 1000:,.2f}\")\nprint(f\"Std Difference: ${np.std(difference) * 1000:,.2f}\")\nprint(f\"Mean Squared Error: ${mean_squared_error(y_test, y_pred):,.2f}\")\n```\n\n````\n\n::: {.cell-output .cell-output-stdout}\n```\nCumulative Difference: $10,792,084.13\nMin Difference: $19,061.95\nMax Difference: $1,968,301.30\nAverage Difference: $269,802.10\nStd Difference: $361,894.28\nMean Squared Error: $203,760.65\n```\n:::\n:::\n\n\nBased on this data, the total difference (sum of all the differences) between the actual and predicted outputs could be better, as the average difference between the labels is around `270` = `$270,000`. The `MSE` is a common metric used in these types of problems, and my `MSE`(mean squared error) score is around 205 (`$205,000`), which is a respectable `MSE` value for this dataset due to its high standard deviation at (`$450,260.22`).\n\n::: {.cell execution_count=9}\n```` { .cell-code}\n```{{python}}\nplt.figure(figsize=(8,8))\nresiduals = result_df['actual'] - result_df['predicted']\n\nplt.scatter(result_df['id'], residuals)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('ID')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=680 height=677}\n:::\n:::\n\n\nAbove is a visualization showing how far the differences are for each value in the test set (the residuals). `0` means no difference between the actual and the predicted, any number below `0` means the predicted value was higher than the actual value, and any number above `0` means the predicted value was lower than the actual value.\n\n## Multiple Non-Linear Support Vector Regression (kernel=\"poly\")\nNow I run SVR on a non-linear kernel and assess its comparison to a linear kernel. Since real world data has a lot of non-linearity, this comparison is worth attempting.\n\n::: {.cell execution_count=10}\n```` { .cell-code}\n```{{python}}\nsvr_poly = SVR(kernel=\"poly\", degree=2, C=75, gamma=\"scale\")\nsvr_poly.fit(X_train_scaled, y_train)\ny_pred = svr_poly.predict(X_test_scaled)\n\nresult_df = pd.DataFrame(columns=[\"id\", \"actual\", \"predicted\"])\n\nfor i, actual, predicted in zip(y_test.index, y_test, y_pred):\n    entry = [i, actual, predicted]\n    df_entry = pd.DataFrame(entry, index=[\"id\", \"actual\", \"predicted\"]).T\n    result_df = pd.concat((result_df, df_entry))\n    \n#print(result_df)\ndifference = abs(result_df[\"actual\"] - result_df[\"predicted\"])\nprint(f\"Cumulative Difference: ${np.sum(difference) * 1000:,.2f}\")\nprint(f\"Min Difference: ${np.min(difference) * 1000:,.2f}\")\nprint(f\"Max Difference: ${np.max(difference) * 1000:,.2f}\")\nprint(f\"Average Difference: ${np.mean(difference) * 1000:,.2f}\")\nprint(f\"Std Difference: ${np.std(difference) * 1000:,.2f}\")\nprint(f\"Mean Squared Error: ${mean_squared_error(y_test, y_pred):,.2f}\")\n```\n\n````\n\n::: {.cell-output .cell-output-stdout}\n```\nCumulative Difference: $13,385,640.30\nMin Difference: $798.03\nMax Difference: $1,581,505.51\nAverage Difference: $334,641.01\nStd Difference: $330,986.90\nMean Squared Error: $221,536.94\n```\n:::\n:::\n\n\nBased on the model run through with a polynomial kernel, the results are overall noticeably worse than the linear kernel, but not by a whole lot. Although, it is worth noting that the `C` value is crucial in this result. I was trying to balance the trade-off between conforming to the function and simplicity. For the polynomial degree, I decide to go with `2`, as `1` is linear and `3` didn't preform as anticipated, as the function plotted didn't represent some of the features as well as `2`.\n\n### Multiple Non-Linear Regression Visualization\n\n::: {.cell execution_count=11}\n```` { .cell-code}\n```{{python}}\nplt.figure(figsize=(16, 12))\nindependent_variables = X_train.columns\ndependent_variable = \"Salary\"\nX_test_numpy = X_test.to_numpy()\nfor i, col in enumerate(independent_variables, 1):\n    plt.subplot(3, 3, i)\n    sns.regplot(x=X_train[col],y=y_train,ci=None,color ='red', order=svr_poly.degree)\n    sns.scatterplot(data=X_train, x=col, y=y_train, color='blue', label='Training Points')\n    sns.scatterplot(data=X_test, x=col, y=y_test, color='green', label='Testing Points')\n    sns.scatterplot(data=X_test, x=col, y=y_pred, color='red', label='Predicted Points')\n    plt.title(f'{dependent_variable} vs. {col}')\n    plt.xlabel(col)\n    plt.ylabel(dependent_variable)\n\nplt.tight_layout()\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){width=1509 height=789}\n:::\n:::\n\n\nBased on the above visualzation, using a polynomial function with degree `2`, shows interesting results. For the `Salary vs PutOuts` plot, the data plotted resembles a linear kernel, but in actuality it's a very zoomed in polynomial kernel. The `Salary vs I_PC1` showed a curve which I expected. It starts off at a peak and then lowers like a parabolic function (degree 2). The `Salary vs C_PC2` plot is interesting in the sense that it's a negative parabola; I would say this is not a fully representative curve as it seems to be fitting to the outlier at the end of the plot. Finally, the `Salary vs F_PC3` plot seems to be similar to the first plot as it resembles more of a linear kernel.\n\n::: {.cell execution_count=12}\n```` { .cell-code}\n```{{python}}\nplt.figure(figsize=(8,8))\nresiduals = result_df['actual'] - result_df['predicted']\n\nplt.scatter(result_df['id'], residuals)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('ID')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-1.png){width=680 height=677}\n:::\n:::\n\n\nAbove is the residual plot for this model with the same setup as the linear kernel one.\n\n\n## Discussion and Improvements\nIn actuality, it appears to me that the `poly` non-linear kernel represents the data better for certain features even though it doesn't perform as well against the `linear` kernel. I believe the tradeoff to this approach highly depends on the `C` value for both approaches and requires more hyperparameter awareness and optimization.\n\nIn terms of features, I believe all of these features have strong presuasion in determing an MLB player's salary, but there is one flaw. Some players, regardless of preformance, are more \"famous\" than other players. It is likely that more famous players have higher salaries simply because they generate more revenue for the teams they play for, but that doesn't necessarily mean the player's popularity is correlated with skill. This is the reason why I believe there are outliers in this dataset. If there were a way to accurately determine popularity, that would be a key feature in predicting salary as well for this particular domain. Speaking of outliers, I would also like to point out that outliers can have a significant impact on these regression models. Depending on the case, deleting outliers may be a valid option, but that should come with caution as deleting data can result in loss of valuable information. \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}