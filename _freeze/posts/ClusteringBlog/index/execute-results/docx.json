{
  "hash": "790b4c84533e6c3c00c46bb0e0993a4b",
  "result": {
    "markdown": "---\nexecute:\n  echo: fenced\ntitle: Spotify Recommendation System With Clustering\nauthor: Daniel Hassler\ndate: '2023-11-12'\ncategories:\n  - code\n  - clustering\nimage: spotify.png\ntoc: true\ntoc-title: Table of contents\nformat:\n  html:\n    embed-resources: true\n    code-copy: true\n    code-link: true\n    code-tools: true\n    theme:\n      dark: darkly\n      light: flatly\n    pdf:\n      title: SpotifyRecommendationKMeans\n      author: Daniel Hassler\n      pdf-engine: 'C:/Program Files (x86)/wkhtmltopdf'\n  docx: default\n  ipynb: default\n  gfm: default\nfilters:\n  - social-share\nshare:\n  permalink: 'https://hassledw.github.io/ML-blog-posts/posts/ClusteringBlog/'\n  description: Spotify Recommendation System Using K-Means\n  twitter: true\n  facebook: true\n  reddit: true\n  stumble: false\n  tumblr: false\n  linkedin: true\n  email: true\n---\n\n<!-- title: \"Comparing Decision Tree and Random Forest Classifier Performance\"\nformat:\n  html:\n    code-fold: true\njupyter: python3 -->\n**Author: Daniel Hassler**\n\n<link rel=\"stylesheet\" type=\"text/css\" href=\"./index.css\">\n<div class=\"social-icons\">\n  <a href=\"https://github.com/hassledw\"><i class=\"fab fa-github\"></i></a>\n  <a href=\"https://www.linkedin.com/in/daniel-hassler-85027a21a/\"><i class=\"fab fa-linkedin\"></i></a>\n  <!-- Add more social media links/icons as needed -->\n</div>\n\n::: {.cell execution_count=1}\n```` { .cell-code}\n```{{python}}\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.model_selection import train_test_split, ParameterGrid\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\n```\n\n````\n:::\n\n\n## Data Analysis\nClustering algorithms can be applied to many real-world applications, including but not limited to security, anomaly detection, document clustering, stock market analysis, image compression, and so much more. The application I decided to approach with clustering is a song recommendation system. I found a dataset on Kaggle containing almost `114,000` songs from the popular music streaming platform Spotify. Each entry in the dataset consists of many features including `artists`, `track_name`, `track_genre`, `popularity`, `danceability`, and many more. \n\nBefore I dive into the visualizaitons, I first dropped duplicates in the dataset to minimize problems with recommendations. Now, below are some visualizations showcasing certain features in a scatterplot. This gives me a rough idea what the dataset looks like with all of these features and genres.\n\n::: {.cell execution_count=2}\n```` { .cell-code}\n```{{python}}\noriginal_df = pd.read_csv(\"./dataset-dedup.csv\")\nprint(original_df.columns)\n# original_df = original_df.drop_duplicates(subset=[\"artists\", \"track_name\"], keep=\"first\").reset_index()\nprint(original_df.shape)\n# original_df.to_csv(\"./dataset-dedup.csv\")\nfeatures_x = [\"loudness\", \"popularity\", \"duration_ms\"]\nfeatures_y = [\"popularity\", \"energy\", \"tempo\"]\n\nfor i, (x,y) in enumerate(zip(features_x, features_y)):\n    scatter = sns.scatterplot(x=x, y=y, hue='track_genre', data=original_df, palette=\"viridis\", alpha=0.25)\n    legend_labels = original_df['track_genre'].unique()# [:3]  # Show only the first 3 genres\n    scatter.legend(title='Genre', labels=legend_labels, prop={'size': 1})\n    plt.title(f\"Scatter Plot of {x} vs {y} by genre\")\n    plt.show()\n\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-stdout}\n```\nIndex(['Unnamed: 0.1', 'index', 'Unnamed: 0', 'track_id', 'artists',\n       'album_name', 'track_name', 'popularity', 'duration_ms', 'explicit',\n       'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n       'time_signature', 'track_genre'],\n      dtype='object')\n(81344, 23)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-docx/cell-3-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-docx/cell-3-output-3.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-docx/cell-3-output-4.png){}\n:::\n:::\n\n\n::: {.cell execution_count=3}\n```` { .cell-code}\n```{{python}}\nunique_vals = original_df['track_genre'].unique()\nplt.bar(unique_vals, original_df['track_genre'].value_counts())\nplt.title(\"Occurrence of Genre\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Genre\")\n_ = plt.xticks(rotation=\"vertical\", fontsize=4)\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-docx/cell-4-output-1.png){}\n:::\n:::\n\n\nBecause a lot of these continuous variables: `loudness`, `popularity`, `duration_ms` overlap by genre significantly, I decided to drop these features during training, as well as many other features like `energy`, `danceability`, `acousticness`, as these metrics are too complex, overlapping, and even subjective. As a Spotify consumer myself, I like when Spotify gives me songs related to the current artist I'm listening to, so I thought important features in this dataset included: `artists`, `track_genre`, minimally. Although, I did try other features like `key`, and `tempo` on top of that.\n\n## K-Means\nThe K-Means algorithm clusters data by minimizing a criteria known as `intertia`, the within-cluster sum-of-squares. The formula for inertia, specified in the K-means documentation for Sklearn, is noted below:\n\nNoting some of the variables in the summation: `n` is the number of datapoints, `mu` is the mean of the cluster, also the cluster_centroid of the cluster `C`, `||x_i - \\mu||^2` represents the squared euclidean distance between point `x_i` and the centroid, and min() takes the min of the calculation\n\n$$\n\\sum_{i=0}^{n}\\min_{\\mu_j \\in C}(||x_i - \\mu_j||^2)\n$$\n\nIt is worth noting that the inertia method has some drawbacks. According to Sklearn, intertia makes the assumption that clusters are convex and isotropic, which may not always be the case. The documentation also states that inertia isn't a \"normalized metric\", so running PCA (principal component analysis) before the K-means clustering is beneficial (which is exactly what I did in later steps).\n\nA great benefit to K-means is its scalability to large sample sets, which is good for this problem since there are now `81,344` points.\n\n### Hyperparameter Tuning\nThe biggest hyperparameter for K-means is the number of clusters `n_clusters`. This hyperparameter is the amount of clusters to generate for the problem. Because the number of clusters largely effects the results of the model, it is important to tune this. In order to chose the best value, I loop through different values up to 80.\n\n::: {.cell execution_count=4}\n```` { .cell-code}\n```{{python}}\ninertia = []\n# train_df is the numeric representation of original_df\ntrain_df = original_df.drop(columns=['Unnamed: 0.1', 'index', 'Unnamed: 0', 'track_id',\n       'album_name', 'track_name', 'popularity', 'duration_ms', 'explicit',\n       'danceability', 'energy', 'loudness', 'mode', 'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n       'time_signature'])\n\nfor col in train_df.columns:\n    if not pd.api.types.is_numeric_dtype(train_df[col]):\n        train_df[col] = pd.factorize(original_df[col])[0]\n\nscaler = StandardScaler()\n# df_scaled is the scaled version of train_df\ndf_scaled = scaler.fit_transform(train_df)\npca_num_components = 2\n\n# df_pca to reduce dimensionality\npca = PCA(n_components=pca_num_components).fit_transform(df_scaled)\ndf_pca = pd.DataFrame(pca,columns=['pca1','pca2'])\n\nfor k in range(1, 80, 10):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit_predict(df_pca)\n    inertia.append(kmeans.inertia_)\n```\n\n````\n\n::: {.cell-output .cell-output-stderr}\n```\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n```` { .cell-code}\n```{{python}}\nplt.plot(range(1, 80, 10), inertia, marker='o')\nplt.title('Elbow Method for Optimal K')\nplt.xlabel('Number of Clusters (K)')\nplt.ylabel('Inertia (Within-Cluster Sum of Squares)')\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-docx/cell-6-output-1.png){}\n:::\n:::\n\n\nThe elbow chart is a great way to visualize intertia vs number of clusters on the dataset. Since our goal is to generalize well, it's not the best to choose the \"lowest\" inertia value. It is generally recommended in practice to choose the \"elbow point\"; I chose `10` as this looks very close to an elbow point for this distribution. Although, one drawback to this approach is its subjectiveness-- you might think the elbow point is 12, whereas I think the elbow point is 10.\n\n### K-Means for Spotify\nAfter taking the resulting elbow point, I run that through my own instance of kmeans, utilizing the Sklearn library, and store the predicted results into the original dataframe.\n\n::: {.cell execution_count=6}\n```` { .cell-code}\n```{{python}}\nkmeans = KMeans(n_clusters=10, random_state=42)\noriginal_df['clusters'] = kmeans.fit_predict(df_pca)\n```\n\n````\n\n::: {.cell-output .cell-output-stderr}\n```\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n```` { .cell-code}\n```{{python}}\nsns.scatterplot(x=\"pca1\", y=\"pca2\", hue=original_df['clusters'], data=df_pca)\nplt.title('K-means Clustering PCA of 3 features [track_genre, artists, key]')\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-docx/cell-8-output-1.png){}\n:::\n:::\n\n\nThis is a PCA visualization of the clusters on the feature set `track_genre`, `artists` and `key`. \n\n## Evaluating K-Means for Spotify\nBelow are some sample mini-clusters. Since the goal of this overall problem is to recommend music based on certain songs, I decided to create a function that grabs an entry from the CSV file, finds the cluster it's in, and computes the k-nearest neighbors of that song. These nearest neighbors would be the \"recommendation\" songs, in order.\n\nThe general idea we should see with these mini-clusters are songs that resemble the query song. In the case of the first example, I ran my function on Daughtry's song \"Home\". The recommended song (top 1) example was another Daughtry song \"It's Not Over\".\n\nWhen testing out different K-means implementations on different features, I found that simplicity is key. Having a ton of features is great for any dataset, but knowing how they interact with each other and how to simplify the problem makes for better results. I tested many different subsests of features including:\n\n1. all of the original dataset features (n=20)\n2. subset of continuous variables\n3. subset of just track_genre and artists\n4. subset of track_genre, artists, tempo, and key. All of which are discrete, factual features.\n5. subset of track_genre, artists, and key.\n\nMy final result ended up being the last option, although those did not generate the most `similar` clusters, especially compared to option 3. Although, I chose the last option as I was trying to find similar songs while spanning across other artists. Option 5 seemed to give me similar options across at least one or more genres with different artists. It is worth noting that some of the results gave me the same artists, which is good since those are similar songs too. \n\n::: {.cell execution_count=8}\n```` { .cell-code}\n```{{python}}\noriginal_df['Distance_to_Centroid'] = kmeans.transform(df_pca).min(axis=1)\n\ndef get_nearest_entry(idx, k=5):\n    # print(original_df.iloc[idx])\n    # print(train_df.iloc[idx])\n    cluster = kmeans.predict(df_pca.iloc[idx].to_frame().T)[0]\n    cluster_data = original_df[original_df[\"clusters\"] == cluster]\n    cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\n    cluster_data = cluster_data.sort_values(by=\"closest_entries_to_idx\")\n    # print(cluster_data[[\"artists\", \"album_name\", \"track_name\", \"track_genre\"]])\n\n    cluster_data.drop(columns=[\"closest_entries_to_idx\"])\n    print(f\"Top {k} Closest Examples to {cluster_data.loc[idx]['artists']}'s \\\"{cluster_data.loc[idx]['track_name']}\\\"\")\n    print(cluster_data[:k][[\"artists\", \"track_name\", \"track_genre\"]])\n    print(\"\\n\\n\")\n\nget_nearest_entry(35640) # rock song\nget_nearest_entry(16587) # country song\nget_nearest_entry(41220) # rap song\n```\n\n````\n\n::: {.cell-output .cell-output-stdout}\n```\nTop 5 Closest Examples to Daughtry's \"September\"\n            artists               track_name     track_genre\n35640      Daughtry                September          grunge\n35381      Daughtry            It's Not Over          grunge\n35839    Stone Sour                 Hesitate          grunge\n55666    Mark Broom                Five/Four  minimal-techno\n40063  TNT;POPR3B3L  I'm Raving - Radio Edit       hardstyle\n\n\n\nTop 5 Closest Examples to Florida Georgia Line's \"Stay\"\n                    artists  \\\n16587  Florida Georgia Line   \n8395    Datsik;Virtual Riot   \n8582            The Prodigy   \n8819            The Prodigy   \n8529            The Prodigy   \n\n                                              track_name track_genre  \n16587                                               Stay     country  \n8395                                               Nasty   breakbeat  \n8582                                               Girls   breakbeat  \n8819                                  We Are The Ruffest   breakbeat  \n8529   Out of Space - Techno Underworld Remix Remastered   breakbeat  \n\n\n\nTop 5 Closest Examples to Future;Lil Uzi Vert's \"Tic Tac\"\n                                          artists              track_name  \\\n41220                         Future;Lil Uzi Vert                 Tic Tac   \n43994  Pritam;Arijit Singh;Shadab;Altamash Faridi  Lambiyaan Si Judaiyaan   \n41226                                    Lil Baby                  All In   \n43981     Pritam;Sukhwinder Singh;Sunidhi Chauhan                Marjaani   \n41207                    Zack Knight;Jasmin Walia         Bom Diggy Diggy   \n\n      track_genre  \n41220     hip-hop  \n43994      indian  \n41226     hip-hop  \n43981      indian  \n41207     hip-hop  \n\n\n\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_15984\\3657151199.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_15984\\3657151199.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_15984\\3657151199.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\n```\n:::\n:::\n\n\nOption 3 on the other hand gave me different songs for the same artists, which is fine for a recommendation system, but not what I was exactly going for. Below is a visualization of the clusters with just two features as well as its predictions.\n\n::: {.cell execution_count=9}\n```` { .cell-code}\n```{{python}}\noriginal_df = pd.read_csv(\"./dataset-dedup.csv\")\n# train_df is the numeric representation of original_df\ntrain_df = original_df.drop(columns=['Unnamed: 0.1', 'index', 'Unnamed: 0', 'track_id',\n       'album_name', 'track_name', 'popularity', 'duration_ms', 'explicit',\n       'danceability', 'key', 'energy', 'loudness', 'mode', 'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n       'time_signature'])\n\nfor col in train_df.columns:\n    if not pd.api.types.is_numeric_dtype(train_df[col]):\n        train_df[col] = pd.factorize(original_df[col])[0]\n\nscaler = StandardScaler()\n# df_scaled is the scaled version of train_df\ndf_scaled = scaler.fit_transform(train_df)\npca_num_components = 2\n\n# df_pca to reduce dimensionality\npca = PCA(n_components=pca_num_components).fit_transform(df_scaled)\ndf_pca = pd.DataFrame(pca,columns=['pca1','pca2'])\n\nkmeans = KMeans(n_clusters=10, random_state=42)\noriginal_df['clusters'] = kmeans.fit_predict(df_pca)\n\nsns.scatterplot(x=\"pca1\", y=\"pca2\", hue=original_df['clusters'], data=df_pca)\nplt.title('K-means Clustering PCA of 2 features [track_genre, artists]')\nplt.show()\noriginal_df['Distance_to_Centroid'] = kmeans.transform(df_pca).min(axis=1)\nget_nearest_entry(35640) # rock song\nget_nearest_entry(16587) # country song\nget_nearest_entry(41220) # rap song\n```\n\n````\n\n::: {.cell-output .cell-output-stderr}\n```\nD:\\Users\\dwh71\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_15984\\3657151199.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_15984\\3657151199.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\nC:\\Users\\dwh71\\AppData\\Local\\Temp\\ipykernel_15984\\3657151199.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cluster_data[\"closest_entries_to_idx\"] = (cluster_data[\"Distance_to_Centroid\"] - cluster_data.loc[idx][\"Distance_to_Centroid\"]).abs()\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-docx/cell-10-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTop 5 Closest Examples to Daughtry's \"September\"\n        artists            track_name track_genre\n35439  Daughtry  Waiting for Superman      grunge\n35678  Daughtry         Gone Too Soon      grunge\n35802  Daughtry            I'll Fight      grunge\n35640  Daughtry             September      grunge\n35336  Daughtry                  Home      grunge\n\n\n\nTop 5 Closest Examples to Florida Georgia Line's \"Stay\"\n                    artists         track_name track_genre\n16592  Florida Georgia Line  I Love My Country     country\n16587  Florida Georgia Line               Stay     country\n16938  Florida Georgia Line           H.O.L.Y.     country\n16598  Florida Georgia Line           Sun Daze     country\n16975  Florida Georgia Line               Life     country\n\n\n\nTop 5 Closest Examples to Future;Lil Uzi Vert's \"Tic Tac\"\n                   artists              track_name track_genre\n41220  Future;Lil Uzi Vert                 Tic Tac     hip-hop\n39123            Lionheart                  Cursed    hardcore\n39035            Lionheart                LHHC '17    hardcore\n39040              Bodyjar  A Hazy Shade of Winter    hardcore\n39033         Naked Raygun              Rat Patrol    hardcore\n\n\n\n```\n:::\n:::\n\n\n## Conclusion\nIn this blog post, I used K-means as a clustering algorithm for a song recommendation system. Though K-means does a good job at coming up with clusters and generating similar examples, other clustering algorithms such as DBSCAN may be a suitable option as well. In general, what I like about clustering algorithms for this problem domain, especially K-means, is its free range to determine what logical clusters should look like and its intuitiveness. There's not only one correct way to do K-means.\n\n",
    "supporting": [
      "index_files\\figure-docx"
    ],
    "filters": []
  }
}